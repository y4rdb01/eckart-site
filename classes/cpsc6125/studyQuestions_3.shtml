<!doctype HTML public "-//W3C//DTD HTML 3.2//EN">
<html lang="en">
<head>
<title>Dr. J Dana Eckart</title>
<link rev="mail" href="mailto:eckart_dana@columbusstate.edu">
</head>
<body>

<!--
	This is the primary overall style for the web site.
-->
<style>
	A:link    { color: #007a00; text-decoration: underline }
	A:visited { color: #7a0000; text-decoration: none }
	A:hover   { text-decoration: none }
	A:active  { color: #ff0000; text-decoration: none }
	Body	{ background-color: #ffffe5; color: #000000 }
	Body	{ font-size: 12pt }
	Address	{ font-size: 10pt }
	Table	{ font-size: 12pt }
	Th, Td	{ vertical-align: top }
	Th, Td	{ padding: 5px }
</style>

<p style="text-align: center; margin: auto; font-size: 150%">
	<strong>Dr. J Dana Eckart</strong>: Advanced Operating Systems (CPSC 6125)
	- Study Questions for Test #3
</p>

<style type="text/css">
	ol.question_list {list-style-type: none;}
	ol.question_list li:before {content: counter(question, decimal) ") ";}
	ol.question_list li { counter-increment: question;}

	ol.answer_list {list-style-type: none;}
	ol.answer_list li:before {content: counter(answer, lower-latin) ") ";}
	ol.answer_list li { counter-increment: answer;}

	ol.match_list {list-style-type: none;}
	ol.match_list li:before {content: counter(match, upper-latin) ") ";}
	ol.match_list li { counter-increment: match;}

	ul.bullet_list {list-style-type: disc;}
	ul.bullet_list li:before {content: "";}
	ul.bullet_list li { counter-increment: bogus;}
</style>

<p>
The following list of exam study questions are provided as a means to help you
assess your understanding of the topics presented in class. While every
reasonable attempt has been made to create a comprehensive list of questions,
they should <strong>not</strong> be the only means by which you assess your
own understanding of the course materials. While many of these questions
may appear on your exam, be aware that the exam may include questions
that do not appear below. However, it is unlikely you will perform well on
the exam if you have difficulty answering these questions correctly.
</p>


<ol class="question_list">
<li> Many of the statements in "C" are similar to those in "Java" since
	<ol class="answer_list">
	<li> C is based on the design of Java.
	<li> C is a design ancestor of C++ and Java.
	<li> All programming languages use the same style of statements.
	<li> Both C and Java share Eiffel as a common design ancestor.
	<li> None of the above
	</ol>
</li><br/>
<li> C has primitive types that are built into the language, but which of the
	following are <strong>not</strong> primitive types in C?
	<ol class="answer_list">
	<li> bool
	<li> string
	<li> int
	<li> char
	<li> None of the above
	</ol>
</li><br/>
<li> C has primitive types that are built into the language, but which of the
	following are <strong>not</strong> primitive types in C?
	<ol class="answer_list">
	<li> bool
	<li> int
	<li> float
	<li> string
	<li> None of the above
	</ol>
</li><br/>
<li> Whenever a boolean type result is required in C (e.g., in an "if"
	statement),
	<ol class="answer_list">
	<li> Any non-zero value represents false.
	<li> Only the zero value represents false.
	<li> Only one value represents true.
	<li> Any non-zero value represents true.
	<li> None of the above
	</ol>
</li><br/>
<li> In C, "&&" and "||" are short-circuit boolean operators just like in
	Java, while the "&" and "|" operators in C
	<ol class="answer_list">
	<li> Perform full boolean evaluation.
	<li> Are undefined.
	<li> Are synonyms for "&&" and "||".
	<li> Perform the bitwise-and and bitwise-or functions.
	<li> None of the above
	</ol>
</li><br/>
<li> The "!" operator in C is used to perform
	<ol class="answer_list">
	<li> Logical negation (e.g., turn "true" into "false").
	<li> Numeric negation (e.g., turn "-3" into "3").
	<li> Numeric inversion (e.g., turn "5" into "1/5").
	<li> Array reversal (reversing the items within an array).
	<li> None of the above
	</ol>
</li><br/>
<li> The assignment operator (=) in C
	<ol class="answer_list">
	<li> works exactly like assignment in Java.
	<li> cannot be used in boolean expressions.
	<li> cannot be used in arithmetic expressions.
	<li> returns the value assigned as its value.
	<li> None of the above
	</ol>
</li><br/>
<li> The assignment operator (=) in C
	<ol class="answer_list">
	<li> is also the equality operator.
	<li> tests for equality, but only when used in boolean expressions.
	<li> cannot be used in arithmetic expressions.
	<li> returns the value assigned as its value.
	<li> None of the above
	</ol>
</li><br/>
<li> Which pairs of operators can be easy to confuse but difficult to notice
	and debug because they can often (though not always) yield the same
	results?
	<ol class="answer_list">
	<li> + and ++
	<li> = and ==
	<li> & and &&
	<li> | and ||
	<li> None of the above
	</ol>
</li><br/>
<li> This example of a pre-increment operator, "x = 12; array[++x] = 3;",
	assigns
	<ol class="answer_list">
	<li> "array[12]" the value of 3 
	<li> "array[13]" the value of 3
	<li> "array[12]" the value of 4
	<li> "array[13]" the value of 4
	<li> None of the above
	</ol>
</li><br/>
<li> This example of a compound assignment, "x = 12; array[x += 2] = 5;",
	<ol class="answer_list">
	<li> is <strong>not</strong> allowed.
	<li> assigns "array[12]" the value of 5
	<li> assigns "array[14]" the value of 5
	<li> assigns "x" the value of 7
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> used for either
	input or output in C?
	<ol class="answer_list">
	<li> scanf
	<li> getc
	<li> printf
	<li> fprintf
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are used for input in C?
	<ol class="answer_list">
	<li> scanf
	<li> getc
	<li> printf
	<li> fprintf
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are used for output in C?
	<ol class="answer_list">
	<li> scanf
	<li> getc
	<li> printf
	<li> fprintf
	<li> None of the above
	</ol>
</li><br/>
<li> All C programs should contain a "main" function with a signature matching
        <ol class="answer_list">
        <li> void main(char* argv[])
        <li> int main(string argv[])
        <li> void main(int argc, string argv[])
        <li> int main(int argc, char* argv[])
        <li> None of the above
        </ol>
</li><br/>
<li> The int value returned by the "main" function is used to indicate
	<ol class="answer_list">
	<li> whether or not the program terminated normally.
	<li> nothing, this is a holdover from early Unix that is no longer used.
	<li> how long (in milliseconds) the program took to complete.
	<li> the error a program experienced if there was a problem.
	<li> None of the above
	</ol>
</li><br/>
<li> Function declarations in C <strong>must always</strong> have
	<ol class="answer_list">
	<li> an implementation given as part of the declaration.
	<li> only primitive types given as formal parameters.
	<li> a non-empty set of formal parameters.
	<li> a non-void return type.
	<li> None of the above
	</ol>
</li><br/>
<li> Function declarations (though not necessarily their implementations) in C
	<ol class="answer_list">
	<li> <strong>must</strong> be given inside of a class definition.
	<li> <strong>must</strong> appear within the source code file in
		which they are used.
	<li> can be nested within another function's implementation/body.
	<li> <strong>must</strong> be given in a separate file (a ".h" file)
		from their implementation.
	<li> None of the above
	</ol>
</li><br/>
<li> Function implementations in C
	<ol class="answer_list">
	<li> cannot be given within another function's implementation/body.
	<li> begin with the "function" reserved keyword, if given separate
		from the function declaration.
	<li> <strong>must always</strong> be given at the same time the
		function is declared.
	<li> appear within curly braces (i.e., { }) directly after the
		function signature.
	<li> None of the above
	</ol>
</li><br/>
<li> A "static" variable declaration within a function
	<ol class="answer_list">
	<li> reserves space for the variable apart from the run-time stack.
	<li> allows the variable to be accessed outside of the function
		implementation.
	<li> ensures that only one version of the variable exists, so that
		recursive calls to the function share the exact same variable
		storage.
	<li> is optional as <strong>all</strong> variables within function
		implementations are treated as "static" by default.
	<li> None of the above
	</ol>
</li><br/>
<li> Variables declared as "static"
	<ol class="answer_list">
	<li> may only appear within a function implementation.
	<li> can be accessed outside the scope of the function in which
		it is declared.
	<li> cannot change their values (i.e., they're constants).
	<li> have space for their values reserved outside of the run-time stack.
	<li> None of the above
	</ol>
</li><br/>
<li> Large C programs are often broken up into files ending with ".c" and ".h", 
	but
	<ol class="answer_list">
	<li> ".c" files <strong>must</strong> contain only function
		implementations.
	<li> ".h" files should contain declarations used by multiple ".c" files.
	<li> the "#include" directives allow ".c" files to use the declarations
		in the named ".h" file.
	<li> variables should <strong>never</strong> be delcared in a ".h" file.
	<li> None of the above
	</ol>
</li><br/>
<li> The "extern" declaration in C is
	<ol class="answer_list">
	<li> used to declare variables <strong>without</strong> setting
		aside storage space.
	<li> implicit for <strong>all</strong> function declarations.
	<li> used for variables declared within a function so that they can be
		accessed outside of the function.
	<li> have space for their values reserved outside of the run-time stack.
	<li> None of the above
	</ol>
</li><br/>

<li> Arrays in C are indexed starting with
	<ol class="answer_list">
	<li> -1
	<li> 0
	<li> 1
	<li> the value indicated when they are declared. 
	<li> None of the above
	</ol>
</li><br/>
<li> The index given for an array reference in C
	<ol class="answer_list">
	<li> only needs to be checked at <em>compile time</em> to ensure that
		it's within range. 
	<li> is <strong>always</strong> checked at <em>run time</em> to ensure
		that it's within range.
	<li> <strong>never</strong> needs to be checked as it is
		<strong>always</strong> within range.
	<li> is <strong>not</strong> checked since it doesn't have to be
		within its range.
	<li> None of the above
	</ol>
</li><br/>
<li> A "string" value in C is really an array of type "char". So the
	<ol class="answer_list">
	<li> string value needs to be null terminated.
	<li> array <strong>must</strong> be exactly as long as the desired
		string.
	<li> array should be at least one char longer than the desired string.
	<li> programer <strong>must</strong> use another variable to keep
		track of the length of the string.
	<li> None of the above
	</ol>
</li><br/>
<li> If an array index in C is outside the bounds of the array, then
	<ol class="answer_list">
	<li> an error may or may not occur as a result of accessing the array
		element.
	<li> an error <strong>always</strong> occurs and the program stops
		execution.
	<li> there may or may not be accessible storage associated with
		that array location.
	<li> there is <strong>always</strong> accessible storage associated
		with that array location.
	<li> None of the above
	</ol>
</li><br/>
<li> The declaration of an array (e.g., variables, function formal parameters)
	<strong>must always</strong> include the
	<ol class="answer_list">
	<li> starting index of the array.
	<li> size of the array.
	<li> corresponding "#define" declaration for the array size.
	<li> type of the array elements.
	<li> None of the above
	</ol>
</li><br/>
<li> Arrays in C
	<ol class="answer_list">
	<li> grow dynamically as more items are added (similar to
		<em>ArrayList</em>s in Java).
	<li> <strong>always</strong> have an associated length that can be
		queried to determine the number of items in the array.
	<li> are polymorphic, meaning that any mix of items can be stored in
		the same array.
	<li> are really just pointers to their item type (e.g., "int x[]" and
		"int *x" are the same).
	<li> None of the above
	</ol>
</li><br/>
<li> A pointer is another name for the
	<ol class="answer_list">
	<li> type of values that can be assigned to a variable.
	<li> forward declaration of a function.
	<li> declaration of a "struct"ure.
	<li> address of the memory location holding a desired value.
	<li> None of the above
	</ol>
</li><br/>
<li> In C, pointers are declared
	<ol class="answer_list">
	<li> only for declarations of arrays.
	<li> by giving an asterisk (*) before the variable name declaration.
	<li> by giving an ampersand (&) before the variable name declaration.
	<li> for <strong>all</strong> formal parameters of functions.
	<li> None of the above
	</ol>
</li><br/>
<li> In C, if "x" and "y" are declared by "int x, *y;", then "y = &x;"
	<ol class="answer_list">
	<li> is undefined.
	<li> assigns "y" the value of "x".
	<li> makes "y" an alias for "x" so that any value assigned to one
		is automatically assigned to the other.
	<li> assigns "y" the memory address where the value for "x" is located.
	<li> None of the above
	</ol>
</li><br/>
<li> This array declaration in C, "char name[10];",
	<ol class="answer_list">
	<li> reserves 10 contiguous bytes (characters) of storage for "name".
	<li> is equivalent to the declaration "string name;".
	<li> declares "name" as a pointer to the reserved storage.
	<li> declares each element of the array "name" as a pointer to a "char".
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following declarations in C is equivalent to "char *var;"?
	<ol class="answer_list">
	<li> char &var;
	<li> char var[10];
	<li> char var[];
	<li> char &var[];
	<li> None of the above
	</ol>
</li><br/>
<li> Strings in C are represented as an array of "char"acters
	<ol class="answer_list">
	<li> with the length kept separately so that the size of the string
		is known (e.g., when printing).
	<li> that are terminated by a null value to denote their end.
	<li> and can be referenced via a "char"acter pointer (e.g., "char *").
	<li> which are immutable, so that the value cannot be changed
		(only copied).
	<li> None of the above
	</ol>
</li><br/>
<li> The address operator in C, ampersand (&),
	<ol class="answer_list">
	<li> converts a variable into a pointer, so that "&y" makes "y" a
		pointer to an "int" when "y" was declared via "int y;".
	<li> is only used when passing parameters.
	<li> returns the memory address of any expression it is applied to
		(e.g., "&(2*x + 3)").
	<li> returns the memory address of the variable it is applied to
		(e.g., "&x").
	<li> None of the above
	</ol>
</li><br/>
<li> In C, if the following declaration/initialization is made,
	"int *x = &y;", then "x++"
	<ol class="answer_list">
	<li> makes "x" point to the next address in memory (following "y")
		that can hold an "int" value.
	<li> is undefined and causes a compilation error.
	<li> is defined but causes a run-time error.
	<li> increases the value of "y" by 1.
	<li> None of the above
	</ol>
</li><br/>
<li> Write a C program that takes a single command line argument and prints
	"YES" if the argument is a palindrome, and "NO" otherwise. The
	program should <strong>not</strong> declare any additional arrays
	other than the one that is a parameter to "main".
</li><br/>
<li> Write a C program that reverses the input given to it. So that the last
	character on the last line of input will be the first one printed.
	You may assume that the input is no more than 2047 characters long.
</li><br/>
<li> Write a C program that reads floats from the input, calculates and prints
	out their average, and indicates how many of the original values were
	above and how many were below the average.
	You may assume that there are no more than 2047 numbers in the input.
</li><br/>

<li> A "struct" declaration in C can group together
	<ol class="answer_list">
	<li> multiple data items.
	<li> data of the same type.
	<li> data of different types.
	<li> None of the above
	</ol>
</li><br/>
<li> A "struct" declaration in C <strong>always</strong> creates
	<ol class="answer_list">
	<li> a new type name that can be used in declaring variables (e.g.,
		"person bob;").
	<li> a named structure form that can be referred to by "struct"
		followed by the structure name (e.g., "struct person").
	<li> a recursive structure so that linked structures (e.g., lists,
		graphs) can be coded.
	<li> a non-recursive structure since recursive structures
		<strong>must</strong> be declared as types instead of "struct"s.
	<li> None of the above
	</ol>
</li><br/>
<li> A self-recursive "struct" requires that at least one of its elements be
	declared as
	<ol class="answer_list">
	<li> "struct NAME", where "NAME" is the name given the "struct" and the
		pointer type is inferred.
	<li> "struct NAME *", where "NAME" is the name given the "struct".
	<li> "struct" where the current structure and pointer type are inferred.
	<li> "struct *" where the current structure name is inferred.
	<li> None of the above
	</ol>
</li><br/>
<li> The "typedef" declaration in C declares the new type NAME corresponding
	to a specified TYPE_DESCRIPTION (either an existing type name or other
	type specification - such as a "struct") via
	<ol class="answer_list">
	<li> "typedef NAME = TYPE_DESCRIPTION;"
	<li> "typedef TYPE_DESCRIPTION NAME;"
	<li> "typedef NAME TYPE_DESCRIPTION;"
	<li> "NAME = typedef TYPE_DESCRIPTION;"
	<li> None of the above
	</ol>
</li><br/>
<li> Assuming the legal C variable declaration "person bob;" then the following
	declaration <strong>must</strong> also have been previously given
	("..." represents other appropriate declarations):
	<ol class="answer_list">
	<li> "struct bob { ... };"
	<li> "typedef struct person { ... } bob;"
	<li> "typedef struct bob bob;"
	<li> "typedef person bob;"
	<li> None of the above
	</ol>
</li><br/>
<li> Assuming the legal C variable declaration "person bob;" then the following
	declaration <strong>must</strong> also have been previously given
	("..." represents other appropriate declarations):
	<ol class="answer_list">
	<li> "struct person { ... };"
	<li> "typedef struct { ... } person;"
	<li> "typedef struct person bob;"
	<li> "typedef person bob;"
	<li> None of the above
	</ol>
</li><br/>
<li> Given the following legal C declarations
<pre><code>
	typedef struct node {
		int x;
		struct node *next;
	} *nodeType;
</code></pre>
	<ol class="answer_list">
	<li> "nodeType" is a type equivalent to "struct node".
	<li> "nodeType" is a type equivalent to the type of the "next" field.
	<li> "nodeType" is <strong>not</strong> a type, but really a variable
		of type "struct node".
	<li> "nodeType" is <strong>not</strong> a type, but really a variable
		of type pointer to a "struct node".
	<li> None of the above
	</ol>
</li><br/>
<li> In C, the standard way to request heap storage space while a program is
	running is to use the function
	<ol class="answer_list">
	<li> <em>getmem</em>
	<li> <em>new</em>
	<li> <em>allocate</em>
	<li> <em>heap_request</em>
	<li> None of the above
	</ol>
</li><br/>
<li> In C, the standard way to request heap storage space while a program is
	running is to use the function
	<ol class="answer_list">
	<li> <em>malloc</em>
	<li> <em>new</em>
	<li> <em>getmem</em>
	<li> <em>getheap</em>
	<li> None of the above
	</ol>
</li><br/>
<li> The standard way in C to return heap storage that was previously allocated
	is to use the function
	<ol class="answer_list">
	<li> <em>delete</em>
	<li> <em>return</em>
	<li> <em>free</em>
	<li> <em>putmem</em>
	<li> None of the above
	</ol>
</li><br/>
<li> In C, it is possible to determine the size of a type (in bytes) by using
	the function
	<ol class="answer_list">
	<li> <em>getsize</em>
	<li> <em>sizeof</em>
	<li> <em>sizeOfType</em>
	<li> <em>typeSize</em>
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following C statements is a typical way that dynamic storage
	might be allocated?
	<ol class="answer_list">
	<li> int x = new int();
	<li> int *x = (int) getmem(int);
	<li> int x = malloc(typeSize(int));
	<li> int *x = (int*) malloc(sizeof(int));
	<li> None of the above
	</ol>
</li><br/>
<li> Write a C program that reads in text of unknown length and prints out
	"YES" if the text forms a palindrome and "NO" if it does not. The
	program <strong>must not</strong> assume a maximal length of the text
	that is read in, and <em>must avoid</em> using arraylist and realloc.
</li><br/>
<li> Write a C program that reads in a list of words (given one per line) and
	prints them out (one word per line) in sorted order. You
	<strong>must</strong> write your own sorting routine (insertion sort
	is recommended). The solution may use arrays and malloc, but
	<strong>must avoid</strong> using arraylist and realloc.
</li><br/>
<li> Write a C program that reads floats from the input, calculates and prints
	out their average, and indicates how many of the original values were
	above and how many were below the average. The program <strong>must
	not</strong> assume a maximal numbers of input, and
	<strong>must avoid</strong> using arraylist and realloc.
</li><br/>

<li> An operating system (OS) provides
	<ol class="answer_list">
	<li> a clean and simple model of the computer.
	<li> manages the computer's resources.
	<li> support for the creation and use of user programs.
	<li> an abstract view of the computer, independent of the specific
		hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are program <em>types</em> in an operating system
	(OS)?
	<ol class="answer_list">
	<li> System programs
	<li> Adventure games.
	<li> Application programs
	<li> CAPTCHAs (Completely Automated Public Turing test to tell
		Computers and Humans Apart).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are layers <strong>within</strong> an
	operating system (OS)?
	<ol class="answer_list">
	<li> Machine language
	<li> Command interpreter (shell)
	<li> Physical devices
	<li> Microcode
	<li> None of the above
	</ol>
</li><br/>
<li> The operating system (OS) is that part of the system which
	<ol class="answer_list">
	<li> implements the editors, compilers, and interpreters used to
		create other programs.
	<li> provides the command line interpreter (e.g., shell) for users.
	<li> comes pre-loaded on purchased computer hardware.
	<li> runs in supervisor (or kernel) mode.
	<li> None of the above
	</ol>
</li><br/>
<li> When viewed as an abstract machine, the operating system (OS)
	<ol class="answer_list">
	<li> enables Windows and Unix to run at the same time.
	<li> hides the details of lower level facilities (e.g., writing data
		to disk).
	<li> operates exactly the way a Turing machine does.
	<li> provides the implementations for data abstractions (e.g., stack,
		queue) leveraged by user programs.
	<li> None of the above
	</ol>
</li><br/>
<li> When viewed as a resource manager, the operating system (OS)
	<ol class="answer_list">
	<li> coordinates the sharing of parts of the computer by different
		programs.
	<li> enables different programs to share the computer hardware.
	<li> determines when more memory or disk should be purchased.
	<li> ensures that the computer hardware is utilized with optimal
		efficiency regarding every user's needs.
	<li> None of the above
	</ol>
</li><br/>

<li> The most common small computer architecture is the
	<ol class="answer_list">
	<li> packet switched.
	<li> bus.
	<li> circuit switched.
	<li> fully connected backplane.
	<li> None of the above
	</ol>
</li><br/>
<li> Match the components in the below diagram (indicated by the letters A-G)
	of a typical small computer system to their names.
	<br/>
	<img src="/eckart/classes/cpsc3125/questions/SystemArch.png" width="532" height="394" alt="Simple computer architecture" />
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> Control bus
		<li> Instruction decoder
		<li> Memory
		<li> System bus
		<li> Input and Output
		<li> Peripheral bus
		<li> CPU
		<li> Address bus
		<li> Memory interface
		<li> Data bus
		</ol>
	</td></tr></table>
</li><br/>
<li> The system bus on a small computer system is typically composed of the
	<ol class="answer_list">
	<li> control bus.
	<li> address bus
	<li> data bus.
	<li> memory bus.
	<li> None of the above
	</ol>
</li><br/>
<li> A bus architecture typically allows
	<ol class="answer_list">
	<li> only a single pair of components to communicate at a time.
	<li> one or two different pairs of components to communicate
		simultaneously.
	<li> up to three different components (<strong>not</strong> pairs)
		to communicate simultaneously.
	<li> <strong>all</strong> components to communicate simultaneously.
	<li> None of the above
	</ol>
</li><br/>
<li> The internet is an example of a
	<ol class="answer_list">
	<li> packet switched architecture.
	<li> bus architecture.
	<li> circuit switched architecture.
	<li> fully connected architecture.
	<li> None of the above
	</ol>
</li><br/>
<li> A computer processor is commonly composed of
	<ol class="answer_list">
	<li> an instruction fetch unit.
	<li> an instruction decoder.
	<li> an arithmetic logic unit.
	<li> a memory interface.
	<li> None of the above
	</ol>
</li><br/>
<li> Match the components in the below diagram (indicated by the letters A-E)
	of a typical computer processor to their names. [Note: No choice is
	used more than once, and some may not be used.]
	<br/>
	<img src="/eckart/classes/cpsc3125/questions/CPUarch.png" alt="CPU" width="663" height="525" />
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> Registers
		<li> Control bus
		<li> Translation Lookaside Buffer (TLB)
		<li> Instruction decoder
		<li> Direct Memory Access (DMA)
		<li> Arithmetic Logic Unit (ALU)
		<li> Instruction fetcher
		<li> Address bus
		<li> Memory interface
		<li> Data bus
		</ol>
	</td></tr></table>
</li><br/>
<li> Registers in those processors which utilize them
	<ol class="answer_list">
	<li> are <strong>always</strong> grouped into sets of "register windows".
	<li> are <strong>sometimes</strong> dedicated to specific purposes
		(e.g., program counter, stack pointer).
	<li> are <strong>always</strong> general purpose, meaning they can be
		used for any activity needing to use a register.
	<li> usually have the same number of bits as the address bus.
	<li> None of the above
	</ol>
</li><br/>
<li> Processor designs that can speed up computation include:
	<ol class="answer_list">
	<li> pipelining
	<li> multi-register
	<li> hyperthreaded
	<li> multi-core
	<li> None of the above
	</ol>
</li><br/>
<li> The memory hierarchy in computer design primarily reflects the
	<ol class="answer_list">
	<li> desire to reduce single points of failure.
	<li> tradeoff between speed and cost for a given amount of memory.
	<li> difference between volatile and non-volatile memory technologies.
	<li> large amounts of certain types of memories.
	<li> None of the above
	</ol>
</li><br/>
<li> A hard disk drive (HDD) can have access times that are
	<ol class="answer_list">
	<li> up to 10 million times slower than a register.
	<li> 10-100 times slower than a solid state disk (SSD).
	<li> only 100 times slower than main memory (e.g., RAM).
	<li> 1 million times slower than main memory (e.g., RAM).
	<li> None of the above
	</ol>
</li><br/>
<li> Physical devices are accessed via their
	<ol class="answer_list">
	<li> corresponding user level resource managers.
	<li> associated (physical) controllers.
	<li> device drivers by the operating system (OS).
	<li> associated command interpreter (e.g., shell) programs.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are device communications styles used by
	operating systems?
	<ol class="answer_list">
	<li> Notify and Sleep
	<li> Busy waiting
	<li> Interrup driven
	<li> Direct Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> Approaches currently in common use for "installing" a new device driver
	include
	<ol class="answer_list">
	<li> relinking the device driver object code with the OS kernel.
	<li> adding the device driver file to an OS config and rebooting.
	<li> adding the ".h" file to the OS config, recompiling the kernel,
		and then rebooting the system.
	<li> dynamically loading the device driver code (no reboot).
	<li> None of the above
	</ol>
</li><br/>
<li> Device drivers that start an I/O then sit in a tight loop, checking to
	see when the operation is complete are using the
	<ol class="answer_list">
	<li> Notify and Sleep communication style.
	<li> Busy waiting communication style.
	<li> Interrup driven communication style.
	<li> Direct Memory Access communication style.
	<li> None of the above
	</ol>
</li><br/>
<li> Device drivers that start an I/O but then block, returning control so
	that other work can be done until the driver is awakened by an
	interrupt are using the
	<ol class="answer_list">
	<li> Notify and Sleep communication style.
	<li> Busy waiting communication style.
	<li> Interrup driven communication style.
	<li> Direct Memory Access communication style.
	<li> None of the above
	</ol>
</li><br/>
<li> Device drivers that utilize special hardware to initiate and complete
	an entire data transfer are using the
	<ol class="answer_list">
	<li> Notify and Sleep communication style.
	<li> Busy waiting communication style.
	<li> Interrup driven communication style.
	<li> Direct Memory Access communication style.
	<li> None of the above
	</ol>
</li><br/>
<li> While the operating system (OS) is handling an interrupt
	<ol class="answer_list">
	<li> <strong>all</strong> other interrupts are disabled to ensure it
		completes.
	<li> only higher-priority interrupts are enabled.
	<li> <strong>all</strong> interrupts continue to be enabled,
		since none should be missed.
	<li> only interrupts of the same kind are enabled.
	<li> None of the above
	</ol>
</li><br/>
<li> The boot process for most small computer systems begins with the
	execution of the
	<ol class="answer_list">
	<li> hard disk drive device driver to allow loading of the OS
		executable file.
	<li> Basic Input/Output System (BIOS).
	<li> OS executable file directly from the boot device (e.g., HDD, SSD).
	<li> GRand Unified Bootloader (GRUB).
	<li> None of the above
	</ol>
</li><br/>

<li> The system calls that an OS makes available
	<ol class="answer_list">
	<li> can only be used by the OS.
	<li> <strong>must never</strong> be used by the OS
	<li> are the OS interface for user programs.
	<li> are provided as a convenience, but aren't really necessary.
	<li> None of the above
	</ol>
</li><br/>
<li> The difference(s) between a program and a process are
	<ol class="answer_list">
	<li> programs are static (non-executing).
	<li> programs are <strong>never</strong> part of the operating system.
	<li> processes also contain a run-time stack and program counter.
	<li> processes make use of system calls, but programs don't.
	<li> None of the above
	</ol>
</li><br/>
<li> A process
	<ol class="answer_list">
	<li> <strong>must</strong> use system calls to accomplish anything
		useful.
	<li> usually only directly communicates with its parent (or child)
		process(es).
	<li> may create a child process.
	<li> cannot use system calls unless it is being run by the
		administrator/root user.
	<li> None of the above
	</ol>
</li><br/>
<li> When a problem or unusual condition is encountered, a process can send
	signals to other processes. Processes receiving a signal
	<ol class="answer_list">
	<li> are required to provide a signal handler.
	<li> can specify specific functions to be called when a particular
		type of signal is received.
	<li> run the function named "sighand" to handle the signal.
	<li> run the specified signal handler function, which takes over
		execution when the signal is received. 
	<li> None of the above
	</ol>
</li><br/>
<li> Every process is limited by the restrictions placed upon it by the
	OS as determined by the process' associated
	<ol class="answer_list">
	<li> programming language in which the code was written.
	<li> program source code file.
	<li> working directory in the file system.
	<li> user identification (uid).
	<li> None of the above
	</ol>
</li><br/>
<li> The address space of a process
	<ol class="answer_list">
	<li> is the size of the program file for the process.
	<li> is the range of addressable bytes starting at 0 up to a maximum, 
		usually 2^N - 1 (where N is the number of address bits).
	<li> is the amount of main memory <em>initially</em>
		requested/needed by the process.
	<li> can be broken into smaller chunks so that programs larger than
		the amount of main memory can be executed.
	<li> None of the above
	</ol>
</li><br/>
<li> System calls <strong>must</strong> be used to create new
	<ol class="answer_list">
	<li> processes.
	<li> user accounts.
	<li> programs.
	<li> files or directories.
	<li> None of the above
	</ol>
</li><br/>
<li> Every process has an associated
	<ol class="answer_list">
	<li> system call for accessing it.
	<li> working directory in the file system.
	<li> user identification (uid).
	<li> address space.
	<li> None of the above
	</ol>
</li><br/>
<li> A file descriptor is a unsigned number that processes use to reference
	(e.g., read, write)
	<ol class="answer_list">
	<li> files that the process currently has open.
	<li> any file in the file system that the process is permitted to
		access/open.
	<li> entries in the process' table of active/open files.
	<li> <strong>all</strong> directories with the same user
		identification (uid) as the process.
	<li> None of the above
	</ol>
</li><br/>
<li> In addition to normal user files (e.g., text files) there are
	also special files that enable
	<ol class="answer_list">
	<li> users to create new file systems.
	<li> hard disk drives (HDDs) to look like files.
	<li> keyboards and mice to look like files.
	<li> processes to directly communicate with one another.
	<li> None of the above
	</ol>
</li><br/>
<li> Every OS has an I/O subsystem that
	<ol class="answer_list">
	<li> implements the details of <strong>all</strong>
		read/write operations.
	<li> performs <strong>all</strong> of the cryptographic functions.
	<li> manages the devices attached to the computer.
	<li> enforces <strong>all</strong> of the OS security measures.
	<li> None of the above
	</ol>
</li><br/>
<li> The command interpreter which provides the non-GUI interface to the OS is
	<ol class="answer_list">
	<li> an ordinary program that uses system calls to access the
		capabilities of the OS.
	<li> a special program that directly implements <strong>all</strong>
		of the available commands.
	<li> often called the "shell".
	<li> unique, with each system having only one such program available.
	<li> None of the above
	</ol>
</li><br/>

<li> Which system organization is <strong>best</strong> described as having:
	a set of service procedures that perform system calls; utility
	procedures to assist in service procedure implementation; and a
	main program that calls requested service procedures?
	<ol class="answer_list">
	<li> Monolithic System
	<li> Layered System
	<li> Micorkernel
	<li> Client-Server
	<li> None of the above
	</ol>
</li><br/>
<li> Which system organization is <strong>best</strong> described
	as a sequence of abstractions built one on top of the other?
	<ol class="answer_list">
	<li> Layered System
	<li> Micorkernel
	<li> Exokernel
	<li> Client-Server
	<li> None of the above
	</ol>
</li><br/>
<li> Which system organization is <strong>best</strong> described as having
	the absolute minimal portion of the OS facilities running in
	kernel/supervisor mode, with the remaining portions running in user
	mode?
	<ol class="answer_list">
	<li> Layered System
	<li> Micorkernel
	<li> Exokernel
	<li> Client-Server
	<li> None of the above
	</ol>
</li><br/>
<li> Which system organization <strong>best</strong> enables the clean
	separation of policy and mechanism?
	<ol class="answer_list">
	<li> Layered System
	<li> Micorkernel
	<li> Exokernel
	<li> Client-Server
	<li> None of the above
	</ol>
</li><br/>
<li> Which system organization is <strong>best</strong> suited for
	distributed systems?
	<ol class="answer_list">
	<li> Monolithic System
	<li> Layered System
	<li> Micorkernel
	<li> Client-Server
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are examples of a layered OS?
	<ol class="answer_list">
	<li> Minix v1
	<li> THE
	<li> Windows NT
	<li> MULTICS
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are examples of a microkernel OS?
	<ol class="answer_list">
	<li> Minix v1
	<li> THE
	<li> Windows NT
	<li> MULTICS
	<li> None of the above
	</ol>
</li><br/>
<li> A virtual machine is designed to
	<ol class="answer_list">
	<li> mimic any type of hardware so that older software can be run.
	<li> provide "copies" of the existing hardware so that multiple
		operating systems can be run at the same time.
	<li> allow the remote (aka "virtual") execution of programs on
		non-local hardware.
	<li> enable the use of virtual memory to run processes that require
		a larger address space than is otherwise supported by the
		hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> An OS that runs directly on the hardware and provides virtual machines in
	which other OSes can run, is called a(n)
	<ol class="answer_list">
	<li> type 1 hypervisor.
	<li> type 2 hypervisor.
	<li> exokernel.
	<li> microkernel.
	<li> None of the above
	</ol>
</li><br/>
<li> An OS that runs on top of a host OS (that in turn is running on top of
	the physical hardware), and which can host other OSes running on top
	of it, is called a(n)
	<ol class="answer_list">
	<li> type 1 hypervisor.
	<li> type 2 hypervisor.
	<li> exokernel.
	<li> microkernel.
	<li> None of the above
	</ol>
</li><br/>
<li> Software that divides up the underlying physical hardware so that each
	running OS has sole access to its assigned hardware is called a(n)
	<ol class="answer_list">
	<li> type 1 hypervisor.
	<li> type 2 hypervisor.
	<li> exokernel.
	<li> microkernel.
	<li> None of the above
	</ol>
</li><br/>
<li> A type 1 hypervisor
	<ol class="answer_list">
	<li> runs directly on the hardware and the operating systems run in
		the hypervisor.
	<li> runs on top of the host OS (which is running directly on the
		hardware).  Additional operating systems run in the hypervisor.
	<li> divides the hardware up into slices, with each OS running
		directly on the hardware. It ensures that each OS
		only uses the parts of the hardware is was assigned.
	<li> provides a microkernel used by each of the operating systems
		running in the hypervisor. Each OS uses the microkernel
		system calls to interact with the hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> A type 2 hypervisor
	<ol class="answer_list">
	<li> runs directly on the hardware and the operating systems run in
		the hypervisor.
	<li> runs on top of the host OS (which is running directly on the
		hardware).  Additional operating systems run in the hypervisor.
	<li> divides the hardware up into slices, with each OS running
		directly on the hardware. It ensures that each OS
		only uses the parts of the hardware is was assigned.
	<li> provides a microkernel used by each of the operating systems
		running in the hypervisor. Each OS uses the microkernel
		system calls to interact with the hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> An exokernel is different from a virtual machine in that it
	<ol class="answer_list">
	<li> runs directly on the hardware and the operating systems run in
		the hypervisor.
	<li> runs on top of the host OS (which is running directly on the
		hardware).  Additional operating systems run in the hypervisor.
	<li> divides the hardware up into slices, with each OS running
		directly on the hardware. It ensures that each OS
		only uses the parts of the hardware is was assigned.
	<li> provides a microkernel used by each of the operating systems
		running in the hypervisor. Each OS uses the microkernel
		system calls to interact with the hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following mechanisms for running multiple operating systems
	on shared hardware has the least amount of additional overhead?
	<ol class="answer_list">
	<li> type 1 hypervisor
	<li> type 2 hypervisor
	<li> microkernel
	<li> exokernel
	<li> None of the above
	</ol>
</li><br/>

<li> The illusion that multiple processes are running at the same time is
	known as
	<ol class="answer_list">
	<li> parallelism.
	<li> pseudoparallelism.
	<li> hyperthreading.
	<li> multiprocessing.
	<li> None of the above
	</ol>
</li><br/>
<li> Pseudoparallelism is commonly achieved on a single processor system by
	<ol class="answer_list">
	<li> letting each process run to completion.
	<li> executing each process until it blocks before running the next
		process.
	<li> running <em>exactly</em> one instruction from each process in
		round-robin fashion.
	<li> rapidly switching of the CPU between processes (multiprogramming).
	<li> None of the above
	</ol>
</li><br/>
<li> A process is
	<ol class="answer_list">
	<li> another name for a program.
	<li> the compiled (executable) version of a program.
	<li> a program in execution.
	<li> a program that is either running (in the CPU) or ready, but
		<strong>not</strong> blocked.
	<li> None of the above
	</ol>
</li><br/>
<li> Existing processes in a Unix/Linix/POSIX system can be viewed using
	which command/facility?
	<ol class="answer_list">
	<li> task manager
	<li> pwd
	<li> ps
	<li> listAll
	<li> None of the above
	</ol>
</li><br/>
<li> Existing processes in a (Microsoft) Windows system can be viewed using
	which command/facility?
	<ol class="answer_list">
	<li> task manager
	<li> pwd
	<li> ps
	<li> listAll
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <em>voluntary</em> reasons for a process to
	terminate/exit?
	<ol class="answer_list">
	<li> Normal
	<li> Error
	<li> Fatal
	<li> Killed
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <em>INvoluntary</em> reasons for a process to
	terminate/exit?
	<ol class="answer_list">
	<li> Normal
	<li> Error
	<li> Fatal
	<li> Killed
	<li> None of the above
	</ol>
</li><br/>
<li> Processes in a POSIX system exist as a hierarchy because
	<ol class="answer_list">
	<li> each process corresponds to a program file (and files are
		hierarchical).
	<li> except for the first process, <strong>all</strong> other
		processes are created by an existing process.
	<li> each process corresponds to a user identification, and uids are
		arranged hierarchically.
	<li> of the time frames in which they were created.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a state in which an
	executing process can exist?
	<ol class="answer_list">
	<li> Running
	<li> Blocked
	<li> Ready
	<li> Waiting
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a transition between
	process states?
	<ol class="answer_list">
	<li> running to blocked
	<li> running to ready
	<li> ready to running
	<li> ready to blocked
	<li> blocked to running
	<li> blocked to ready
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following transition(s) are accomplished by the scheduler?
	<ol class="answer_list">
	<li> running to blocked
	<li> running to ready
	<li> ready to running
	<li> ready to blocked
	<li> blocked to running
	<li> blocked to ready
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following transition(s) occur as the result of receiving
	a signal?
	<ol class="answer_list">
	<li> running to blocked
	<li> running to ready
	<li> ready to running
	<li> ready to blocked
	<li> blocked to running
	<li> blocked to ready
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following transition(s) are accomplished when an I/O
	operation is completed?
	<ol class="answer_list">
	<li> running to blocked
	<li> running to ready
	<li> ready to running
	<li> ready to blocked
	<li> blocked to running
	<li> blocked to ready
	<li> None of the above
	</ol>
</li><br/>
<li> Which state is a process in when it terminates normally?
	<ol class="answer_list">
	<li> Running
	<li> Blocked
	<li> Ready
	<li> Waiting
	<li> None of the above
	</ol>
</li><br/>
<li> Which state does a process first start in when it is created?
	<ol class="answer_list">
	<li> Running
	<li> Blocked
	<li> Ready
	<li> Waiting
	<li> None of the above
	</ol>
</li><br/>
<li> Match the ovals (process states) and arcs (state transitions) in the below
	diagram (indicated by the letters A-I) with their names/descriptions.
	[Note: No choice is used more than once, and some may not be used.]
	<br/>
	<img src="/eckart/classes/cpsc3125/questions/ProcessState.png" alt="Process state diagram" width="637" height="417" />
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> I/O Wait
		<li> Process Termination
		<li> Reload
		<li> Blocked
		<li> Dispatch (scheduling)
		<li> Parked
		<li> Process Entry
		<li> I/O Completion
		<li> Running
		<li> Swapping
		<li> Timeout
		<li> Ready
		</ol>
	</td></tr></table>
</li><br/>
<li> The process table is an array with each entry containing
	<ol class="answer_list">
	<li> the file of the corresponding program.
	<li> only the current program counter for that process.
	<li> only the process id number for that process.
	<li> <strong>all</strong> of the associated information for a single
		process.
	<li> None of the above
	</ol>
</li><br/>
<li> A process that has been started but has <strong>not</strong> yet
	terminated, is represented by an entry in the process table when
	the process is in which of the following states?
	<ol class="answer_list">
	<li> Running
	<li> Blocked
	<li> Ready
	<li> Waiting
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX system call that creates a new process is called
	<ol class="answer_list">
	<li> new
	<li> create
	<li> dup
	<li> fork
	<li> None of the above
	</ol>
</li><br/>
<li> The family of POSIX system calls that replace the current process with a
	new one is called
	<ol class="answer_list">
	<li> replace
	<li> exec
	<li> dup2
	<li> getpid
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX system call that causes a parent process to wait for a specific
	child process to finish is called
	<ol class="answer_list">
	<li> wait
	<li> waitpid
	<li> fini
	<li> done
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX system call, <em>signal</em> is used to
	<ol class="answer_list">
	<li> send a signal to another process.
	<li> declare what signals may be sent to a process.
	<li> respond back to a signal sent from another process.
	<li> indicate which function should be called when a particular
		signal is received by the process.
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX system call that moves a running process to the blocked state
	is called
	<ol class="answer_list">
	<li> wait
	<li> break
	<li> pause
	<li> block
	<li> None of the above
	</ol>
</li><br/>
<li> Write a C program that takes a single command line argument indicating the
	number of child processes to create. Each child process prints out its
	PID to the stdout and then exits. The parent process prints the message
	"All done." after <strong>all</strong> of the children have completed,
	and then exits as well.
</li><br/>

<li> Threads are also commonly referred to as
	<ol class="answer_list">
	<li> hyperthreads
	<li> lightweight processes
	<li> execution traces
	<li> shared libraries
	<li> None of the above
	</ol>
</li><br/>
<li> When compared to processes, threads
	<ol class="answer_list">
	<li> are more limited in the size of their address space.
	<li> cannot be used for multiprogramming.
	<li> take the same amount of time for a context switch.
	<li> can be created and destroyed 10-100 times faster.
	<li> None of the above
	</ol>
</li><br/>
<li> Each thread <strong>must</strong> maintain its own
	<ol class="answer_list">
	<li> address space.
	<li> program counter.
	<li> run-time stack.
	<li> heap storage area.
	<li> signal handlers.
	<li> state (blocked, ready, running).
	<li> None of the above
	</ol>
</li><br/>
<li> Every process contains at least
	<ol class="answer_list">
	<li> one thread.
	<li> two threads, one each for execution and garbage collection.
	<li> three threads, for execution, signal management, and garbage
		collection.
	<li> one child process.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following items belong to the process (rather than to
	each of its threads)?
	<ol class="answer_list">
	<li> Program counter
	<li> Address space
	<li> Static variables
	<li> Run-time stack
	<li> Heap storage
	<li> Registers
	<li> File descriptors
	<li> Signal handlers
	<li> State (blocked, ready, running)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following items belong to each thread (rather than to
	the process)?
	<ol class="answer_list">
	<li> Program counter
	<li> Address space
	<li> Static variables
	<li> Run-time stack
	<li> Heap storage
	<li> Registers
	<li> File descriptors
	<li> Signal handlers
	<li> State (blocked, ready, running)
	<li> None of the above
	</ol>
</li><br/>
<li> Because the address space is shared by the process' threads, care must
	be taken (to avoid race conditions) when
	<ol class="answer_list">
	<li> calling the same function from multiple threads.
	<li> reading/setting values for the same static variables and heap
		allocated objects from multiple threads.
	<li> creating new threads.
	<li> a thread terminates.
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX pthread library call that creates a new thread is called
	<ol class="answer_list">
	<li> pthread_new
	<li> pthread_fork
	<li> pthread_create
	<li> pthread_dup
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX pthread library call that returns the thread's identification is
	called
	<ol class="answer_list">
	<li> pthread_getid
	<li> pthread_self
	<li> pthread_pid
	<li> pthread_thread_id
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX pthread library call that terminates the calling thread is called
	<ol class="answer_list">
	<li> pthread_exit
	<li> pthread_terminate
	<li> pthread_yield
	<li> pthread_return
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX pthread library call that blocks the calling thread, resuming
	when the indicated thread completes, is called
	<ol class="answer_list">
	<li> pthread_wait
	<li> pthread_block
	<li> pthread_pause
	<li> pthread_join
	<li> None of the above
	</ol>
</li><br/>
<li> The POSIX pthread library call that moves a running thread to the blocked
	state is called
	<ol class="answer_list">
	<li> pthread_block
	<li> pthread_yield
	<li> pthread_stop
	<li> pthread_pause
	<li> None of the above
	</ol>
</li><br/>
<li> Threads that are managed in user/process space
	<ol class="answer_list">
	<li> require the OS to be thread aware.
	<li> require the process to keep track of the threads via a thread
		table (in addition to the process table that the OS maintains).
	<li> enable context switching that is about 10x faster than
		kernel space threads.
	<li> cause the entire process to block if any one of its threads blocks
		for a system resource.
	<li> None of the above
	</ol>
</li><br/>
<li> Threads that are managed in kernel space
	<ol class="answer_list">
	<li> requires the OS to be thread aware.
	<li> requires the process to keep track of the threads via a thread
		table (in addition to the process table that the OS maintains).
	<li> enables context switching that is about 10x faster than
		user/process space threads.
	<li> cause the entire process to block if any one of its threads blocks
		for a system resource.
	<li> None of the above
	</ol>
</li><br/>
<li> Thread pools are collections of already allocated/created threads that
	<ol class="answer_list">
	<li> are assigned as requested to reduce the costs of thread creation
		and destruction.
	<li> are only beneficial for user/process space managed threads.
	<li> are only beneficial for kernel space managed threads.
	<li> prevent an over abundance of threads, if the pool is the only
		means for acquiring a thread.
	<li> None of the above
	</ol>
</li><br/>
<li> Hybrid User-Kernel space thread management allocates
	<ol class="answer_list">
	<li> one or more kernel threads to a process, with the process able
		to start multiple user threads within each kernel thread.
	<li> exactly one user space and one kernel space thread to each
		process.
	<li> any number of user space and kernel space threads to each process
		as long as they are requested from the corresponding user and
		kernel thread pools.
	<li> only user space threads, but enables the kernel to recognize when
		a user thread is blocked.
	<li> None of the above
	</ol>
</li><br/>
<li> Scheduler activations use
	<ol class="answer_list">
	<li> kernel space threads, but only the process schedules them.
	<li> kernel space threads, but the process recommends to the OS
		which thread to schedule/run next.
	<li> user space threads, but the kernel recognizes when a blocked
		thread doesn't prevent other threads from running.
	<li> both user and kernel space threads for a process, with each
		thread type being scheduled as it normally would be.
	<li> None of the above
	</ol>
</li><br/>

<li> The key issues for interprocess communication are
	<ol class="answer_list">
	<li> message passing.
	<li> sharing information.
	<li> semaphores.
	<li> sequencing process interactions.
	<li> None of the above
	</ol>
</li><br/>
<li> A race condition exists when
	<ol class="answer_list">
	<li> two or more processes share the same variable.
	<li> only user space threads are used for multiprogramming.
	<li> only kernel space threads are used for multiprogramming.
	<li> different execution orderings of instructions in multiple
		threads/processes can produce different results.
	<li> None of the above
	</ol>
</li><br/>
<li> Race conditions can occur when
	<ol class="answer_list">
	<li> two or more threads/processes can read (but <strong>not</strong>
		change) the same variable(s).
	<li> two or more threads/processes can read and write the same
		variable(s) via UNinterruptible actions.
	<li> two or more threads/processes can read and write the same
		variable(s) via interruptible actions.
	<li> when there is a mix of user space and kernel space threads
		within the same process, but no shared variable(s).
	<li> None of the above
	</ol>
</li><br/>
<li> A critical section is a portion of executable code
	<ol class="answer_list">
	<li> that makes system calls.
	<li> in which only one thread (or process) should be active at a time.
	<li> that is protected by one or more semaphores.
	<li> which is shared by multiple threads.
	<li> None of the above
	</ol>
</li><br/>
<li> Critical sections should be as small as possible because
	<ol class="answer_list">
	<li> larger critical sections run more slowly.
	<li> this reduces the amount of process blocking.
	<li> there is a maximum code size for which semaphores will work.
	<li> they <strong>must</strong> run completely through
		<strong>without</strong> blocking.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following help to prevent difficulties when simultaneous
	threads/processes share data?
	<ol class="answer_list">
	<li> No 2 threads/processes are in their critical sections at the
		same time.
	<li> No assumptions are made about process execution speeds.
	<li> No thread/process should wait arbitrarily long to enter its
		critical section.
	<li> No thread/process outside its critical section should block other
		threads/processes.
	<li> None of the above
	</ol>
</li><br/>

<li> Allowing processes to disable interrupts to prevent their being interrupted
	while in their critical section(s) is <strong>not</strong> a desirable
	solution since
	<ol class="answer_list">
	<li> it requires process threads to run in kernel space.
	<li> it requires process threads to run in user space.
	<li> a user program could use this to hog the CPU.
	<li> it fails to work if there are multiple CPUs.
	<li> None of the above
	</ol>
</li><br/>
<li> Strict alternation of processes can be accomplished by setting and
	repeatedly checking a shared variable (i.e., spin locking), but this
	<ol class="answer_list">
	<li> is only suitable if the expected wait time is very short.
	<li> solution only works for two processes (or threads).
	<li> solution only works if there is only a single CPU involved.
	<li> is wasteful of the CPU resource.
	<li> None of the above
	</ol>
</li><br/>
<li> Peterson's solution uses what mechanism(s) to implement mutual exclusion
	for a critical section?
	<ol class="answer_list">
	<li> disabling interrupts.
	<li> kernel space threads.
	<li> lock variable(s).
	<li> busy waiting.
	<li> None of the above
	</ol>
</li><br/>
<li> A test and set instruction is special because it
	<ol class="answer_list">
	<li> runs with interrupts disabled.
	<li> does the value copy and set as a single indivisible action.
	<li> works by performing the busy wait within a single instruction.
	<li> locks the memory bus (for multiple CPU systems).
	<li> None of the above
	</ol>
</li><br/>
<li> The priority inversion problem occurs when a busy waiting solution
	for critical section access is used and
	<ol class="answer_list">
	<li> the busy waiting process/thread has a higher priority.
	<li> the busy waiting process/thread has a lower priority.
	<li> the busy waiting process/thread is a user space thread.
	<li> the busy waiting process/thread is a kernel space thread.
	<li> None of the above
	</ol>
</li><br/>
<li> Fill in the rest of the code that implements Peterson's solution:
<pre><code>
	int turn;
	int interested[2];	// Solution for 2 processes.

	void enterRegion(int process) {
		int other = 1 - process;	// Other process.

		// Missing Code

	}

	void leaveRegion(int process) {
		interested[process] = 0;	// process NOT interested.
	}
</code></pre>
</li><br/>
<li> The key feature(s) of the TSL instruction which allows it to implement
	critical section access is
	<ol class="answer_list">
	<li> only being callable by the OS (and <strong>not</strong> by user
		programs).
	<li> saving the current value of a register while the register's
		value is set to a new value.
	<li> blocks if the value being assigned is different from the current
		value of the register.
	<li> that it is atomic (i.e., cannot be interrupted once begun).
	<li> None of the above
	</ol>
</li><br/>
<li> Fill in the rest of the code that implements a busy-waiting solution using
	the TSL (test and set lock) instruction:
<pre><code>
	enterRegion:

		// Missing Code

		JNE enterRegion ; if register N isn't 0 then loop ("lock" was set)
		RET		; if register N is 0 then enter critical section

	leaveRegion:

		// Missing Code

		RET
</code></pre>
</li><br/>

<li> Assuming that <em>sleep</em> (causing the calling process to block) and
	<em>wakeup</em> (which unblocks the indicated process) are system
	calls, then they can be used to implement critical sections if the
	following condition(s) are kept:
	<ol class="answer_list">
	<li> <em>wakeup</em> signals are <strong>not</strong> buffered.
	<li> <em>sleep</em> cannot be called by an already blocked process.
	<li> only strict alternation of critical sections is implemented.
	<li> <em>wakeup</em> is <strong>never</strong> called first.
	<li> None of the above
	</ol>
</li><br/>
<li> The <em>sleep</em> (causing the calling process to block) and
        <em>wakeup</em> (which unblocks the indicated process) calls enable
	<ol class="answer_list">
	<li> strictly alternating critical sections to be protected
		<strong>without</strong> busy waiting.
	<li> strictly alternating critical sections to be protected but only
		when a test and set instruction is available.
	<li> the protection of any critical section so long as
		<em>wakeup</em> is <strong>never</strong> called first.
	<li> the protection of any critical section so long as
		<em>sleep</em> is <strong>never</strong> called twice in a row.
	<li> None of the above
	</ol>
</li><br/>
<li> The <em>sleep</em> (causing the calling process to block) and
        <em>wakeup</em> (which unblocks the indicated process) calls, can
	only support strictly alternating critical sections because
	<ol class="answer_list">
	<li> <em>sleep</em> only blocks a process for a maximum period of
		time.
	<li> <em>sleep</em> cannot be called by an already blocked process.
	<li> <em>wakeup</em> cannot unblock a process that called <em>sleep</em>
		twice in a row.
	<li> <em>wakeup</em> calls are <strong>not</strong> buffered.
	<li> None of the above
	</ol>
</li><br/>
<li> Indicate where the <em>sleep()</em> and <em>wakeup(...)</em> calls should
	go (with the correct argument for <em>wakeup</em>) in the following
	code to allow alternation of producer and consumer actions. [Note: Some
	options may be used more than once, or not at all.]
<pre><code>
	void producer() {
		while (1) {
			ITEM item = produceItem();
			insertItem(item);

			// Insert statement(s) for section "A" here.

		}
	}

	void consumer() {
		while (1) {

			// Insert statement(s) for section "B" here.

			ITEM item = removeItem();
			consumeItem(item);

			// Insert statement(s) for section "C" here.

		}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> sleep();
		<li> wakeup(producer);
		<li> sleep(); wakeup(producer);
		<li> wakeup(producer); sleep();
		<li> wakeup(consumer);
		<li> sleep(); wakeup(consumer);
		<li> wakeup(consumer); sleep();
		</ol>
	</td></tr></table>
</li><br/>

<li> Semaphores were introduced in 1965 by 
	<ol class="answer_list">
	<li> Gary Peterson
	<li> Andrew Tannenbaum
	<li> Edgar Dijkstra
	<li> Alan Turing
	<li> None of the above
	</ol>
</li><br/>
<li> The P (DOWN) semaphore operation is <strong>best</strong> described by
	<ol class="answer_list">
	<li> count--; if (count <= 0) { sleep(); }
	<li> sleep(); if (count <= 0) { count--; }
	<li> if (count <= 0) { count--; } sleep();
	<li> if (count <= 0) { sleep(); } count--;
	<li> None of the above
	</ol>
</li><br/>
<li> The V (UP) semaphore operation is <strong>best</strong> described by
	<ol class="answer_list">
	<li> if (count == 1) { wakeup(sleeping_process); } count++;
	<li> if (count == 1) { count++; } wakeup(sleeping_process);
	<li> wakeup(sleeping_process); if (count == 1) { count++; }
	<li> count++; if (count == 1) { wakeup(sleeping_process); }
	<li> None of the above
	</ol>
</li><br/>
<li> The starting value (count) of a semaphore indicates the number of
	<ol class="answer_list">
	<li> total times that the semaphore can be used (e.g., P/DOWN
		operations).
	<li> available items of that resource.
	<li> different types of resources.
	<li> concurrent processes (but <strong>not</strong> threads) that can
		share the resource(s).
	<li> None of the above
	</ol>
</li><br/>
<li> If semaphores are compared to having copies of a book in the library, then
	<ol class="answer_list">
	<li> P/DOWN is the equivalent of checking out a book.
	<li> P/DOWN is the equivalent of returning a book.
	<li> V/UP is the equivalent of checking out a book.
	<li> V/UP is the equivalent of returning a book.
	<li> None of the above
	</ol>
</li><br/>
<li> The semaphore operations of P/DOWN and V/UP <strong>must</strong>
	<ol class="answer_list">
	<li> be system calls.
	<li> behave atomically (i.e., uninterruptible).
	<li> be implemented in the OS kernel.
	<li> be implemented by the computer hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> A mutex is a binary semaphore that
	<ol class="answer_list">
	<li> is implemented using lock variables.
	<li> uses <em>sleep</em> and <em>wakeup</em> calls for its
		implementation.
	<li> is implemented using monitors.
	<li> is optimized for having only two values/states.
	<li> None of the above
	</ol>
</li><br/>
<li> Indicate where the <em>down(&mutex)</em> and <em>up(&mutex)</em> calls
	should go in the following code to allow interleaving of producer
	and consumer actions, where "insertItem" and "removeItem" are
	operations on a shared buffer.
	[Note: Some options may be used more than once, or not at all.]
<pre><code>
	int mutex = 1;

	void producer() {
		while (1) {
			ITEM item = produceItem();

			// Insert statement(s) for section "A" here.

			insertItem(item);

			// Insert statement(s) for section "B" here.

		}
	}

	void consumer() {
		while (1) {

			// Insert statement(s) for section "C" here.

			ITEM item = removeItem();

			// Insert statement(s) for section "D" here.

			consumeItem(item);
		}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> down(&mutex);
		<li> up(&mutex);
		<li> down(&mutex); up(&mutex);
		<li> up(&mutex); down(&mutex);
		</ol>
	</td></tr></table>
</li><br/>
<li> Write a C program that takes a single command line argument which
	indicates the initial value of a shared counter (called "counter").
	The program should use 5 threads to eventually reduce the value of
	"counter" down to 0. Each thread will reduce "counter" by 1, then
	sleep for 1 second before continuing to decrease "counter".  The
	"counter" value should NOT go below 0 and the program should exit
	once "counter" reaches 0. You may use the following library calls to
	help write your program: <em>pthread_create</em>, <em>pthread_exit</em>,
	<em>pthread_mutex_lock</em>, <em>pthread_mutex_unlock</em>,
	<em>ptrhead_mutex_init</em>.  You can write your code without the usual
	"include"s or the use of <em>pthread_attr_init</em> and
	<em>pthread_attr_setscope</em>.
</li><br/>

<li> Monitors, unlike semaphores, <strong>must</strong> be implemented as
	<ol class="answer_list">
	<li> a (system) library so that different languages can share the
		same implementation.
	<li> part of the programming language design.
	<li> part of the OS design.
	<li> a feature within the computer hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> Monitors are most easily adapted to programming languages that support
	<ol class="answer_list">
	<li> classes (e.g., Java).
	<li> records/structures (e.g., C).
	<li> functions (e.g., C, Cobol, Fortran).
	<li> conditional looping constructs, like "while" (e.g., C, Java).
	<li> None of the above
	</ol>
</li><br/>
<li> What feature in Java can be used to implement a monitor?
	<ol class="answer_list">
	<li> virtualized
	<li> try-catch
	<li> finalized
	<li> synchronized
	<li> None of the above
	</ol>
</li><br/>
<li> A different semaphore associated with each object instance can be used
	to implement a monitor by having each
	<ol class="answer_list">
	<li> object do P/DOWN when it is created and a V/UP
		when it is destroyed.
	<li> object do V/UP when it is created and a P/DOWN
		when it is destroyed.
	<li> object method do P/DOWN when begun, and V/UP just before returning.
	<li> object method do V/UP when begun, and P/DOWN just before returning.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following interprocess communication mechanisms is well
	suited for distributed sharing (i.e., across multiple computer systems)?
	<ol class="answer_list">
	<li> lock variables
	<li> semaphores
	<li> monitors
	<li> message passing
	<li> None of the above
	</ol>
</li><br/>
<li> Message passing, like semaphores (and unlike monitors),
	<ol class="answer_list">
	<li> can easily be implemented as a (system) library so that different
		languages can share the same implementation.
	<li> <strong>must</strong> be part of the programming language design.
	<li> <strong>must</strong> be part of the OS design.
	<li> <strong>must</strong> be a feature within the computer hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> The "send" and "receive" system calls for message passing generally
	<ol class="answer_list">
	<li> <strong>must</strong> be used in conjunction with semaphores to
		perform the coordination (e.g., to ensure the "receive"r
		is ready and waiting when the "send" is done).
	<li> can only be used when the communicating processes are on
		different systems.
	<li> are considered impractically for efficiency reasons, and thus
		are seldom used.
	<li> <strong>must</strong> indicate the message contents and
		destination (for "send") and a place to store the message
		and the source (for "received").
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are typical design issues associated with messages?
	<ol class="answer_list">
	<li> Handling lost (or missing) messages.
	<li> Minimum message size.
	<li> Maximum time between messages (not counting acknowledgements).
	<li> Messages are received in the order they were sent.
	<li> None of the above
	</ol>
</li><br/>
<li> Indicate where the <em>send</em> and <em>receive</em> calls should go (with
	the appropriate arguments) in the following code to allow alternation of
	producer and consumer actions.
	[Note: Some options may be used more than once, or not at all.]
<pre><code>
	void producer() {
		MESSAGE msg;
		while (1) {
			ITEM item = produceItem();
			addItem(&msg, item);

			// Insert statement(s) for section "A" here.

		}
	}

	void consumer() {
		MESSAGE msg;
		while (1) {

			// Insert statement(s) for section "B" here.

			ITEM item = removeItem(msg);

			// Insert statement(s) for section "C" here.

			consumeItem(item);
		}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> send(consumer, &msg);
		<li> send(producer, &msg);
		<li> receive(consumer, &msg);
		<li> receive(producer, &msg);
		<li> send(consumer, &msg); receive(producer, &msg);
		<li> send(producer, &msg); receive(consumer, &msg);
		<li> receive(consumer, &msg); send(producer, &msg);
		<li> receive(producer, &msg); send(consumer, &msg);
		</ol>
	</td></tr></table>
</li><br/>
<li> A popular system that implements message passing, developed in the
	early 1990s, and available for a variety of different languages
	(e.g., Java, Python, C) is called
	<ol class="answer_list">
	<li> The Messaging Library (TML)
	<li> Message Passing Interface (MPI)
	<li> Distributed Messaging Library (DML)
	<li> Distributed Process Communication (DPC)
	<li> None of the above
	</ol>
</li><br/>
<li> When a message passing system causes the sender to block until the
	receiver is ready (and vice versa), these semantics are called a
	<ol class="answer_list">
	<li> rendezvous.
	<li> date.
	<li> coupling.
	<li> synch-up.
	<li> None of the above
	</ol>
</li><br/>
<li> The mechanism in POSIX systems that provides a maximum size "mailbox"
	that causes the writing/sending process to block once the "mailbox"
	becomes full (allowing the writer/sender to proceed only when there
	is room in the "mailbox" - perhaps due to the receiver reading/removing
	items), is called
	<ol class="answer_list">
	<li> mail.
	<li> spooling.
	<li> pipes.
	<li> conduits.
	<li> None of the above
	</ol>
</li><br/>
<li> Write a C program that creates a single child process. The parent process
	reads in numbers (separated by white-space) from stdin and sends them
	to the child process using a pipe. The child process then writes the
	numbers to stdout (one number per line).
</li><br/>

<li> Barriers are useful when a number of different threads/processes need to
	<ol class="answer_list">
	<li> take turns working on a common task.
	<li> <strong>must all</strong> meet at a common place before
		proceeding to the next phase of a computation.
	<li> share a set of common variables/objects.
	<li> none of the threads/processes are allowed to go past a
		certain point in the computation.
	<li> None of the above
	</ol>
</li><br/>
<li> If <strong>all</strong> the threads/processes take very close to the same
	amount of time to reach the barrier, then a good way of implementing
	the barrier is
	<ol class="answer_list">
	<li> spin locks.
	<li> semaphores.
	<li> monitors.
	<li> message passing.
	<li> None of the above
	</ol>
</li><br/>
<li> The "thread" function below is run multiple times simultaneously (each
	instance is passed a different value of "i" from 0 to N-1). The
	"barrier" function ensures that <strong>all</strong>
	the threads wait until <strong>all</strong> have
	reached this point before proceeding. Use "count", and the "down"
	and "up" operations on both the N "mutex_thread" (declared
	as an array) and "mutex_count" semaphores to implement the
	"barrier" function semantics.
<pre><code>
        pthread_mutex_t mutex_thread[N];
        pthread_mutex_t mutex_count;
        int count = 0;

        int main(...) {
                ...
                int thread_num;
                pthread_mutex_init(&mutex_count);
                for (thread_num = 0; thread_num < N; thread_num++) {
                        pthread_t thread_id;
                        pthread_mutex_init(&mutex_thread[thread_num]);
                        pthread_mutex_lock(&mutex_thread[thread_num]);
                        pthread_create(&thread_id, thread, thread_num)
                }
                ...
        }

        void thread(int i) {
                while (1) {
                        do_something(i);
                        barrier(i);
                        do_something_else(i);
                        barrier(i);
                }
        }
        void barrier(int i) {

                // MISSING CODE

        }
</code></pre>
</li><br/>

<li> The Dining Philosophers Problem is a synchronization problem developed
	by Edgar Dijkstra to
	<ol class="answer_list">
	<li> show how monitors can handle situations that semaphores cannot.
	<li> show how semaphores can handle situations that message
		passing cannot.
	<li> demonstrate the use of semaphores.
	<li> demonstrate the use of message passing.
	<li> None of the above
	</ol>
</li><br/>
<li> The Dining Philosophers Problem is comprised of
	<ol class="answer_list">
	<li> 5 philosophers each with a plate of slippery pasta.
	<li> philosophers needing to take turns eating, going around the
		table in a clockwise fashion.
	<li> 5 forks, one between each philosopher.
	<li> philosophers needing 2 forks to eat.
	<li> None of the above
	</ol>
</li><br/>
<li> If no process within a group of processes is able to run because they
	are <strong>all</strong> in the blocked state, and each is waiting
	for the other to do something before it can continue, this situation
	is called
	<ol class="answer_list">
	<li> starvation.
	<li> deadlock.
	<li> livelock.
	<li> mutual stall.
	<li> None of the above
	</ol>
</li><br/>
<li> If <strong>all</strong> the processes within a group of processes is able
	to run (i.e., none are in the blocked state), but none is able to
	accomplish any useful work for lack of a resource, this situation is
	called
	<ol class="answer_list">
	<li> starvation.
	<li> deadlock.
	<li> livelock.
	<li> resource hogging.
	<li> None of the above
	</ol>
</li><br/>
<li> In the Dining Philosophers Problem, if each philosopher grabs a fork and
	holds it until they are able to grab a second fork, this
	<ol class="answer_list">
	<li> leads to starvation.
	<li> leads to deadlock.
	<li> leads to livelock.
	<li> provides a correct solution.
	<li> None of the above
	</ol>
</li><br/>
<li> In the Dining Philosophers Problem, if each philosopher grabs the fork
	to their right, then seeing that the left fork is unavailable (having
	been grabbed by the philosopher to their left) puts down the fork they
	are holding, counts to 10, then tries again. This algorithm
	<ol class="answer_list">
	<li> leads to livelock.
	<li> leads to deadlock.
	<li> leads to resource hogging.
	<li> provides a correct solution.
	<li> None of the above
	</ol>
</li><br/>
<li> The key to one working solution of the Dining Philosophers Problem is to
	have philosophers exist in one of three states (THINING, HUNGRY, EATING)
	and that when a philosopher is
	<ol class="answer_list">
	<li> finished EATING, she ensures that any HUNGRY philosopher sitting
		next to her gets an opportunity to eat.
	<li> HUNGRY, she waits until each of her neighbors is THINKING before
		trying to pick up forks.
	<li> THINKING, she waits until a neighbor is also THINKING before
		becoming HUNGRY.
	<li> finished THINKING, she waits until one of her neighbors is EATING
		before becoming HUNGRY.
	<li> None of the above
	</ol>
</li><br/>

<li> The Readers and Writers Problem involves multiple processes trying to
	read or write to the same shared variable, in which 
	<ol class="answer_list">
	<li> only one reader or writer can access the variable at a time.
	<li> only one reader or multiple writers can access the variable
		at a time.
	<li> multiple readers or a single writer can access the variable
		at a time.
	<li> multiple readers or multiple writers can access the variable
		at a time.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary types of solutions to the Readers and Writers Problem are
	when
	<ol class="answer_list">
	<li> writers are given preference over readers.
	<li> readers are given preference over writers.
	<li> readers and writers have simultaneous access so that no
		preference is given to either one.
	<li> readers and writers strictly alternate, with every read followed
		by a write and every write followed by a read.
	<li> None of the above
	</ol>
</li><br/>
<li> Indicate where the <em>down(...)</em> and <em>up(...)</em> calls
	should go in the following code to allow a single reader or a single
	writer to access the shared database <strong>without</strong> giving
	either readers or writers any preference.
	[Note: Some options may be used more than once, or not at all.]
<pre><code>
	int mutex = 1;

	void reader() {
		while (1) {

			// Insert statement(s) for section "A" here.

			readDatabase();

			// Insert statement(s) for section "B" here.

		}
	}

	void writer() {
		while (1) {
			acquireData();

			// Insert statement(s) for section "C" here.

			writeDatabase();

			// Insert statement(s) for section "D" here
}}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> down(&mutex);
		<li> up(&mutex);
		<li> up(&mutex); down(&mutex);
		<li> down(&mutex); up(&mutex);
		</ol>
	</td></tr></table>
</li><br/>
<li> Indicate where the <em>down(...)</em> and <em>up(...)</em> calls
	should go in the following code to give readers preference over
	writers for access to the shared database.
	[Note: Some options may be used more than once, or not at all.]
<pre><code>
	int db_mutex = 1;
	int counter_mutex = 1;
	int counter = 0;

	void reader() {
		while (1) {

			// Insert statement(s) for section "A" here.

			if (1 == ++counter) {

				// Insert statement(s) for section "B" here.

			}


			// Insert statement(s) for section "C" here.

			readDatabase();


			// Insert statement(s) for section "D" here.

			if (0 == --counter) {

				// Insert statement(s) for section "E" here.

			}

			// Insert statement(s) for section "F" here.

		}
	}

	void writer() {
		while (1) {
			acquireData();

			// Insert statement(s) for section "G" here.

			writeDatabase();

			// Insert statement(s) for section "H" here.

		}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> up(&db_mutex);
		<li> up(&counter_mutex);
		<li> down(&db_mutex);
		<li> down(&counter_mutex);
		</ol>
	</td></tr></table>
</li><br/>
<li> Indicate where the <em>down(...)</em> and <em>up(...)</em> calls
	should go in the following code to give writers preference over
	readers for access to the shared database.
	[Note: Some options may be used more than once, or not at all.]
<pre><code>
	int read_mutex = 1;
	int write_mutex = 1;
	int counter_mutex = 1;
	int counter = 0;

	void reader() {
		while (1) {

			// Insert statement(s) for section "A" here.

			readDatabase();

			// Insert statement(s) for section "B" here.

		}
	}

	void writer() {
		while (1) {
			acquireData();


			// Insert statement(s) for section "C" here.

			if (1 == ++counter) {


				// Insert statement(s) for section "D" here.


			}

			// Insert statement(s) for section "E" here.


			writeDatabase();


			// Insert statement(s) for section "F" here.

			if (0 == --counter) {


				// Insert statement(s) for section "G" here.


			}

			// Insert statement(s) for section "H" here.

		}
	}
</code></pre>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> /* No Operation */
		<li> up(&counter_mutex);
		<li> up(&read_mutex);
		<li> up(&write_mutex);
		<li> down(&counter_mutex);
		<li> down(&read_mutex);
		<li> down(&write_mutex);
		<li> up(&counter_mutex); down(&read_mutex);
		<li> up(&read_mutex); down(&counter_mutex);
		<li> up(&counter_mutex); down(&write_mutex);
		<li> up(&write_mutex); down(&counter_mutex);
		</ol>
	</td></tr></table>
</li><br/>

<li> The scheduler is that part of the OS that
	<ol class="answer_list">
	<li> starts a new process.
	<li> manages the CPU.
	<li> determines when a blocked process becomes ready.
	<li> determines when a running process blocks.
	<li> None of the above
	</ol>
</li><br/>
<li> A process scheduling algorithm should try to address the following
	concerns:
	<ol class="answer_list">
	<li> memory availability for the process to use.
	<li> fairness, so that every process gets their fair share.
	<li> the efficient use of the CPU.
	<li> ensure availability of peripheral resources.
	<li> maximize the number of processes completed per hour.
	<li> minimize response time for interactive users.
	<li> None of the above
	</ol>
</li><br/>
<li> The ability of the OS to switch from running one process to another
	process VERY quickly is important because it
	<ol class="answer_list">
	<li> reduces the chance of an OS bug causing a problem.
	<li> increases the efficient use of the CPU.
	<li> helps reduce the response time for interactive users.
	<li> ensures peripheral resource availability.
	<li> None of the above
	</ol>
</li><br/>
<li> Regarding the set of concerns that a process scheduling algorithm
	should address,
	<ol class="answer_list">
	<li> <strong>all</strong> of them can be effectively address by a
		single algorithm.
	<li> an OS should use two or more scheduling algorithms alternately
		so as to address <strong>all</strong> of the concerns.
	<li> only a couple of the concerns are truly important, and the
		remainder can be safely ignored.
	<li> some of the concerns are contradictory, so invariably
		<strong>not all</strong> concerns can be addressed by the OS.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to process scheduling, it is helpful to view a process as
	<ol class="answer_list">
	<li> being primarily CPU bound.
	<li> being primarily I/O bound.
	<li> an intense period of CPU usage. 
	<li> alternating sequences of CPU usage and waiting for I/O.
	<li> None of the above
	</ol>
</li><br/>
<li> A CPU bound process is one in which
	<ol class="answer_list">
	<li> it spends most of its time in the ready or running state.
	<li> it spends most of its time in the blocked state.
	<li> once it obtains the CPU, it runs to completion.
	<li> most of the time it's waiting for the memory to be available.
	<li> None of the above
	</ol>
</li><br/>
<li> An I/O bound process is one in which
	<ol class="answer_list">
	<li> it spends most of its time in the ready or running state.
	<li> it spends most of its time in the blocked state.
	<li> once it obtains the CPU, it runs to completion.
	<li> most of the time it's waiting for the memory to be available.
	<li> None of the above
	</ol>
</li><br/>
<li> Preemptive scheduling is when a process
	<ol class="answer_list">
	<li> spends most of its time in the ready or running state.
	<li> spends most of its time in the blocked state.
	<li> can be booted out of the CPU by the scheduler, before the
		process is finished.
	<li> can be booted out of the CPU by another user process, before the
		process is finished.
	<li> None of the above
	</ol>
</li><br/>
<li> Non-preemptive scheduling is when a process
	<ol class="answer_list">
	<li> spends most of its time in the ready or running state.
	<li> spends most of its time in the blocked state.
	<li> <strong>never</strong> becomes blocked.
	<li> <strong>always</strong> runs to termination once it's in the CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of process scheduling is <strong>best</strong> suited
	for interactive processing?
	<ol class="answer_list">
	<li> non-preemptive.
	<li> dedicated.
	<li> busy wait.
	<li> preemptive.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are batch process scheduling algorithms?
	<ol class="answer_list">
	<li> Priority Scheduling.
	<li> First Come, First Serverd.
	<li> Guaranteed Scheduling.
	<li> Shortest Remaining Time.
	<li> None of the above
	</ol>
</li><br/>
<li> The following steps describe which process scheduling algorithm?
	<blockquote>
	<ul class="bullet_list">
	<li> Processes are added to a single queue in the order they are
		started.
	<li> The process at the front of the queue is run until it either
		completes, or blocks.
	<li> Once a process becomes ready again (after being blocked), it joins
		 the end of the queue.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Round Robin Scheduling.
	<li> First Come, First Served.
	<li> Guaranteed Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following process scheduling algorithms needs to know the
	typical amount of time that a process often takes to complete?
	<ol class="answer_list">
	<li> Guaranteed Scheduling.
	<li> Shortest Job First.
	<li> Priority Scheduling.
	<li> Fair-Share Scheduling.
	<li> Shortest Remaining Time.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following process scheduling algorithms favors CPU bound
	over I/O bound processes?
	<ol class="answer_list">
	<li> First Come, First Served
	<li> Shortest Job First
	<li> Shortest Remaining Time
	<li> Guaranteed Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following process scheduling algorithms improves the average
	turnaround time on a batch system?
	<ol class="answer_list">
	<li> First Come, First Served
	<li> Shortest Job First
	<li> Shortest Remaining Time
	<li> Guaranteed Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following process scheduling algorithms improves the average
	throughput on a batch system?
	<ol class="answer_list">
	<li> First Come, First Served
	<li> Shortest Job First
	<li> Shortest Remaining Time
	<li> Guaranteed Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The following steps describe which process scheduling algorithm?
	<blockquote>
	<ul class="bullet_list">
	<li> Processes are chosen to run from the front of a single ready queue.
	<li> Each process can run a max time called a quantum.
	<li> Processes may block before <strong>all</strong> of their
		time quantum is used.
	<li> As processes become ready, they go to the end of the ready queue.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> First Come, First Served.
	<li> Round Robin Scheduling.
	<li> Priority Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The following steps describe which process scheduling algorithm?
	<blockquote>
	<ul class="bullet_list">
	<li> Each process has a priority.
	<li> The ready process with the highest priority runs next.
	<li> Priorities can be set statically and/or dynamically.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Round Robin Scheduling.
	<li> Guaranteed Scheduling.
	<li> Priority Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The following steps describe which process scheduling algorithm?
	<blockquote>
	<ul class="bullet_list">
	<li> Each queue corresponds to a different max time quantum to run.
	<li> If a process uses <strong>all</strong> of its time quantum,
		it's added to the end of the next highest quantum queue when
		it's moved to the ready state.
	<li> If a process blocks before exhausting its time quantum, it's added
		to the end of the next lowest quantum queue when it's moved to
		the ready state.
	<li> The processes in queues with larger quantum are run less
		frequently.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Round Robin Scheduling.
	<li> Multiple Queues.
	<li> Priority Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which process scheduling algorithm(s) are particularly well suited to
	<em>very</em> slow context switching?
	<ol class="answer_list">
	<li> Guaranteed Scheduling.
	<li> Multiple Queues.
	<li> Lottery Scheduling.
	<li> First Come, First Served.
	<li> None of the above
	</ol>
</li><br/>
<li> Which process scheduling algorithm(s) are particularly well suited to
	enforcing an advertised policy?
	<ol class="answer_list">
	<li> Multiple Queues.
	<li> Guaranteed Scheduling.
	<li> Lottery Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The following steps describe which process scheduling algorithm?
	<blockquote>
	<ul class="bullet_list">
	<li> Each process (in the ready queue) is given a (probably different)
		number of tickets.
	<li> When picking the next process to run, a ticket is chosen at random
		and the ready process with that ticket is run.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Multiple Queues.
	<li> Guaranteed Scheduling.
	<li> Lottery Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The <em>most</em> space efficient way of implementing Lottery Scheduling
	is to have
	<ol class="answer_list">
	<li> each ready process use a hash table to keep track of its randomly
		assigned tickets.
	<li> each ready process use a unsorted linked list of its randomly
		assigned tickets, performing a linear search to determine if
		it holds the "winning" ticket.
	<li> each ready process stores the range of tickets that it holds,
		so finding the process to run requires searching the queue of
		ready processes for the correct range that holds the "winning"
		ticket.
	<li> a common array used by <strong>all</strong> processes. Each array
		entry corresponds to a ticket, and records the process holding
		that ticket.
	<li> None of the above
	</ol>
</li><br/>
<li> The way of implementing Lottery Scheduling so that finding the process
	with the "winning" ticket is the <em>fastest</em> is to have
	<ol class="answer_list">
	<li> each ready process use a hash table to keep track of its randomly
		assigned tickets.
	<li> each ready process use a unsorted linked list of its randomly
		assigned tickets, performing a linear search to determine if
		it holds the "winning" ticket.
	<li> each ready process stores the range of tickets that it holds,
		so finding the process to run requires searching the queue of
		ready processes for the correct range that holds the "winning"
		ticket.
	<li> a common array used by <strong>all</strong> processes. Each array
		entry corresponds to a ticket, and records the process holding
		that ticket.
	<li> None of the above
	</ol>
</li><br/>
<li> Which process scheduling algorithm ensures that each user (rather than
	each process) gets an equal share of the CPU resource?
	<ol class="answer_list">
	<li> Multiple Queues.
	<li> Guaranteed Scheduling.
	<li> Lottery Scheduling.
	<li> Fair-Share Scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Real-time process scheduling is used
	<ol class="answer_list">
	<li> when processes <strong>must never</strong> block.
	<li> to ensure "quick" response to real world events.
	<li> to ensure the <strong>most</strong> efficient use of the
		CPU resource. 
	<li> when there is <strong>never</strong> more than one ready process
		at a time.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Hard</em> real-time systems
	<ol class="answer_list">
	<li> try to meet most deadlines, but terminate processes when its
		deadline is missed.
	<li> should meet <strong>all</strong> deadlines, but sometimes they
		are missed.
	<li> <strong>must</strong> meet <strong>all</strong> deadlines,
		otherwise something dire happens.
	<li> guarantees that <strong>all</strong> deadlines are met,
		no deadline is ever missed.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Soft</em> real-time systems
	<ol class="answer_list">
	<li> try to meet most deadlines, but terminate processes when its
		deadline is missed.
	<li> should meet <strong>all</strong> deadlines, but sometimes they
		are missed.
	<li> <strong>must</strong> meet <strong>all</strong> deadlines,
		otherwise something dire happens.
	<li> guarantees that <strong>all</strong> deadlines are met,
		no deadline is ever missed.
	<li> None of the above
	</ol>
</li><br/>
<li> The process scheduling policy and the scheduling mechanism are
	<ol class="answer_list">
	<li> different, with the policy choosing what to run next and the
		mechanism performing the context switch.
	<li> different, with mechanism used for batch systems and policy  
		used for real-time systems.
	<li> similar, with policy doing everything that mechanism does but
		adding to it support for non-preemptive scheduling.
	<li> actually different names for the same thing.
	<li> None of the above
	</ol>
</li><br/>
<li> Thread scheduling is
	<ol class="answer_list">
	<li> <strong>always</strong> done by the OS kernel.
	<li> <strong>always</strong> done by the process, when the process
		is running.
	<li> performed by both the kernel <em>and</em> by the process (if
		it is running).
	<li> accomplished by another aspect of the OS kernel using algorithms
		very different from those for process scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The following describes which type of thread scheduling?
	<blockquote>
	<ul class="bullet_list">
	<li> The kernel picks/schedules a process.
	<li> The scheduled process then schedules its own threads.
	<li> If a thread blocks for a system resource,
		then the entire process blocks and no other
		threads from the process can be run until the block is
		"cleared".
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Process scheduling
	<li> Kernel scheduling
	<li> Hybrid scheduling
	<li> Combined scheduling
	<li> None of the above
	</ol>
</li><br/>
<li> The following describes which type of thread scheduling?
	<blockquote>
	<ul class="bullet_list">
	<li> The kernel picks/schedules a thread (from any process).
	<li> If the thread blocks for a system resource,
		then the kernel picks/schedules another
		thread that can be from the same or another process.
	</ul>
	</blockquote>
	<ol class="answer_list">
	<li> Process scheduling
	<li> Kernel scheduling
	<li> Hybrid scheduling
	<li> Combined scheduling
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following thread scheduling mechanisms provides the fastest
	switching between threads from the same process?
	<ol class="answer_list">
	<li> Process scheduling
	<li> Kernel scheduling
	<li> Hybrid scheduling
	<li> Combined scheduling
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following thread scheduling mechanisms avoids a single
	blocked thread from preventing <strong>all</strong> of that
	process' threads from running?
	<ol class="answer_list">
	<li> Preemptive scheduling
	<li> Process scheduling
	<li> Kernel scheduling
	<li> Non-preemptive scheduling
	<li> None of the above
	</ol>
</li><br/>

<li> Which Unix command is used to display system documentation?
	<ol class="answer_list">
	<li> man
	<li> help
	<li> doc
	<li> system
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints out the current date and time?
	<ol class="answer_list">
	<li> time
	<li> date
	<li> today
	<li> now
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints out the calendar for the current month?
	<ol class="answer_list">
	<li> dates
	<li> year
	<li> cal
	<li> calendar
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command provides an arbitrary precision calculator?
	<ol class="answer_list">
	<li> cal
	<li> apc
	<li> rpn
	<li> bc
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints where a specified command is located?
	<ol class="answer_list">
	<li> find
	<li> where
	<li> which
	<li> locate
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command captures a transcript of the terminal session?
	<ol class="answer_list">
	<li> script
	<li> term
	<li> session
	<li> capture
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command lists the "real", user, and system time taken by
	another command?
	<ol class="answer_list">
	<li> date
	<li> time
	<li> timer
	<li> sw
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a common Unix command shell?
	<ol class="answer_list">
	<li> shell
	<li> sh
	<li> cmd
	<li> bash
	<li> term
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command is a commonly used full screen editor?
	<ol class="answer_list">
	<li> edit
	<li> fse
	<li> vi
	<li> ed
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command allows the filtering and transformation of text?
	<ol class="answer_list">
	<li> edit
	<li> fse
	<li> tr
	<li> sed
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints out the type of a specified file?
	<ol class="answer_list">
	<li> file
	<li> type
	<li> ft
	<li> kind
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the contents of one or more files to the screen?
	<ol class="answer_list">
	<li> print
	<li> cat
	<li> type
	<li> list
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command makes a copy of a file?
	<ol class="answer_list">
	<li> mv
	<li> dup
	<li> cp
	<li> copy
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command is used to rename a file or directory?
	<ol class="answer_list">
	<li> cp
	<li> rename
	<li> dup
	<li> mv
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command displays a file, a page at a time, on the screen?
	<ol class="answer_list">
	<li> more
	<li> page
	<li> type
	<li> list
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the first few lines of a file?
	<ol class="answer_list">
	<li> start
	<li> head
	<li> begin
	<li> first
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the last few lines of a file?
	<ol class="answer_list">
	<li> end
	<li> fini
	<li> tail
	<li> rest
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command can sort the lines of a file?
	<ol class="answer_list">
	<li> order
	<li> list
	<li> file
	<li> sort
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command shows the differences between two files?
	<ol class="answer_list">
	<li> diff
	<li> filecmp
	<li> compare
	<li> shdiff
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command returns the number of characters, words, and lines in
	a file?
	<ol class="answer_list">
	<li> count
	<li> wc
	<li> words
	<li> list
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command lists the contents of a directory?
	<ol class="answer_list">
	<li> list
	<li> files
	<li> ls
	<li> show
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix shell command changes the current working directory?
	<ol class="answer_list">
	<li> dir
	<li> pwd
	<li> cwd
	<li> cd
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the location of the current working directory?
	<ol class="answer_list">
	<li> pwd
	<li> show
	<li> whereami
	<li> location
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the lines of a file that match a specified
	pattern?
	<ol class="answer_list">
	<li> pat
	<li> grep
	<li> match
	<li> files
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command recursively visits a directory structure looking for
	files that match a set of criteria?
	<ol class="answer_list">
	<li> list
	<li> files
	<li> find
	<li> match
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command creates a new directory?
	<ol class="answer_list">
	<li> new
	<li> create
	<li> touch
	<li> mkdir
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command removes files (or directories)?
	<ol class="answer_list">
	<li> rm
	<li> del
	<li> purge
	<li> mv
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command reports the amount of free disk space remaining?
	<ol class="answer_list">
	<li> free
	<li> df
	<li> disksp
	<li> dstat
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command estimates the amount of disk currently being used?
	<ol class="answer_list">
	<li> usage
	<li> disk
	<li> du
	<li> dstat
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command prints the status of existing processes?
	<ol class="answer_list">
	<li> pstat
	<li> procs
	<li> ready
	<li> ps
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix command is used to send signals to processes?
	<ol class="answer_list">
	<li> kill
	<li> signal
	<li> send
	<li> ping
	<li> None of the above
	</ol>
</li><br/>

<li> The input redirection provided by the Unix "bash" shell enables
	<ol class="answer_list">
	<li> output from one command to be used as input to another command.
	<li> the contents of a file to be fed as input to a command (as if
		coming from the keyboard).
	<li> keyboard input to be captured within a file.
	<li> keyboard input to be fed directly to a running command.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are example(s) of input redirection using the Unix
	"bash" shell, where "data_file" is the name of a file containing
	input data and "command" is the name of a Unix executable?
	<ol class="answer_list">
	<li> data_file > command
	<li> data_file | command
	<li> command | data_file
	<li> command < data_file
	<li> None of the above
	</ol>
</li><br/>
<li> The output redirection provided by the Unix "bash" shell enables
	<ol class="answer_list">
	<li> output from one command to be used as input to another command.
	<li> the output of a command to be captured within a file.
	<li> the output of a command to be displayed on the screen.
	<li> keyboard "output" to be fed directly as input to a running command.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are example(s) of output redirection using the Unix
	"bash" shell, where "data_file" is the name of an output file
        and "command" is the name of a Unix executable?
	<ol class="answer_list">
	<li> data_file > command
	<li> data_file | command
	<li> command | data_file
	<li> command < data_file
	<li> None of the above
	</ol>
</li><br/>
<li> The Unix "bash" shell concept of pipe enables
	<ol class="answer_list">
	<li> the output of a command to be captured within a file.
	<li> output from one command to be used as input to another command.
	<li> the contents of a file to be fed as input to a command (as if
		coming from the keyboard).
	<li> both the input and output for a command to be associated with the
		same file.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are example(s) of pipe(s) using the Unix
	"bash" shell, where "data_file" is the name of a file containing
        data and "command" is the name of a Unix executable?
	<ol class="answer_list">
	<li> data_file > command
	<li> data_file | command
	<li> command | data_file
	<li> command < data_file
	<li> None of the above
	</ol>
</li><br/>
<li> The following Unix "bash" shell expression uses which of these
	capabilities, where "data_file" is the name of a file containing
        data?
<pre><code>
	cmd_1 < data_file | cmd_2 | cmd_3
</code></pre>
	<ol class="answer_list">
	<li> Input redirection
	<li> Output redirection
	<li> Pipe(s)
	<li> The expression is illegal
	<li> None of the above
	</ol>
</li><br/>
<li> The following Unix "bash" shell expression uses which of these
	capabilities, where "data_file" is the name of a file that can
        hold data?
<pre><code>
	cmd_1 | cmd_2 > data_file
</code></pre>
	<ol class="answer_list">
	<li> Input redirection
	<li> Output redirection
	<li> Pipe(s)
	<li> The expression is illegal
	<li> None of the above
	</ol>
</li><br/>
<li> The following Unix "bash" shell expression uses which of these
	capabilities, where "data_file_1" and "data_file_2" are the names
        of files that can hold data?
<pre><code>
	cmd_1 < data_file_1 | cmd_2 | cmd_3 > data_file_2
</code></pre>
	<ol class="answer_list">
	<li> Input redirection
	<li> Output redirection
	<li> Pipe(s)
	<li> The expression is illegal
	<li> None of the above
	</ol>
</li><br/>
<li> The following Unix "bash" shell expression uses which of these
	capabilities, where "data_file" is the name of a file containing
        data?
<pre><code>
	data_file > cmd_1 | cmd_2
</code></pre>
	<ol class="answer_list">
	<li> Input redirection
	<li> Output redirection
	<li> Pipe(s)
	<li> The expression is illegal
	<li> None of the above
	</ol>
</li><br/>
<li> In the Unix "bash" shell, to run a command in the background when it
	is first started
	<ol class="answer_list">
	<li> put a "|" at the end of the command line.
	<li> place a "&" at the end of the command line.
	<li> put the parts of the command line to run in the background
		between "<" and ">".
	<li> put the parts of the command line to run in the background
		between two "&".
	<li> None of the above
	</ol>
</li><br/>
<li> In the Unix "bash" shell, the job control action accomplished by
	^c (ctl-c) is to
	<ol class="answer_list">
	<li> terminate the foreground process.
	<li> terminate <strong>all</strong> background processes.
	<li> suspend the foreground process.
	<li> suspend <strong>all</strong> background processes.
	<li> None of the above
	</ol>
</li><br/>
<li> In the Unix "bash" shell, the job control action accomplished by
	^z (ctl-z) is to
	<ol class="answer_list">
	<li> terminate the foreground process.
	<li> terminate <strong>all</strong> background processes.
	<li> suspend the foreground process.
	<li> suspend <strong>all</strong> background processes.
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix "bash" shell command lists <strong>all</strong> (and only)
	the child processes of the current shell execution?
	<ol class="answer_list">
	<li> ps
	<li> list
	<li> procs
	<li> jobs
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix "bash" shell command causes a suspended/background process
	to run in the foreground?
	<ol class="answer_list">
	<li> fore
	<li> fg
	<li> &
	<li> job
	<li> None of the above
	</ol>
</li><br/>
<li> Which Unix "bash" shell command causes a suspended process to run in the
	background?
	<ol class="answer_list">
	<li> back
	<li> bg
	<li> &
	<li> job
	<li> None of the above
	</ol>
</li><br/>
<li> To get a listing of the signals which can be sent to a process in Unix,
	use this command
	<ol class="answer_list">
	<li> signal -list
	<li> kill -l
	<li> proc -sig
	<li> ps -l
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are true about Unix signals?
	<ol class="answer_list">
	<li> There is no mechanism to handle multiple signals of the same kind.
	<li> If a process doesn't specify a handler for a signal, then the OS
		will handle the signal using the default handler.
	<li> All signals have the same priority.
	<li> A process does <strong>not</strong> necessarily run immediately
		in the CPU when a signal is sent to it.
	<li> None of the above
	</ol>
</li><br/>

<li> The technique that allows data that flows from one "program piece" to
	the next "program piece" to remain in memory is known as
	<ol class="answer_list">
	<li> overflows.
	<li> overlays.
	<li> piecewise communication.
	<li> memory mapping.
	<li> None of the above
	</ol>
</li><br/>
<li> When the OS automatically breaks a process into multiple pieces of the
	same size, thus enabling processes otherwise too large to run within
	a smaller amount of memory, this is called
	<ol class="answer_list">
	<li> overlays.
	<li> memory mapping.
	<li> break out.
	<li> virtual memory.
	<li> None of the above
	</ol>
</li><br/>
<li> What is the most common virtual memory technique?
	<ol class="answer_list">
	<li> Overlays
	<li> Paging
	<li> Memory Mapping.
	<li> Swapping.
	<li> None of the above
	</ol>
</li><br/>
<li> The virtual address space of a process is
	<ol class="answer_list">
	<li> composed of those parts of main memory (e.g., RAM) holding
		parts of the process (e.g., code, run-time stack).
	<li> exactly the same size as the main memory (e.g., RAM) of the
		computer system.
	<li> larger than the maximum number of addressable bytes (e.g.,
		more than 2^32 bytes on a 32 bit architecture).
	<li> the address space the process would exist in if main memory
		(e.g., RAM) were big enough to hold the process, and the
		process was loaded into memory starting at address 0.
	<li> None of the above
	</ol>
</li><br/>
<li> Page frames are the
	<ol class="answer_list">
	<li> blocks in physical memory that hold virtual address space pages.
	<li> uniformly sized chunks that the virtual address space is divided
		into.
	<li> disk blocks that hold the virtual address space pages.
	<li> entries in the Memory Management Unit (MMU) that indicate which
		virtual address space page holds a desired address.
	<li> None of the above
	</ol>
</li><br/>
<li> What part of the computer system is responsible for converting a virtual
	address into a physical memory address?
	<ol class="answer_list">
	<li> Virtual Address Translation (VAT)
	<li> Memory Management Unit (MMU)
	<li> Arithmetic Logic Unit (ALU)
	<li> System Bus
	<li> None of the above
	</ol>
</li><br/>
<li> The standard implementation for a page (map) table of a process has an
	entry for each
	<ol class="answer_list">
	<li> page in the process' virtual address space.
	<li> page frame in physical (main) memory.
	<li> and only those virtual address space pages currently residing in
		a page frame.
	<li> and only those virtual address space pages currently
		<strong>not</strong> residing in a page frame.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> part of the page (map)
	table entries?
	<ol class="answer_list">
	<li> Referenced bit
	<li> Modified bit
	<li> Present/Absent bit
	<li> Page Frame number
	<li> None of the above
	</ol>
</li><br/>
<li> The higher order bits of a big-endian virtual address normally
	<ol class="answer_list">
	<li> are used as the index into the page table when translating
		a virtual address into a physical address.
	<li> represent the page in the virtual address space.
	<li> indicates which page frame the virtual address resides in.
	<li> provide the offset into the page frame for the virtual address.
	<li> None of the above
	</ol>
</li><br/>
<li> The lower order bits of a big-endian virtual address normally
	<ol class="answer_list">
	<li> are used as the index into the page table when translating
		a virtual address into a physical address.
	<li> represent the page in the virtual address space.
	<li> indicates which page frame the virtual address resides in.
	<li> provide the offset into the page frame for the virtual address.
	<li> None of the above
	</ol>
</li><br/>
<li> The page size for paged virtual memory is <strong>always</strong> a
	power of 2 because
	<ol class="answer_list">
	<li> it allows a fixed number of bits of the virtual address to be used
		as the page offset.
	<li> that's the convention often used when using binary computers.
	<li> adding the page offset to the page frame base can be done more
		quickly using "bit-wise or" rather than addition. 
	<li> the arrays (used to implement page tables) <strong>must</strong>
		have a power of 2 size.
	<li> None of the above
	</ol>
</li><br/>
<li> If, during virtual address translation, the entry in the page table for
	the virtual address has a 0 for the "present" bit, then 
	<ol class="answer_list">
	<li> the 0 page frame is used in calculating the physical address.
	<li> a page fault is generated.
	<li> the "present" bit is set to 1.
	<li> the "modified" bit is set to 1.
	<li> None of the above
	</ol>
</li><br/>
<li> The page table associated with a process is used when
	<ol class="answer_list">
	<li> the process transitions from the blocked to the ready state.
	<li> the next instruction in the process <strong>must</strong> be
		loaded.
	<li> a global page replacement algorithm is determining which page
		frame to reuse.
	<li> a value is stored into a variable (in memory).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> normally part of the
	page table entry?
	<ol class="answer_list">
	<li> Caching disabled flag
	<li> Referenced bit
	<li> Modified bit
	<li> Present/Absent bit
	<li> Protection bit(s)
	<li> Page frame number
	<li> None of the above
	</ol>
</li><br/>
<li> Virtual address translation <strong>must</strong> be <em>very</em> fast
	since it
	<ol class="answer_list">
	<li> can happen many times for every instruction, depending upon the
		machine instructions provided by that computer system.
	<li> prevents the CPU from doing any other work while it is happening.
	<li> happens at least once for every instruction (to retrieve it).
	<li> ties up the system bus while it is happening.
	<li> None of the above
	</ol>
</li><br/>
<li> Assuming that page sizes remain the same, as virtual memories get bigger,
	so does the size of the
	<ol class="answer_list">
	<li> process table.
	<li> page frames.
	<li> page table.
	<li> Memory Management Unit (MMU).
	<li> None of the above
	</ol>
</li><br/>
<li> The Translation Lookaside Buffer (TLB) is part of the
	<ol class="answer_list">
	<li> System Bus.
	<li> Memory Management Unit (MMU).
	<li> Page Table.
	<li> Arithmetic Logic Unit (ALU).
	<li> None of the above
	</ol>
</li><br/>
<li> The Translation Lookaside Buffer (TLB) holds
	<ol class="answer_list">
	<li> the entire page table for a process.
	<li> many of the most recent virtual address translation results
		(since they're likely to be seen again - e.g., loops).
	<li> the process table entries for processes in the ready state.
	<li> a small number of (recently used) page table entries.
	<li> None of the above
	</ol>
</li><br/>
<li> Entries in the Translation Lookaside Buffer (TLB) <strong>must</strong>
	add this item as a field since it cannot be used as an index.
	<ol class="answer_list">
	<li> Page frame number.
	<li> Virtual address page number.
	<li> Present/absent bit.
	<li> Page offset.
	<li> None of the above
	</ol>
</li><br/>
<li> Finding the desired entry in a Translation Lookaside Buffer (TLB)
	<strong>must</strong> be very fast, so this is done by using a(n)
	<ol class="answer_list">
	<li> linear search.
	<li> binary search.
	<li> hash table.
	<li> associative memory.
	<li> None of the above
	</ol>
</li><br/>
<li> If a virtual address doesn't correspond to one of the rows in the
	Translation Lookaside Buffer (TLB), then
	<ol class="answer_list">
	<li> a page fault is generated.
	<li> the MMU updates the TLB immediately.
	<li> the TLB is purged and reloaded.
	<li> a lookup is performed using the full page table.
	<li> None of the above
	</ol>
</li><br/>
<li> If a Translation Lookaside Buffer (TLB) is larger, it can be effectively
	managed by software instead of by the Memory Management Unit (MMU).
	If a page entry is <strong>not</strong> in the TLB, but is in memory,
	this is called a
	<ol class="answer_list">
	<li> cache miss.
	<li> soft miss.
	<li> hard miss.
	<li> page entry fault.
	<li> None of the above
	</ol>
</li><br/>
<li> If a Translation Lookaside Buffer (TLB) is larger, it can be effectively
	managed by software instead of by the Memory Management Unit (MMU).
	If a page entry is <strong>not</strong> in the TLB, and is also
	<strong>not</strong> in memory, this is called a
	<ol class="answer_list">
	<li> cache miss.
	<li> soft miss.
	<li> hard miss.
	<li> page entry fault.
	<li> None of the above
	</ol>
</li><br/>
<li> As the size of the virtual address space increases, so to does the
	size of the page table (provided page size remains the same). 
	Techniques for dealing with very large page tables include
	<ol class="answer_list">
	<li> Multi-level Page Tables.
	<li> Hashed Page Tables.
	<li> Memory Mapped Page Tables.
	<li> Inverted Page Tables.
	<li> None of the above
	</ol>
</li><br/>
<li> Multi-level Page Tables divide the page table up into segments based on
	<ol class="answer_list">
	<li> which pages of the page table are referenced most frequently,
		reducing the number of page entries which need to be kept in
		memory.
	<li> how many processes are in the process table.
	<li> the higher order bits that index the page table (e.g., the top
		10 bits of the 32 bit virtual address indicates which group
		of 1024 pages to use - for 4K sized pages).
	<li> the size of a software managed Translation Lookaside Buffer (TLB),
		with each segment reflecting the TLB size.
	<li> None of the above
	</ol>
</li><br/>
<li> Instead of keeping a page table for each process, Inverted Page Tables
	have an entry for each
	<ol class="answer_list">
	<li> process, keeping track of which page frames are being used by
		that process.
	<li> virtual page, keeping track of which process the virtual page
		belongs to.
	<li> page frame, keeping track of which virtual page from which
		process is loaded in the corresponding page frame.
	<li> virtual page and process pairing, keeping track of which page
		frame corresponds to the pairing.
	<li> None of the above
	</ol>
</li><br/>
<li> In order to speed access to an Inverted Page Table, while still
	conserving space, they are often implemented using
	<ol class="answer_list">
	<li> content addressable memories.
	<li> hash tables.
	<li> 2 dimensional arrays.
	<li> balance B-trees.
	<li> None of the above
	</ol>
</li><br/>

<li> After <strong>all</strong> page frames in a system are occupied,
	page replacement in a virtual memory system is the process
	<ol class="answer_list">
	<li> of choosing a virtual page to load into a given page frame.
	<li> of choosing both a virtual page and a page frame in which to
		load the chosen virtual page.
	<li> of choosing a page frame and loading a given virtual page into that
		page frame.
	<li> that's performed in response to a page fault.
	<li> None of the above
	</ol>
</li><br/>
<li> The essentially unobtainable aspect of optimal page replacement which sets
	it apart from <strong>all</strong> other page replacement algorithms is
	<ol class="answer_list">
	<li> only loading virtual pages that will be used immediately (or in
		the very near future).
	<li> ensuring that no process ever thrashes.
	<li> ensuring that no page frame is replaced before
		<strong>all</strong> other frames have already been replaced.
	<li> only replacing page frames whose resident virtual page wont be
		used for the longest period of time.
	<li> None of the above
	</ol>
</li><br/>
<li> Nearly <strong>all</strong> page replacement algorithms make use of the
	following flag(s) which are associated with each page frame:
	<ol class="answer_list">
	<li> reference bit
	<li> cached bit
	<li> present bit
	<li> modified bit
	<li> None of the above
	</ol>
</li><br/>
<li> The reference bit associated with a page frame is set to
	<ol class="answer_list">
	<li> 0 whenever a virtual page is loaded into the frame. 
	<li> 0 only the first time a virtual page is loaded from the
		virtual address space.
	<li> 1 whenever a virtual page is loaded into a new page frame.
	<li> 1 whenever the page frame contents are changed.
	<li> None of the above
	</ol>
</li><br/>
<li> The modified bit associated with a page frame is set to
	<ol class="answer_list">
	<li> 0 whenever a virtual page is loaded into the frame. 
	<li> 0 only the first time a virtual page is loaded from the
		virtual address space.
	<li> 1 whenever a virtual page is loaded into a new page frame.
	<li> 1 whenever the page frame contents are changed.
	<li> None of the above
	</ol>
</li><br/>
<li> The Not-Recently-Used (NRU) page replacement algorithm replaces pages
	based on the values of their reference and modified bits. What is
	the preference ordering (most prefered replacement listed first) given
	the following combinations?
<pre>
	1 -> referenced, <strong>not</strong> modified
	2 -> <strong>not</strong> referenced, <strong>not</strong> modified
	3 -> referenced, modified
	4 -> <strong>not</strong> referenced, modified
</pre>
	<ol class="answer_list">
	<li> 1, 2, 3, 4
	<li> 2, 4, 1, 3
	<li> 3, 1, 2, 4
	<li> 4, 3, 2, 1
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm <strong>always</strong> replaces the
	oldest page, regardless of the value of the referenced and modified
	bits, is
	<ol class="answer_list">
	<li> First-in First-out Page Replacement
	<li> Second-Chance Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm is the easiest to implement and often
	performs adequately?
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Not Frequently Used (NFU) Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm is a variation of First-in First-out
	page replacement that uses the reference (R) bit. If the page frame
	being considered has R = 0, the page is replaced. Otherwise, if R = 1,
	then R is set to 0 and the page frame is put at the end of the queue
	to be considered again later.
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Second-Chance Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm cycles through the set of page frames
	in order (starting after where it left off last time) looking for one
	with R = 0, which it replaces when found. If R = 1, then R is set to
	0 and the next page frame is considered (until a suitable one is found).
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Second-Chance Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> A disadvantage of both Second-Chance and Clock page replacement algorithms
	is that
	<ol class="answer_list">
	<li> they can immediately (re)load the exact same virtual page already
		in the page frame.
	<li> modified pages are given preference for replacement.
	<li> if <strong>all</strong> reference bits = 1, then the entire
		"queue" <strong>must</strong> be iterated
		through before a replacement page will be found.
	<li> if <strong>all</strong> modified bits = 1, then the entire "queue"
		<strong>must</strong> be iterated
		through before a replacement page will be found.
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm replaces the page that has gone unused
	for the longest period of time?
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm can make effective use of a counter
	associated with each page frame, with the counter value set to the
	global clock tick counter each time the page is referenced?
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Clock Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm adds the value of the page frame's R bit
	to a counter for that page frame, each time <strong>all</strong>
	of the R bits are reset? The counter is reset to 0 whenever a new page
	is loaded into the frame.
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Least Recently Used (LRU) Page Replacement
	<li> Not Frequently Used (NFU) Page Replacement
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm can end up replacing useful pages (e.g.,
	very recently used) instead of pages that haven't been used for a long
	time?
	<ol class="answer_list">
	<li> Second-Chance Page Replacement
	<li> Clock Page Replacement
	<li> Not Recently Used (NRU) Page Replacement
	<li> Not Frequently Used (NFU) Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithm improves upon the primary deficiency of
	the Not Frequently Used (NFU) page replacement algorithm?
	<ol class="answer_list">
	<li> Clock Page Replacement
	<li> Not Recently Used (NRU) Page Replacement
	<li> Least Recently Used (NRU) Page Replacement
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which algorithm is similar to the Not Frequently Used (NFU) page
        replacement algorithm, but instead of simply adding the R bit to the
        page frame counter, it shifts the counter 1 bit to the right before
        the leftmost bit is set to the R value?
	<ol class="answer_list">
	<li> Clock Page Replacement
	<li> Not Recently Used (NRU) Page Replacement
	<li> Least Recently Used (NRU) Page Replacement
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Paging algorithms that only load virtual pages that are the cause of a
	page fault are categorized as
	<ol class="answer_list">
	<li> lazy paging.
	<li> eager paging.
	<li> laissez faire paging.
	<li> demand paging.
	<li> None of the above
	</ol>
</li><br/>
<li> Locality of reference is a property exhibited by most programs in which
	<ol class="answer_list">
	<li> nearly <strong>all</strong> the variables are declared within
		functions rather than as global/static variables.
	<li> the vast majority of program statements are some type of loop.
	<li> the parts of the virtual address space needed are usually close
		to those most recently used. 
	<li> only a small part of the virtual address space of a process is
		ever loaded into a page frame.
	<li> None of the above
	</ol>
</li><br/>
<li> The set of virtual pages that a process is currently using are known
	as the
	<ol class="answer_list">
	<li> loaded pages.
	<li> process set.
	<li> working set.
	<li> recency list.
	<li> None of the above
	</ol>
</li><br/>
<li> Which category of page replacement algorithms tries to reduce the page
	fault rate by <em>prepaging</em>?
	<ol class="answer_list">
	<li> Working set model
	<li> Global allocation
	<li> Local allocation
	<li> Process model
	<li> None of the above
	</ol>
</li><br/>
<li> Thrashing occurs in a multi-tasking system when by the next time a process
	is chosen to run in the CPU
	<ol class="answer_list">
	<li> it's already been blocked by another process.
	<li> it's been terminated by the OS kernel.
	<li> most of its needed virtual pages are <strong>not</strong>
		in page frames.
	<li> most of its needed virtual pages are already loaded in page frames.
	<li> None of the above
	</ol>
</li><br/>
<li> Thrashing is almost certain to occur when
	<ol class="answer_list">
	<li> the size of working sets for <strong>all</strong> processes
		varies greatly from one another.
	<li> the combined working sets of <strong>all</strong> runnable
		processes excedes the size of physical memory.
	<li> the Translation Lookaside Buffer is smaller than largest process
		working set.
	<li> there are more processes than will fit in the process table.
	<li> None of the above
	</ol>
</li><br/>
<li> The amount of time that a process has spent running in the CPU since
	the process started is known as its
	<ol class="answer_list">
	<li> CPU time.
	<li> process time.
	<li> processor virtual time.
	<li> current virtual time.
	<li> None of the above
	</ol>
</li><br/>
<li> The working set of a process is the list of virtual pages the process
	has used
	<ol class="answer_list">
	<li> within the last T seconds of its current virtual time.
	<li> since the process started.
	<li> since the process started or since the last time process was
		blocked (whichever is most recent).
	<li> since it last generated a page fault.
	<li> None of the above
	</ol>
</li><br/>
<li> Which page replacement algorithms introduce an additional field for page
	table entries that holds the last (virtual) time of use for that
	virtual page?
	<ol class="answer_list">
	<li> Clock Page Replacement
	<li> Aging Page Replacement
	<li> Working Set Model Page Replacement
	<li> Working Set Clock Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> For Working Set Model Page Replacement, the virtual page in a page
	frame is evicted if the reference (R) bit
	<ol class="answer_list">
	<li> = 1 and the current virtual time minus the time of last use is
		greater than a predetermined cutoff.
	<li> = 0 (regardless of the time of last use).
	<li> = 0 and the modified bit = 0 (regardless of the time of last use).
	<li> = 0 and the current virtual time minus the time of last use is
		greater than a predetermined cutoff.
	<li> None of the above
	</ol>
</li><br/>
<li> For Working Set Model Page Replacement, the virtual page in a page
	frame is evicted if the reference (R) bit
	<ol class="answer_list">
	<li> = 0, <strong>all</strong> virtual pages in page frames have been
		referenced within the predetermined cutoff, but this page
		has been referenced least recently.
	<li> = 0, <strong>all</strong> virtual pages in page frames have been
		referenced within the predetermined cutoff, but this page has
		been modified.
	<li> = 0 (regardless of the time of last use).
	<li> = 0 and the modified bit = 0 (regardless of the time of last use).
	<li> None of the above
	</ol>
</li><br/>
<li> The major disadvantage of Working Set Model Page Replacement is that it
	requires
	<ol class="answer_list">
	<li> an additional field for the page table entries (to
		hold the time of last use).
	<li> maintaining the current virtual time of each process.
	<li> each page table entry to be examined (to determine if the page
		is in a page frame and if so what its time of last use is).
	<li> the page entry be updated for <em>every</em> access to the page.
	<li> None of the above
	</ol>
</li><br/>
<li> Working Set Clock Page Replacement addresses the short-coming(s) of
	Working Set Model Page Replacement by
	<ol class="answer_list">
	<li> associating the time of last use with the page frame, rather than
		the page table entries.
	<li> only setting the time of use when that page frame is being
		considered for reuse, rather than every time a page fault
		occurs.
	<li> considering each page frame in a "round robin" fashion, rather 
		than looking for virtual pages that have been in a page
		frame for a sufficiently long period of time.
	<li> using the modified (M) bit in deciding whether to immediately
		reuse the page frame or not, rather than ignoring the M bit
		value.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following page replacement algorithms is widely used in
	practice due to its relatively simple implementation and good
	performance?
	<ol class="answer_list">
	<li> Not Recently Used (NRU) Page Replacement
	<li> Clock Page Replacement
	<li> Working Set Clock Page Replacement.
	<li> Aging Page Replacement
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following page replacement algorithms schedules a modified
	page frame to be written back to disk <em>in advance</em> of evicting
	the virtual page from the page frame?
	<ol class="answer_list">
	<li> Clock Page Replacement
	<li> Aging Page Replacement
	<li> Working Set Model Page Replacement.
	<li> Working Set Clock Page Replacement.
	<li> None of the above
	</ol>
</li><br/>

<li> A global allocation policy, with respect to page replacement, is one in
	which the page frames considered for eviction are associated with
	<ol class="answer_list">
	<li> the process that generated the page fault.
	<li> most (generally all) processes.
	<li> only those processes in the ready state.
	<li> the process that generated the page fault along with
		<strong>all</strong> of its child processes.
	<li> None of the above
	</ol>
</li><br/>
<li> An advantage that a global allocation policy (GAP) generally has over
	a local allocation policy (LAP) is that
	<ol class="answer_list">
	<li> GAP increases the likelihood of thrashing.
	<li> LAP increases the likelihood of thrashing.
	<li> LAP can easily increase/decrease the number of virtual pages in
		physical memory for a process.
	<li> GAP can easily increase/decrease the number of virtual pages in
		physical memory for a process.
	<li> None of the above
	</ol>
</li><br/>
<li> A global allocation policy can cause thrashing if
	<ol class="answer_list">
	<li> the combined virtual address space of <strong>all</strong>
		processes is 10x larger than physical memory.
	<li> the pages for a process are nearly always modified.
	<li> a process' pages are evicted by the time it acquires the CPU.
	<li> demand paging is also used.
	<li> None of the above
	</ol>
</li><br/>
<li> A local allocation policy can cause thrashing if
	<ol class="answer_list">
	<li> the combined virtual address space of <strong>all</strong>
		processes is 10x larger than physical memory.
	<li> the pages for a process are nearly always modified.
	<li> demand paging is also used.
	<li> a process' working set is too small.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a way to effectively
	deal with thrashing while the computer is running?
	<ol class="answer_list">
	<li> Swapping a process out to disk and giving its page frames
		to one or more of the processes left.
	<li> Add additional physical memory to the computer system.
	<li> Temporarily "block" the creation of any new processes if the
		combined working sets of existing processes is close to
		or exceeds the size of physical memory.
	<li> Increase the working set of <strong>all</strong> processes to
		ensure the virtual pages they need will be in page frames
		when the process gets the CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> On average, the last page of a "segment" containing code or data will
	be half full. This is a reason to choose
	<ol class="answer_list">
	<li> smaller page sizes.
	<li> larger page sizes.
	<li> page sizes that are <strong>always</strong> a power of 2.
	<li> half sized pages (so the last one will be closer to full).
	<li> None of the above
	</ol>
</li><br/>
<li> Because the page table grows as the number of pages grows, this is a
	reason to use
	<ol class="answer_list">
	<li> smaller page sizes.
	<li> larger page sizes.
	<li> any page size since the page table size doesn't matter.
	<li> None of the above
	</ol>
</li><br/>
<li> Although the 4KB page size remains the most commonly used page size,
	IEEE published results suggest that a more appropriate page size
	would be
	<ol class="answer_list">
	<li> 2KB
	<li> 8KB
	<li> 16KB
	<li> 32KB
	<li> None of the above
	</ol>
</li><br/>
<li> While large 32 and 64 bit virtual address spaces make this less
	necessary, having separate address spaces for code and data can
	still be useful
	<ol class="answer_list">
	<li> for page replacement, so that only virtual pages containing data
		could be loaded into a page frame already holding data (and
		similarly for code/instructions).
	<li> to reduce the working set size of processes.
	<li> to prevent thrashing.
	<li> for parts of the memory hierarch (e.g., caches) which are small
		in size.
	<li> None of the above
	</ol>
</li><br/>
<li> Apart from interprocess communication, it's possible for different
	processes to share the same memory page (i.e., page frame) if
	<ol class="answer_list">
	<li> the page is read only.
	<li> any attempted modification to the page causes a copy to be made
		(and modified) for the writing process.
	<li> the pages are part of a shared library.
	<li> when one of the sharing processes is swapped out (or finishes)
		that the shared pages aren't also evicted.
	<li> None of the above
	</ol>
</li><br/>
<li> When a change to a shared page causes a copy of the page to
	first be made, and then the change made to the copy, this is called
	<ol class="answer_list">
	<li> write through.
	<li> copy on write.
	<li> writable copying.
	<li> page duping.
	<li> None of the above
	</ol>
</li><br/>
<li> Compared to statically linked executables, shared libraries
	<ol class="answer_list">
	<li> decreases the demands on physical memory, allowing more
		processes to execute.
	<li> enable programs to use updated libraries <strong>without</strong>
		being recompiled.
	<li> often reduce the size of executable programs.
	<li> reduce the chance of thrashing.
	<li> None of the above
	</ol>
</li><br/>
<li> Shared libraries and shared memory pages are
	<ol class="answer_list">
	<li> related because the code corresponding to a shared library is
		often held in a shared memory page.
	<li> interdependent since shared memory pages only exist for
		implementing shared libraries.
	<li> completely different since the code for shared libraries is
		statically linked to each program separately at compile time
		while shared memory pages are assigned at run-time.
	<li> there is no relationship except that both can be shared by
		multiple processes.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Position independent-code</em> is used to solve the problem of using
	absolute addressing for shared libraries since
	<ol class="answer_list">
	<li> different processes could load the shared library at different
		locations in their virtual address spaces.
	<li> base offset addressing cannot be used because the offsets will
		be different for each process.
	<li> the page table entries of the sharing processes may each point to
		different page frames.
	<li> the combined size of the libraries and program code could be
		larger than physical memory.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Mapped files</em> enable processes to work with files as if they
	were large character arrays by 
	<ol class="answer_list">
	<li> copying the file contents to shared memory page frames.
	<li> redirecting the file contents as input to the process.
	<li> mapping the file to the virtual address space for the process.
	<li> using pipes to stream the data from the file to the process.
	<li> None of the above
	</ol>
</li><br/>
<li> Having multiple processes share the same mapped file, enables
	<ol class="answer_list">
	<li> the file to be larger than it would if only mapped by a single
		process.
	<li> a larger number of processes to be executing at once.
	<li> mutual exclusion on the critical sections of each process.
	<li> fast interprocess communication.
	<li> None of the above
	</ol>
</li><br/>
<li> Because page replacement algorithms <strong>must</strong> write modified
	page frames back to disk before loading in another virtual page, thus
	slowing down the resolution of a page fault,
	<ol class="answer_list">
	<li> many page replacement algorithms give preference to UNmodified
		pages over modified pages.
	<li> page modifications are often implemented as <em>write-through</em>
		so that the dirty page is immediately written out to disk after
		each change.
	<li> a <em>paging daemon</em> can be used as a background process to
		write dirty pages back to disk, reducing the number of dirty
		page frames.
	<li> the frequency of page modifications is reduced by translating
		writes to the page directly into writes out to the disk.
	<li> None of the above
	</ol>
</li><br/>

<li> When the OS creates a new process, and its corresponding entry in the
	process table, it <strong>must</strong> also
	<ol class="answer_list">
	<li> add it to the set of blocked processes.
	<li> create a page table for the process.
	<li> allocate a swap area (on disk) for the process.
	<li> pre-load its working set into page frames.
	<li> None of the above
	</ol>
</li><br/>
<li> When a context switch occurs to run a process,
	<ol class="answer_list">
	<li> reset <strong>all</strong> of the "present" bits in the process'
		page table to 0 (showing that they are <strong>not</strong>
		loaded into page frames).
	<li> the Memory Management Unit (MMU) <strong>must</strong> be reset.
	<li> restore the CPU registers to the values they had when the process
		was last running.
	<li> the Translation Lookaside Buffer (TLB) <strong>must</strong>
		be flushed.
	<li> None of the above
	</ol>
</li><br/>
<li> What is often one of the easiest (and often cheapest) ways to increase
	the speed of a computer system?
	<ol class="answer_list">
	<li> Upgrade to a faster processor.
	<li> Add more memory.
	<li> Add more hard disk space.
	<li> Add more processors.
	<li> None of the above
	</ol>
</li><br/>
<li> When a process terminates, the OS should
	<ol class="answer_list">
	<li> release the page frames it was using (unless they were shared
		and other processes are still using them).
	<li> release/deallocate the process' page table.
	<li> free the disk space assigned as the process' swap area.
	<li> remove its entry from the process table.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following steps are typically done to handle a page fault:
	<ol class="answer_list">
	<li> the hardware traps to the OS kernel
	<li> the program counter (and instruction information) is saved
	<li> general registers values are saved before calling the OS
	<li> the faulting process is swapped out to disk
	<li> the OS determines which virtual page is needed
	<li> ensure the virtual address is valid
	<li> Use the Translation Lookaside Buffer (TLB) to find a page frame
		to use
	<li> <strong>always</strong> schedule the chosen page frame to be
		written back to disk
	<li> update the page table after the page is loaded into the page frame
	<li> load the swapped out faulting process back into memory
	<li> set the program counter back to the instruction that caused the
		page fault
	<li> schedule the faulting process to run
	</ol>
</li><br/>
<li> Instruction backup is necessary because
	<ol class="answer_list">
	<li> while the first part of the instruction was in physical memory
		the last portion may not be - causing a page fault.
	<li> a context switch requires that the last instruction attempted
		by the process being evicted from the CPU <strong>must</strong>
		be reexecuted when the process runs again.
	<li> if the instruction causes a page fault, some or all of it will
		need to be reexecuted.
	<li> just like for files, you need to ensure that the instruction
		doesn't get removed accidentally.
	<li> None of the above
	</ol>
</li><br/>
<li> Locking a page in memory (i.e., pinning), so that the page replacement
	algorithm cannot select it for eviction, is necessary because
	<ol class="answer_list">
	<li> it reduces the likelihood of thrashing.
	<li> some operations load file data directly into a page frame, and
		reassigning the page could expose protected information to
		another process.
	<li> most page replacement algorithms would preferentially choose
		the most recently loaded page again for replacement.
	<li> if modified, it needs to be written back to disk by the
		paging daemon before it's reused.
	<li> None of the above
	</ol>
</li><br/>
<li> The swap area on disk associated with a process is
	<ol class="answer_list">
	<li> only needed if the virtual address space is larger than physical
		memory.
	<li> <strong>not</strong> necessary, but can help improve performance.
	<li> created when a process gets its first page fault.
	<li> used to store process information <strong>not</strong> currently
		in memory (e.g., a page frame).
	<li> None of the above
	</ol>
</li><br/>
<li> Basic approaches for allocating swap space include allocating space
	<ol class="answer_list">
	<li> for the entire process when the process is initially created.
	<li> for the entire process only after the process
		generates its first page fault.
	<li> only for pages <strong>not</strong> currently in physical memory,
		adding to the allocation as needed (e.g., when the run-time
		stack grows).
	<li> only when the run-time stack or heap grow beyond a certain
		pre-defined limit.
	<li> None of the above
	</ol>
</li><br/>
<li> A drawback of initially allocating <strong>all</strong> the swap space
	for a process is
	<ol class="answer_list">
	<li> that a larger than necessary page table <strong>must</strong>
		also be created.
	<li> the corresponding process table entry is larger.
	<li> as the process needs more space (e.g., for the run-time stack)
		it may need to copy the swap area to a larger chunk of disk
		space.
	<li> it can increase the chance of thrashing since there are more
		virtual pages that might need to be loaded into page frames.
	<li> None of the above
	</ol>
</li><br/>
<li> Allocating space in the swap area only for a process' pages
	<strong>not</strong> currently in physical memory
	<ol class="answer_list">
	<li> decreases the size of the page table.
	<li> often reduces the total amount of swap space needed.
	<li> reduces the chance of thrashing since there are fewer
		virtual pages that might need to be loaded into page frames.
	<li> easily handles the case when a process' address space
		<strong>must</strong> increase (e.g., for the run-time stack).
	<li> None of the above
	</ol>
</li><br/>
<li> The separation of policy and mechanism with respect to virtual memory
	and page replacement algorithms is
	<ol class="answer_list">
	<li> <strong>not</strong> attainable since page replacement algorithms
		need access to the page tables which are protected data within
		the OS kernel.
	<li> can be done by having the MMU and page fault handlers run within
		the OS kernel while a user space  page loader copies the
		virtual page to memory (the page fault handler remaps the page
		frame to the process afterwards). 
	<li> while theoretically possible, there are no operating systems that
		currently implement this capability.
	<li> is possible, though somewhat complicated, and is implemented
		by the Mach kernel.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following are <strong>not</strong> essential qualities of
	files?
	<ol class="answer_list">
	<li> Multiple processes can access the same file simultaneously.
	<li> Are stored on either hard disk drives (HDDs) or solid-state
		disks (SSDs).
	<li> Ability to store LARGE amounts of information.
	<li> The lifetime of a file is not bound to any process.
	<li> None of the above
	</ol>
</li><br/>
<li> The file system provides an abstraction of how files are
	<ol class="answer_list">
	<li> manipulated.
	<li> read.
	<li> managed.
	<li> written.
	<li> None of the above
	</ol>
</li><br/>
<li> File names
	<ol class="answer_list">
	<li> are a sequence of (readable) characters for referring to the file.
	<li> may be of any length, and contain both upper and lower case
		characters.
	<li> <strong>must</strong> have a single extension
		(e.g., ".txt", ".exe") indicating the type of data they hold.
	<li> differ from system to system, depending upon the file system(s)
		the OS makes available.
	<li> None of the above
	</ol>
</li><br/>
<li> The extension (e.g.,  ".txt", ".exe") for a file name
	<ol class="answer_list">
	<li> is typically only meaningful to application programs.
	<li> indicates to the file system what type of data to restrict the
		file to holding (e.g., a ".txt" file may not hold a pdf
		document).
	<li> is a convenience to users to make it easier to discern what
		kind of data each file holds.
	<li> can be ignored by the OS (e.g., Unix/Linux).
	<li> None of the above
	</ol>
</li><br/>
<li> The logical organization of the information in a file is called its
	<ol class="answer_list">
	<li> file type.
	<li> file system.
	<li> file structure.
	<li> file attributes.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are the basic types of file structures?
	<ol class="answer_list">
	<li> Unstructured sequence of bytes.
	<li> Sequence of (structured) records.
	<li> List of object-oriented tables.
	<li> Tree of (structured) records.
	<li> None of the above
	</ol>
</li><br/>
<li> Regular files in Unix/Linux and Windows are organized as
	<ol class="answer_list">
	<li> an unstructured sequence of bytes.
	<li> a sequence of (structured) records.
	<li> a list of object-oriented tables.
	<li> a tree of (structured) records.
	<li> None of the above
	</ol>
</li><br/>
<li> Indexed Sequential Access Method (ISAM) files on mainframes are organized
	as
	<ol class="answer_list">
	<li> an unstructured sequence of bytes.
	<li> a sequence of (structured) records.
	<li> a list of object-oriented tables.
	<li> a tree of (structured) records.
	<li> None of the above
	</ol>
</li><br/>
<li> Indexed Virtual Storage Access Method (Indexed VSAM) files on mainframes
	are organized as
	<ol class="answer_list">
	<li> an unstructured sequence of bytes.
	<li> a sequence of (structured) records.
	<li> a list of object-oriented tables.
	<li> a tree of (structured) records.
	<li> None of the above
	</ol>
</li><br/>
<li> File types describe the broad categorization of the kinds of information
	a file represents/holds. Which of the following are common file types?
	<ol class="answer_list">
	<li> Regular (data) files.
	<li> Executable files.
	<li> Special (device) files.
	<li> Directories.
	<li> None of the above
	</ol>
</li><br/>
<li> Files that model I/O devices that work with one byte at time are called
	<ol class="answer_list">
	<li> I/O files.
	<li> System files.
	<li> Character special files.
	<li> Block special files.
	<li> None of the above
	</ol>
</li><br/>
<li> Files that model I/O devices that work with arrays of bytes at a time
	are called
	<ol class="answer_list">
	<li> I/O files.
	<li> System files.
	<li> Character special files.
	<li> Block special files.
	<li> None of the above
	</ol>
</li><br/>
<li> Devices such as keyboards, mice, and printers usually correspond to
	<ol class="answer_list">
	<li> I/O files.
	<li> System files.
	<li> Character special files.
	<li> Block special files.
	<li> None of the above
	</ol>
</li><br/>
<li> Devices such as hard disk drives (HDDs) and solid-state disks (SSDs)
	usually correspond to
	<ol class="answer_list">
	<li> I/O files.
	<li> System files.
	<li> Character special files.
	<li> Block special files.
	<li> None of the above
	</ol>
</li><br/>
<li> The two primary file access methods are
	<ol class="answer_list">
	<li> Linear access.
	<li> Sequential access.
	<li> Indexed access.
	<li> Random access.
	<li> None of the above
	</ol>
</li><br/>
<li> If accessing the Nth byte of a file requires first accessing byte N-1,
	then the file access type is
	<ol class="answer_list">
	<li> Linear access.
	<li> Sequential access.
	<li> Indexed access.
	<li> Random access.
	<li> None of the above
	</ol>
</li><br/>
<li> If accessing the Nth byte of a file can be done <strong>without</strong>
	first accessing byte N-1, then the file access type is
	<ol class="answer_list">
	<li> Linear access.
	<li> Sequential access.
	<li> Indexed access.
	<li> Random access.
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX system call can be used to skip to a particular byte
	location within a file (thus supporting random access)?
	<ol class="answer_list">
	<li> find
	<li> lseek
	<li> skip
	<li> rand
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file attributes are common to many systems?
	<ol class="answer_list">
	<li> File name.
	<li> The number of disk blocks the file occupies.
	<li> Size of the file in bytes.
	<li> The optimal block size for reading or writing this file
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to create a new file?
	<ol class="answer_list">
	<li> creat
	<li> mkfile
	<li> new
	<li> make
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to open an existing file (for reading
	or writing)?
	<ol class="answer_list">
	<li> stat
	<li> fopen
	<li> find
	<li> create
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to close an open file (ensuring that any
	buffers are written back disk)?
	<ol class="answer_list">
	<li> end
	<li> flush
	<li> fclose
	<li> done
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to delete an existing file?
	<ol class="answer_list">
	<li> delete
	<li> free
	<li> deallocate
	<li> remove
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to read data from a file?
	<ol class="answer_list">
	<li> read
	<li> read_byte
	<li> scan
	<li> get_byte
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to write data to a file?
	<ol class="answer_list">
	<li> print
	<li> put_char
	<li> write
	<li> write_byte
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to set the current position within an open file?
	<ol class="answer_list">
	<li> find
	<li> locate
	<li> seek
	<li> position
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to obtain information about a file?
	<ol class="answer_list">
	<li> get_info
	<li> finfo
	<li> fattr
	<li> stat
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to change the name of an existing file?
	<ol class="answer_list">
	<li> move
	<li> rename
	<li> chfile
	<li> touch
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to change the owner of a file?
	<ol class="answer_list">
	<li> finfo
	<li> fattr
	<li> chown
	<li> owner
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to change the permissions of a file?
	<ol class="answer_list">
	<li> perm
	<li> chmod
	<li> chperm
	<li> stat
	<li> None of the above
	</ol>
</li><br/>

<li> File systems that have only a single directory of files, with no
	sub-directory hierarchies
	<ol class="answer_list">
	<li> are called flat file systems.
	<li> can accommodate larger files than a hierarchical file system.
	<li> were being used until the mid-1980s.
	<li> are in common use today, typically for USB flash drives.
	<li> None of the above
	</ol>
</li><br/>
<li> The most common type of file system in use today are
	<ol class="answer_list">
	<li> flat file systems.
	<li> linear file systems.
	<li> indexed file systems.
	<li> hierarchical file systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Hard links to files
	<ol class="answer_list">
	<li> require the file to be copied, with future changes to the
		original being propigated to the copy.
	<li> maintain a single copy of the file, but accessible from different
		places/directories within the file system.
	<li> use "copy on write" to maintain a single copy of the file until
		a change is made.
	<li> is the kind used when a file is first created.
	<li> None of the above
	</ol>
</li><br/>
<li> Hard links to a directory, from within the current directory,
	<ol class="answer_list">
	<li> can be created <strong>without</strong> any issues.
	<li> are only allowed when the directory being linked to is an
		ancestor (i.e., occurs in the path of the current location).
	<li> are only allowed when the directory being linked to is a
		decendant (e.g., child, grandchild) of the current directory.
	<li> are <strong>not</strong> allowed since this would create a
		circular structure that wouldn't be freeable later.
	<li> None of the above
	</ol>
</li><br/>
<li> Hard links can be used
	<ol class="answer_list">
	<li> to link to files on the same computer but in a different file
		system.
	<li> to link to remote files on different computer systems.
	<li> only to link to files on the same computer, either in the same
		or a different file system.
	<li> only to link to files within the same physical file system.
	<li> None of the above
	</ol>
</li><br/>
<li> Symbolic (soft) links to files and directories
	<ol class="answer_list">
	<li> can link to files and directories in different file
		systems as well as on different computers.
	<li> can link to non-existent files.
	<li> are allowed to create circular structures (e.g., a directory
		can have itself as its parent/child).
	<li> are automatically deleted when the orignal file/directory is
		removed.
	<li> None of the above
	</ol>
</li><br/>
<li> Absolute path names begin with a
	<ol class="answer_list">
	<li> "./" on both Unix/Linux and Windows systems.
	<li> "../" on both Unix/Linux and Windows systems.
	<li> slash (/) on Unix/Linux systems.
	<li> drive letter (e.g., A:) on a Windows system.
	<li> None of the above
	</ol>
</li><br/>
<li> Relative path names specify the location of a file or directory
	<ol class="answer_list">
	<li> starting from the current working directory.
	<li> with respect to the root directory of the file system.
	<li> relative to the user's home directory.
	<li> starting from the directory where the program requesting the
		file is located.
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to create a new directory?
	<ol class="answer_list">
	<li> new
	<li> mkdir
	<li> dir
	<li> create
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to delete an existing directory?
	<ol class="answer_list">
	<li> remove
	<li> delete
	<li> rmdir
	<li> free
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to open a directory for examination?
	<ol class="answer_list">
	<li> find
	<li> stat
	<li> examine
	<li> opendir
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to close a directory and ensure that any
	changes to it are written back to the disk?
	<ol class="answer_list">
	<li> closedir
	<li> flush
	<li> finish
	<li> done
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to change the name of a directory?
	<ol class="answer_list">
	<li> move
	<li> rename
	<li> chname
	<li> chmod
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to create a new directory entry for an existing
	file?
	<ol class="answer_list">
	<li> add2dir
	<li> direntry
	<li> link
	<li> new
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to remove the directory entry for a file?
	<ol class="answer_list">
	<li> rmfile
	<li> delete
	<li> free
	<li> unlink
	<li> None of the above
	</ol>
</li><br/>
<li> Which POSIX call is used to change the current working directory?
	<ol class="answer_list">
	<li> pwd
	<li> chdir
	<li> cd
	<li> go2dir
	<li> None of the above
	</ol>
</li><br/>
<li> The key difference(s) between hard links and soft/symbolic links are that
	<ol class="answer_list">
	<li> soft links allow files to be accessed randomly while hard linked
		files are limited to sequential access. 
	<li> soft links are implemented by software but hard links require
		hardware support to be implemented.
	<li> hard links point to a physical (disk) location while soft links
		give the "directions" (i.e., path) to the linked file.
	<li> hard links provide copy on write semantics whereas symbolic links
		do <strong>not</strong>.
	<li> None of the above
	</ol>
</li><br/>
<li> Deleting a soft link
	<ol class="answer_list">
	<li> <strong>always</strong> deletes the file that the link pointed to.
	<li> will delete the file the link points to, but only if the file
		is located on the same file system as the link.
	<li> will delete the file the link points to, but only if this is
		the last soft link to the file.
	<li> <strong>never</strong> deletes the file that the link pointed to.
	<li> None of the above
	</ol>
</li><br/>
<li> Deleting a hard link
	<ol class="answer_list">
	<li> <strong>always</strong> deletes the file that the link pointed to.
	<li> will delete the file the link points to, but only if the file
		is located on the same file system as the link.
	<li> will delete the file the link points to, but only if this is
		the last hard link to the file.
	<li> <strong>never</strong> deletes the file that the link pointed to.
	<li> None of the above
	</ol>
</li><br/>
<li> Soft links can point to files
	<ol class="answer_list">
	<li> but only when there's <strong>not</strong> another soft link
		already pointing to them.
	<li> but only when there's <strong>not</strong> a hard link already
		pointing to them.
	<li> that don't actually exist.
	<li> on different file systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Hard links can point to files
	<ol class="answer_list">
	<li> but only when there's <strong>not</strong> a soft link already
		pointing to them.
	<li> but only when there's <strong>not</strong> another hard link
		already pointing to them.
	<li> that don't actually exist.
	<li> on different file systems.
	<li> None of the above
	</ol>
</li><br/>

<li> File systems are
	<ol class="answer_list">
	<li> <strong>all</strong> very similar with the few differences
		between them of little importance.
	<li> nearly always stored on disk (either HDD or SSD).
	<li> dedicated to the OS, with each OS supporting only a single
		file system.
	<li> generally contained within a single disk partition (if stored on
		a disk).
	<li> None of the above
	</ol>
</li><br/>
<li> A disk (either HDD or SSD) has
	<ol class="answer_list">
	<li> a single Master Boot Record (MBR).
	<li> one or more partitions.
	<li> a boot block for each partition.
	<li> a partition table (stored at the end of the MBR).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> steps in the boot process?
	<ol class="answer_list">
	<li> the BIOS identifies the boot device and ensures it's attached.
	<li> the program in the Master Boot Record (MBR) is loaded and run.
	<li> the MBR program loads and runs the code in the first block of the
		active partition.
	<li> the boot block program loads the OS from the active partition.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> part of every disk
	partition?
	<ol class="answer_list">
	<li> Root directory
	<li> Boot block
	<li> Superblock
	<li> Master Boot Record (MBR)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> a basic technique for
	implementing files?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which file implementation technique stores the file in consecutive disk
	blocks, so that only the last block has any wasted space?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which file implementation technique uses the first word of each block
	(or alternately a separate table representing those words) to indicate
	the disk block holding the next portion of the file?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which file implementation technique only requires the location of the
	first disk block and the number of disk blocks allocated for the file
	in order to access it?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which file implementation technique uses a single data structure that
	stores standard metadata along with direct and indirect pointers to
	the file blocks containing the file data?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file implementation techniques suffers from
	external fragmentation if files can be removed or modified?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file implementation techniques completely
	avoid any internal fragmentation?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> File Allocation Tables are an aspect of which of the following file
	implementation techniques?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file implementation techniques has the
	<strong>best</strong> support for random access?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> Linked List Allocation
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which file system implementation is suitable for smaller disks
	but is less suitable as the size of the disk grows larger?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> File Allocation Table (FAT)
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> An advantage that i-nodes have over FAT file systems is that
	<ol class="answer_list">
	<li> they have better performance for randomly accessing files.
	<li> they have less external fragmentation.
	<li> they have less internal fragmentation.
	<li> only direct and indirect links to the disk blocks for the desired
		file (and no other files) are needed in memory.
	<li> None of the above
	</ol>
</li><br/>
<li> ISO 9660 was developed for use by compact disks (for music) and later
	DVDs (for movies). Which file system implementation approach is
	ISO 9660 an example of?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Superblock Allocation
	<li> File Allocation Table (FAT)
	<li> Spiral Allocation
	<li> Index-nodes
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> a commonly used technique
	for storing file/directory names in directory entries?
	<ol class="answer_list">
	<li> Use a fixed amount of space for the name (e.g., 8.3 names).
	<li> Allocate the string for the name from heap storage and put the
		pointer in the directory entry.
	<li> Allow directory entries to be variable in length with the file
		name appearing at the end of the entry.
	<li> Each directory entire has a pointer into a shared string space
		for storing <strong>all</strong> the names in that directory.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are true about the root directory of a file
	system?
	<ol class="answer_list">
	<li> It's the only file/directory <strong>without</strong> a name.
	<li> Both hard and soft links to the root directory are
		<strong>not</strong> allowed.
	<li> The root is stored in the boot block of the file system partition.
	<li> A pointer to it <strong>must</strong> be kept in a known/standard
		location.
	<li> None of the above
	</ol>
</li><br/>
<li> The attributes associated with a file (or directory) are
	<ol class="answer_list">
	<li> <strong>always</strong> stored in its directory entry.
	<li> are stored in the directory entry for contiguous and linked list
		file allocations.
	<li> are stored in the file's (or directory's) i-node if index-nodes
		are used for file allocation.
	<li> <strong>never</strong> stored in its directory entry.
	<li> None of the above
	</ol>
</li><br/>
<li> As the number of directory entries increases (e.g., >= 100),
	<ol class="answer_list">
	<li> searching for a file (or directory) name may get slower if a
		hash table isn't used.
	<li> the i-nodes used (for an index-node implementation) increases
		faster than the number of files in the directory.
	<li> space within the directory (itself a special file) is likely
		to become exhausted.
	<li> the file system as a whole becomes slower to access.
	<li> None of the above
	</ol>
</li><br/>
<li> Soft links are slower to access a file than hard links because
	<ol class="answer_list">
	<li> every file system on the disk (in different partitions)
		<strong>must</strong> be checked for the file.
	<li> hard links point directly to the file (e.g., via its
		i-node number). 
	<li> the linked file is <strong>always</strong> specified as an
		absolute path.
	<li> they require visiting (multiple) other directories along the
		indicated file path to find the linked file.
	<li> None of the above
	</ol>
</li><br/>
<li> Copying soft links can be problematic because
	<ol class="answer_list">
	<li> there are two ways to copy them, making a copy of the link or
		making a copy of the file the link points to.
	<li> the link may point to another file system, and copies across
		file systems cannot be done.
	<li> the resulting copy will <strong>always</strong> be a soft link.
	<li> they can sometimes be mistaken for a hard link by the OS.
	<li> None of the above
	</ol>
</li><br/>
<li> Log structure files buffer writes, to a hard disk disk (HDD), in memory
	<ol class="answer_list">
	<li> improving the write efficiency to the disk while reducing wait
		time for processes writing to disk.
	<li> thus preventing file system problems if a system failure occurs.
	<li> but this increases the risk that information will be lost if a
		system failure occurs.
	<li> because file system operations generally require many small writes
		to the HDD, so bundling them together improves efficiency.
	<li> None of the above
	</ol>
</li><br/>
<li> Log structure files
	<ol class="answer_list">
	<li> keep a complete record of <strong>all</strong> writes that have
		ever been made to a disk drive.
	<li> are becoming increasingly important to have as solid state disks
		(SSDs) become more common.
	<li> are becoming less necessary as hard disk drives (HDDs) are replaced
		with solid state disks (SSDs).
	<li> have a special process that regularly "cleans" and compacts them
		(as some items in the log are no longer needed as time passes).
	<li> None of the above
	</ol>
</li><br/>
<li> Journaling file systems are used to
	<ol class="answer_list">
	<li> improve the write efficiency to the disk.
	<li> prevent file system problems if a system failure occurs.
	<li> enable access to remote file systems.
	<li> create an up-to-date backup of a file system, so that it can be
		restored in the case of a disk failure/crash.
	<li> None of the above
	</ol>
</li><br/>
<li> A journaling file system keeps a log of actions to be performed
	<ol class="answer_list">
	<li> to improve the write efficiency to the disk.
	<li> removing them only after their completion has been confirmed.
	<li> but each action <strong>must</strong> be <em>idempotent</em> in
		case there are multiple system crashes.
	<li> to create an up-to-date backup of a file system, so that it can be
		restored in the case of a disk failure/crash.
	<li> None of the above
	</ol>
</li><br/>
<li> An <em>idempotent</em> operation is one that
	<ol class="answer_list">
	<li> makes an atomic change to disk (i.e., either the entire operation
		completes or none of it does).
	<li> <strong>always</strong> runs from within a critical section.
	<li> is <strong>not</strong> critical, so that if it
		<strong>never</strong> gets performed it's okay.
	<li> <strong>must</strong> be executed <em>exactly</em> once
		(e.g., making a deposit to a bank account). 
	<li> None of the above
	</ol>
</li><br/>
<li> A virtual file system
	<ol class="answer_list">
	<li> implements both journaling and log file structures, improving
		both efficiency and safety.
	<li> enables multiple file systems to be seamless integrated so that
		they appear as a single file system.
	<li> provides access to file systems on remote computers.
	<li> ensures that both soft and hard links can be used across file
		systems.
	<li> None of the above
	</ol>
</li><br/>
<li> A virtual file system
	<ol class="answer_list">
	<li> does <strong>not</strong> actually store files itself, but
		interfaces with multiple other file systems.
	<li> stores files on disks that are shared among multiple computer
		systems.
	<li> provides access to file systems on remote computers.
	<li> ensures that both soft and hard links can be used across file
		systems.
	<li> None of the above
	</ol>
</li><br/>

<li> Most file systems do <strong>not</strong> store files in contiguous disk
	blocks because
	<ol class="answer_list">
	<li> this makes the file system design and implementation simpler.
	<li> of the delay that the rotational latency of a HDD requires when
		the disk blocks are next to one another.
	<li> it reduces the amount of internal fragmentation.
	<li> files tend to grow in size, requiring the disk blocks for the 
		file to be copied.
	<li> None of the above
	</ol>
</li><br/>
<li> For the same size disk, larger disk block sizes tend to
	<ol class="answer_list">
	<li> improve the data transfer rate to and from disk.
	<li> reduce internal fragmentation.
	<li> reduce the number of blocks needed to store a file.
	<li> decrease the amount of storage need to keep track of bad disk
		blocks.
	<li> None of the above
	</ol>
</li><br/>
<li> It's not uncommon, particularly for hard disk drives (HDDs), for disk
	blocks to become bad. To avoid using them, bad blocks are
	<ol class="answer_list">
	<li> tracked by the controller in a list stored in a pre-specified
		disk location.
	<li> filled with a special value to identify them.
	<li> moved to another location on the disk.
	<li> removed from the list of free blocks.
	<li> None of the above
	</ol>
</li><br/>
<li> The free (unused) blocks on a disk are often tracked by
	<ol class="answer_list">
	<li> moving the used blocks to the lower disk block numbers (i.e.,
		compacting) and keeping a single number which is the lowest
		disk number of the set of contiguous free blocks.
	<li> exchanging (copying) free blocks and used blocks (as a
		background process) to combine the free blocks into a small
		number of contiguous areas.
	<li> keeping a linked list of blocks, with each block in the list
		pointing to a large number of free blocks.
	<li> using a bit map contained in 1 or more blocks, with bits
		having either a value of 1 (used) or 0 (free).
	<li> None of the above
	</ol>
</li><br/>
<li> Disk quotas are a mechanism to prevent
	<ol class="answer_list">
	<li> account holders from using more than their "fair share"
		of the file system space.
	<li> disk failure due to overuse.
	<li> the inadvertent loss of files due to user error.
	<li> a disk from filling up.
	<li> None of the above
	</ol>
</li><br/>
<li> A disk quota can be used to limit the total
	<ol class="answer_list">
	<li> amount of space on the disk.
	<li> amount of disk space a user's files take up.
	<li> number of swap areas (and thus processes) that can exist. 
	<li> number of files a user has.
	<li> None of the above
	</ol>
</li><br/>
<li> A disk quota soft limit
	<ol class="answer_list">
	<li> warns the user when they are getting close to their hard limit.
	<li> prevents the creation of new files when exceeded, but allows
		existing files to be appended to.
	<li> restricts the number of executable files (software) that the
		user has.
	<li> indicates the maximum number of soft links that the user can
		have.
	<li> None of the above
	</ol>
</li><br/>
<li> A disk quota hard limit
	<ol class="answer_list">
	<li> is the maximum amount of space (or number of files) a user is
		permitted to have.
	<li> applies only to hard disk drives (HDDs).
	<li> can prevent a user from saving their work, if reached.
	<li> indicates the maximum number of hard links that the user can
		have.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary types of file system backups are
	<ol class="answer_list">
	<li> physical dump.
	<li> traditional dump.
	<li> logical dump.
	<li> virtual dump.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of file system dump is fast and simple to implement, but
	can't do incremental backups or restore select files?
	<ol class="answer_list">
	<li> physical dump.
	<li> traditional dump.
	<li> logical dump.
	<li> virtual dump.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of file system dump can restore select files and dump only
	changed files and directories, but is slower and more complex to
	implement?
	<ol class="answer_list">
	<li> physical dump.
	<li> traditional dump.
	<li> logical dump.
	<li> virtual dump.
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>full backup</em> is
	<ol class="answer_list">
	<li> taken when a file system is full, so it can be stored off-site.
	<li> a complete copy of the current state of the file system.
	<li> done when each user has reached their disk quota soft limit.
	<li> done when each user has reached their disk quota hard limit.
	<li> None of the above
	</ol>
</li><br/>
<li> An <em>incremental backup</em> is
	<ol class="answer_list">
	<li> frequently used because it takes less space and finishes more
		quickly than a full backup.
	<li> created whenever a user reaches their disk quota soft limit.
	<li> used to make copies of only those files (and directories) that
		have changed since the last backup.
	<li> performed automatically by the file system on a periodic basis.
	<li> None of the above
	</ol>
</li><br/>
<li> Keeping file system backups on-site is
	<ol class="answer_list">
	<li> preferred as this makes it easier and faster to restore user files.
	<li> <strong>never</strong> a good idea since a single problem
		(e.g., tornado) can cause a loss of both the file system
		(on disks) and its backup.
	<li> no better or worse than keeping them off-site.
	<li> necessary only if the backup is made to other disks in near
		real-time.
	<li> None of the above
	</ol>
</li><br/>
<li> For an i-node based file system, which type of check recursively
	traverses a directory keeping a count for each i-node of the number
	of references to that i-node?
	<ol class="answer_list">
	<li> OS consistency check
	<li> File consistency check
	<li> Block consistency check
	<li> I-node check
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of check uses two counters per disk block to count the number
	of times they are pointed to by either an i-node (1st counter) or
	from the free list (2nd counter)?
	<ol class="answer_list">
	<li> OS consistency check
	<li> File consistency check
	<li> Block consistency check
	<li> I-node check
	<li> None of the above
	</ol>
</li><br/>
<li> The i-node counts created by a file consistency check should
	<ol class="answer_list">
	<li> equal the i-node reference counts.
	<li> sum to the total number of disk blocks.
	<li> equal the count of the number of free blocks.
	<li> <strong>never</strong> be greater than 1.
	<li> None of the above
	</ol>
</li><br/>
<li> After a block consistency check has created its counts corresponding to
	the number of times the block is used or appears in the free list
	respectively, a problem exists if
	<ol class="answer_list">
	<li> both counts are 0.
	<li> if the sum of the two counts = 1.
	<li> both counts are >= 1.
	<li> either count is > 1.
	<li> None of the above
	</ol>
</li><br/>
<li> To improve the performance of a file system, particularly those stored on
	HDDs,
	<ol class="answer_list">
	<li> the disk block size should be kept small (e.g., 1 KB) to reduce
		internal fragmentation.
	<li> a block cache should keep a number of disk blocks in memory to
		reduce read times.
	<li> changes to a file's disk block that's been loaded into memory,
		should <strong>not</strong> be written back to disk after
		every change.
	<li> Perform a regular incremental backup. 
	<li> None of the above
	</ol>
</li><br/>
<li> Pre-fetching the next few disk blocks for a file, can improve the
	performance for which type of file access (particularly on HDDs)?
	<ol class="answer_list">
	<li> Random access.
	<li> Spiral access.
	<li> Sequential access.
	<li> Tree access.
	<li> None of the above
	</ol>
</li><br/>
<li> Part of the advantage of caching disk blocks in memory is to reduce
	the total number of writes to disk (i.e., make several changes to the
	cache before writing the cached block back to disk), what if any
	advantage is there to having a cache that writes through
	to the disk every time the cache is changed?
	<ol class="answer_list">
	<li> In the event of a system failure, write-through-caches can reduce
		the amount of lost data.
	<li> All writes to disk <strong>must</strong> come from memory/cache
		anyway, so there is no difference in write performance.
	<li> The write-through-cache also provides the faster read access.
	<li> Using a write-through-cache lowers contention on the system bus
		since the disk block is written as soon as it's changed.
	<li> None of the above
	</ol>
</li><br/>
<li> When possible, in order to improve file read/write performance, the free
	blocks allocated to store the contents of a file should be
	<ol class="answer_list">
	<li> randomly distributed on the disk to avoid access contention.
	<li> in the same disk partition.
	<li> one after another (on HDDs) to reduce seek time.
	<li> of the same size.
	<li> None of the above
	</ol>
</li><br/>
<li> The Unix <em>sync</em> and Windows <em>FlushFileBuffers</em> processes
	<ol class="answer_list">
	<li> ensure that the contents of cached disk blocks are written to
		disk periodically when they've been modified. 
	<li> update the contents of any file disk blocks cached in memory
		that might have been altered on disk.
	<li> are unnecessary if write-through caches are
		<strong>always</strong> used.
	<li> only need to be used in combination with a virtual file system.
	<li> None of the above
	</ol>
</li><br/>
<li> Defragmenting a disk
	<ol class="answer_list">
	<li> compacts the file system, placing <strong>all</strong> of the
		free disk blocks at one end of the partition.
	<li> reduces the amount of internal fragmentation.
	<li> can place a file's disk blocks consecutively so as to increase
		read/write access speeds.
	<li> increases the number of free disk blocks available to the file
		system.
	<li> None of the above
	</ol>
</li><br/>
<li> Defragmenting a disk is <em>most</em> useful for which type of file system?
	<ol class="answer_list">
	<li> Contiguous Allocation
	<li> Linked List Allocation
	<li> Index Node
	<li> Virtual File System
	<li> None of the above
	</ol>
</li><br/>

<li> A kilobyte (KB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> A gigabyte (GB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> A megabyte (MB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> A terabyte (TB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> A petabyte (PB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> An exabyte (EB) is how many bytes?
	<ol class="answer_list">
	<li> 2^10
	<li> 2^20
	<li> 2^30
	<li> 2^40
	<li> 2^50
	<li> 2^60
	</ol>
</li><br/>
<li> Which of the following are examples of a contiguous allocation file
	system?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660 with Joliet Extensions
	<li> ISO 9660 with Rock Ridge Extensions
	<li> ext2
	<li> ext4
	<li> Network File System (NFS)
	<li> Virtual File System (VFS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are examples of a linked list allocation file 
	system?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660 with Joliet Extensions
	<li> ISO 9660 with Rock Ridge Extensions
	<li> ext2
	<li> ext4
	<li> Network File System (NFS)
	<li> Virtual File System (VFS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are examples of an index-node allocation file 
	system?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660 with Joliet Extensions
	<li> ISO 9660 with Rock Ridge Extensions
	<li> ext2
	<li> ext4
	<li> Network File System (NFS)
	<li> Virtual File System (VFS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following files systems provide journaling?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660
	<li> ext2
	<li> NTFS
	<li> ext4
	<li> ReFS
	<li> Btrfs
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following files systems support hard links?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660
	<li> ext2
	<li> NTFS
	<li> ext4
	<li> ReFS
	<li> Btrfs
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following files systems support soft links?
	<ol class="answer_list">
	<li> FAT-32
	<li> ISO 9660
	<li> ext2
	<li> NTFS
	<li> ext4
	<li> ReFS
	<li> Btrfs
	<li> None of the above
	</ol>
</li><br/>

<li> File servers enabled the use of inexpensive client machines by
	<ol class="answer_list">
	<li> hosting users' file systems that were remotely connected to by
		the clients.
	<li> providing the vast majority of the computational power,
		displaying the results back to the client (e.g., via X11).
	<li> performing the virtual memory management for the client - thus
		greatly simplifying its hardware.
	<li> treating them as dumb terminals, so that <strong>all</strong> of
		the processing and information display is controlled by the
		server. 
	<li> None of the above
	</ol>
</li><br/>
<li> The purpose of concurrency control, by a file server, is to
	<ol class="answer_list">
	<li> enable files to be locked so that only the process with the
		lock can use the file (until the lock is released).
	<li> allow true simultaneous actions to occur (even on a single
		CPU system).
	<li> achieve serializability, even if operations/actions are
		interruptible.
	<li> ensure that operations are <strong>always</strong> performed
		uninterrupted, by wrapping <strong>all</strong> actions
		inside of critical sections controlled by semaphores. 
	<li> None of the above
	</ol>
</li><br/>
<li> Problems can occur when file data is only partially updated (e.g., due
	to a system failure), as in the case of a bank's transferring money
	from savings to checking. Which of the following ensures that no
	partial updates are performed?
	<ol class="answer_list">
	<li> Serializability
	<li> Transactions
	<li> Concurrency Control
	<li> Replicated Files
	<li> None of the above
	</ol>
</li><br/>
<li> Replicating files as they are modified (copy-on-write) provides a degree of
	<ol class="answer_list">
	<li> concurrency control since each process changing a file gets its own
		copy.
	<li> fault tolerance by making copies of files that can be used if the
		primary server is unavailable.
	<li> atomic transaction protection, by ensuring that
		<strong>all</strong> partial updates fail (though some
		completed updates could also fail).
	<li> performance improvement over in-place modification, since only the
		changed parts of the file need to be copied.
	<li> None of the above
	</ol>
</li><br/>
<li> Transactions are atomic, which requires that
	<ol class="answer_list">
	<li> no actual changes to the file are visible in the file system
		until the transaction is committed.
	<li> <strong>all</strong> transactions which change information
		<strong>must</strong> be kept on a stable store (e.g.,
		phase-change memory) until the transaction is committed.
	<li> file system journaling (or something like it)
		<strong>must</strong> be in place to ensure
		transactions survive file system failures.
	<li> file system log structure files <strong>must</strong> be kept
		to ensure serializability of multiple file system transactions.
	<li> None of the above
	</ol>
</li><br/>
<li> The Network File System (NFS) uses a client-server protocol that works by
	<ol class="answer_list">
	<li> copying requested files from the server to the client and then
		copying them back once the client has finished with them.
	<li> having the server keep track of a file pointer on behalf of the
		client, so when the client makes read/write requests, the
		server knows exactly where in the file to perform the actions.
	<li> providing the client with a file handle that it uses to retrieve
		(read) portions of the file from the server or to request file
		modifications (write) based on file offsets.
	<li> locking the remote file so that only the client has access to it
		while it's being used.
	<li> None of the above
	</ol>
</li><br/>
<li> The Network File System (NFS) uses a client-server protocol
	<ol class="answer_list">
	<li> that is stateless, requiring any requested file modifications be
		committed before the Remote Procedure Call (RPC) returns
		results.
	<li> that is stateless, so that failure of the file server won't
		necessarily impact the client after the file server is
		recovered.
	<li> that is stateful, thus enabling the client to fail and then
		recover <strong>without</strong> impacting any pending file
		transactions.
	<li> that is stateful, making the implementation of NFS much simpler.
	<li> None of the above
	</ol>
</li><br/>
<li> The Network File System (NFS) uses a client-server protocol
	<ol class="answer_list">
	<li> that requires any programs that access remote files to be
		modified and recompiled so that they can use the NFS API.
	<li> that requires any programs that access remote files to be be	
		recompiled so that they can use the NFS version of system
		call implementations (though no program code modifications
		are necessary).
	<li> which is incompatible with the Virtual File System (VFS), thus
		most clients that use NFS are diskless (i.e., they get
		<strong>all</strong> of their files remotely).
	<li> that has no impact on user programs since their file access is
		through the Virtual File System (VFS) of which NFS is just
		one participating file system.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file systems can distribute the information
	from one file system instance across multiple computer systems?
	<ol class="answer_list">
	<li> Network File System (NFS)
	<li> Lustre
	<li> Hadoop
	<li> Btrfs
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following file systems are horizontally scalable (i.e., they
	can handle more files and file operations within the same file system
	instance by adding more computer systems)?
	<ol class="answer_list">
	<li> Network File System (NFS)
	<li> Lustre
	<li> Hadoop
	<li> Btrfs
	<li> None of the above
	</ol>
</li><br/>
<li> The Lustre File System (LFS) uses the metadata server to
	<ol class="answer_list">
	<li> perform pathname and permission checks
	<li> store file information (e.g., filenames, directories, file layout,
		access permissions).
	<li> store the actual file contents.
	<li> store which of its object storage targets holds the various parts
		of the files its responsible for.
	<li> None of the above
	</ol>
</li><br/>
<li> The Lustre File System (LFS) uses the metadata targets to
	<ol class="answer_list">
	<li> perform pathname and permission checks
	<li> store file information (e.g., filenames, directories, file layout,
		access permissions).
	<li> store the actual file contents.
	<li> store which of its object storage targets holds the various parts
		of the files its responsible for.
	<li> None of the above
	</ol>
</li><br/>
<li> The Lustre File System (LFS) uses the object storage target to
	<ol class="answer_list">
	<li> perform pathname and permission checks
	<li> store file information (e.g., filenames, directories, file layout,
		access permissions).
	<li> store the actual file contents.
	<li> store which of its object storage targets holds the various parts
		of the files its responsible for.
	<li> None of the above
	</ol>
</li><br/>
<li> The Lustre File System (LFS) uses the object storage servers to
	<ol class="answer_list">
	<li> perform pathname and permission checks
	<li> store file information (e.g., filenames, directories, file layout,
		access permissions).
	<li> store the actual file contents.
	<li> store which of its object storage targets holds the various parts
		of the files its responsible for.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following Lustre File System (LFS) components is
	implemented as an enhanced version of ext4?
	<ol class="answer_list">
	<li> metadata source
	<li> metadata target
	<li> object storage source
	<li> object storage target
	<li> None of the above
	</ol>
</li><br/>
<li> The Hadoop file system uses
	<ol class="answer_list">
	<li> only 1 namenode.
	<li> 1 or more namenodes.
	<li> only 1 datanode.
	<li> 1 or more datanodes.
	<li> None of the above
	</ol>
</li><br/>
<li> Clients using the Hadoop file system work directly with
	<ol class="answer_list">
	<li> only the namenode(s).
	<li> only the datanode(s).
	<li> both the namenode(s) and datanode(s).
	<li> neither since <strong>all</strong> interactions with Hadoop
		<strong>must</strong> be mediated by the Virtual File System
		(VFS).
	<li> None of the above
	</ol>
</li><br/>
<li> Hadoop datanodes get the file data they are to store
	<ol class="answer_list">
	<li> only from the namenode(s), which gets it from the client.
	<li> only from the client.
	<li> from both the client (1st datanode) and other datanodes.
	<li> from the client, namenode(s), and other datanodes depending on the
		circumstances.
	<li> None of the above
	</ol>
</li><br/>
<li> In Hadoop, the following are responsible for determining how many
	replicates to make of the file data:
	<ol class="answer_list">
	<li> client.
	<li> namenode(s).
	<li> datanode(s).
	<li> both the namenode(s) and datanode(s) via a majority voting
		algorithm.
	<li> None of the above
	</ol>
</li><br/>
<li> In Hadoop, the following are responsible for determining which datanodes
	are to be used for storing the file data:
	<ol class="answer_list">
	<li> client.
	<li> namenode(s).
	<li> datanode(s).
	<li> both the namenode(s) and datanode(s) via a majority voting
		algorithm.
	<li> None of the above
	</ol>
</li><br/>
<li> Hadoop writes files using a block size that is determined by 
	<ol class="answer_list">
	<li> the client.
	<li> the namenode(s).
	<li> the datanode(s).
	<li> both the namenode(s) and datanode(s) via a majority voting
		algorithm.
	<li> None of the above
	</ol>
</li><br/>
<li> When a client wants to access a file stored in Hadoop, it gets the list of
	file blocks and the nodes on which the file data is stored from:
	<ol class="answer_list">
	<li> only the namenode(s).
	<li> only the datanode(s).
	<li> either a namenode(s) or datanode(s) depending upon which is able to
		handle the request first.
	<li> None of the above
	</ol>
</li><br/>
<li> In Hadoop, once a client has the list of blocks and nodes on which the
	file data resides, it retrieves the data by
	<ol class="answer_list">
	<li> asking a namenode to get the file data blocks, assemble them,
		and return them to the client.
	<li> asking a datanode to get the file data blocks, assemble them,
		and return them to the client.
	<li> asking each namenode to get the file blocks from the datanode(s)
		it's responsible for.
	<li> retrieving each of the file data blocks directly from datanode(s).
	<li> None of the above
	</ol>
</li><br/>
<li> What are the three types of failure that can be seen with the Hadoop
	(and Lustre) file systems?
	<ol class="answer_list">
	<li> Client failure.
	<li> Node or target/server failure.
	<li> Communication failure.
	<li> Data corruption.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following remote file systems have at least one single point
	of failure?
	<ol class="answer_list">
	<li> Network File System (NFS)
	<li> Lustre
	<li> Hadoop
	<li> None of the above
	</ol>
</li><br/>
<li> What conditions does Hadoop use to identify network and/or node failures?
	<ol class="answer_list">
	<li> A namenode fails to receive a heartbeat message from a datanode
		for more than 10 minutes.
	<li> A namenode doesn't hear from one of its clients for more than
		10 minutes.
	<li> A datanode reports its unavailability to its namenode.
	<li> A sender <strong>not</strong> receiving an acknowledgement for
		a sent message after several retries.
	<li> None of the above
	</ol>
</li><br/>

<li> The primary kinds of devices are
	<ol class="answer_list">
	<li> character devices.
	<li> linear devices.
	<li> parallel devices.
	<li> block devices.
	<li> None of the above
	</ol>
</li><br/>
<li> What kind of device works with information in fixed size blocks and often
	provides random access?
	<ol class="answer_list">
	<li> character devices.
	<li> linear devices.
	<li> parallel devices.
	<li> block devices.
	<li> None of the above
	</ol>
</li><br/>
<li> What kind of device works with information as a sequence of bytes and
	only provides sequentially access?
	<ol class="answer_list">
	<li> character devices.
	<li> linear devices.
	<li> parallel devices.
	<li> block devices.
	<li> None of the above
	</ol>
</li><br/>
<li> What software interacts with the device controller to enable the OS to
	utilize a device?
	<ol class="answer_list">
	<li> I/O interface
	<li> Device driver
	<li> Interrupt driver
	<li> Control bus
	<li> None of the above
	</ol>
</li><br/>
<li> The primary nature of an I/O device is
	<ol class="answer_list">
	<li> mechanical and based on material properties. 
	<li> electronic (circuits).
	<li> software.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary nature of an device controller is
	<ol class="answer_list">
	<li> mechanical and based on material properties. 
	<li> electronic (circuits).
	<li> software.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary nature of an device driver is
	<ol class="answer_list">
	<li> mechanical and based on material properties. 
	<li> electronic (circuits).
	<li> software.
	<li> None of the above
	</ol>
</li><br/>
<li> Each device controller
	<ol class="answer_list">
	<li> controls only a single device.
	<li> may control many devices.
	<li> communicates with its corresponding device driver.
	<li> <strong>must</strong> have its own dedicated bus to both the CPU
		and the Direct Memory Access (DMA) controller.
	<li> None of the above
	</ol>
</li><br/>
<li> Devices controllers have
	<ol class="answer_list">
	<li> support for <strong>all</strong> types of devices, so that any
		device can utilize any type of device controller.
	<li> data buffers that can be written to or read from.
	<li> control registers to communicate with the CPU.
	<li> their own instruction sets that <strong>must</strong> be
		supported by the CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> Port I/O, used by early computers,
	<ol class="answer_list">
	<li> assigns each control register to its corresponding I/O port.
	<li> is very slow since large data transfers <strong>must</strong>
		be done by reading/writing data in many small chunks (using
		the control registers).
	<li> the I/O ports are separate from memory (requiring protected
		instructions to read/write the I/O ports).
	<li> requires some portion of the device drivers to be written in
		assembly.
	<li> cannot use device drivers as the OS kernel <strong>must</strong>
		directly control the device.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantages of Memory-Mapped I/O are
	<ol class="answer_list">
	<li> reduces the number of CPU executed instructions to put information
		into control registers.
	<li> ease of protecting the control registers by <strong>not</strong>
		putting the mapped memory in a user accessible address space.
	<li> data can be read/written directly to the device since the device
		buffer is mapped to memory.
	<li> device drivers can be written in a higher-level language (e.g.,
		C) enabling them to be more portable.
	<li> it makes effective use of both cached memory and the memory bus
		between main memory and the CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> The drawbacks of Port I/O include:
	<ol class="answer_list">
	<li> very slow transfers of large data amounts since they
		<strong>must</strong> read/write data in many small chunks
		(using the control registers).
	<li> the device controller data buffers cannot be utilized.
	<li> increases the number of CPU executed instructions to communicate
		with the device controller.
	<li> access to the control registers <strong>must</strong> be
		restricted/protected.
	<li> None of the above
	</ol>
</li><br/>
<li> The drawbacks of Memory-Mapped I/O include:
	<ol class="answer_list">
	<li> caching <strong>must</strong> be disabled for memory mapped
		control registers.
	<li> the device controller data buffers cannot be utilized.
	<li> increases the number of CPU executed instructions to communicate
		with the device controller.
	<li> the MMU <strong>must</strong> determine which bus to use
		(based on the address) if there's a dedicated bus between
		the CPU and main memory in addition to the system bus. 
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following uses the CPU to copy data into (or out of) memory
	from a device (e.g., HDD) using a tight read/write loop?
	<ol class="answer_list">
	<li> Programmed I/O
	<li> Memory-Mapped I/O
	<li> Device to Device copy (D2D)
	<li> Direct Memory Access (DMA)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following has the CPU setup and initiate the copying of
	data into (or out of) memory, but the CPU is free to do other actions
	while the copying occurs?
	<ol class="answer_list">
	<li> Programmed I/O
	<li> Memory-Mapped I/O
	<li> Device to Device copy (D2D)
	<li> Direct Memory Access (DMA)
	<li> None of the above
	</ol>
</li><br/>
<li> If a system has a Direct Memory Access (DMA) capability, the process of
	writing a particular set of data directly to memory (or a device),
	instead of going through the DMA, is called
	<ol class="answer_list">
	<li> burst mode.
	<li> direct mode.
	<li> fly-by mode.
	<li> cycle stealing.
	<li> None of the above
	</ol>
</li><br/>
<li> When the Direct Memory Access (DMA) transfers a chunk of data a single
	word at a time, by grabbing the system bus for short periods of time,
	this is called
	<ol class="answer_list">
	<li> burst mode.
	<li> direct mode.
	<li> fly-by mode.
	<li> cycle stealing.
	<li> None of the above
	</ol>
</li><br/>
<li> When the Direct Memory Access (DMA) transfers a chunk of data
	<strong>all</strong> at once, this is called
	<ol class="answer_list">
	<li> burst mode.
	<li> direct mode.
	<li> fly-by mode.
	<li> cycle stealing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following makes efficient use of the system bus when large
	amounts of data <strong>must</strong> be transferred by the
	Direct Memory Access (DMA) unit?
	<ol class="answer_list">
	<li> burst mode.
	<li> direct mode.
	<li> fly-by mode.
	<li> cycle stealing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following makes efficient use of both the CPU and system bus
        when a single word of data <strong>must</strong> be
        transferred by the Direct Memory Access (DMA) unit?
	<ol class="answer_list">
	<li> burst mode.
	<li> direct mode.
	<li> fly-by mode.
	<li> cycle stealing.
	<li> None of the above
	</ol>
</li><br/>
<li> Device controllers generate interrupts by
	<ol class="answer_list">
	<li> writing specific values to the control registers.
	<li> having the CPU periodically check/ping each controller.
	<li> using the Direct Memory Access (DMA) unit to write a special
		value into a variable the OS kernel monitors.
	<li> putting a specific signal on the system bus.
	<li> None of the above
	</ol>
</li><br/>
<li> An interrupt is serviced only if
	<ol class="answer_list">
	<li> no other interrupts are currently being handled.
	<li> there's no process currently running in the CPU.
	<li> no higher-priority interrupts are simultaneously received.
	<li> the system bus is currently operating in direct mode.
	<li> None of the above
	</ol>
</li><br/>
<li> The value that the interrupt controller puts on the system bus address
	lines represents
	<ol class="answer_list">
	<li> an index into the interrupt vector.
	<li> the priority of the interrupt.
	<li> the time-stamp of when the interrupt occurred.
	<li> the beginning address of the interrupt handler.
	<li> None of the above
	</ol>
</li><br/>
<li> The interrupt vector is an array that holds
	<ol class="answer_list">
	<li> the priorities of the different kinds of interrupts (the
		interrupt number used as the array index).
	<li> a list of the currently pending interrupts as a queue.
	<li> references to the set of control registers associated with
		the device that generates that interrupt (the interrupt
		number is used as the array index). 
	<li> the beginning address of the interrupt handler for
		<strong>all</strong> possible interrupts (the interrupt number
		is used as the array index).
	<li> None of the above
	</ol>
</li><br/>
<li> When the interrupt controller handles an interrupt it 
	<ol class="answer_list">
	<li> interrupts the CPU.
	<li> prompts the CPU to save the state of the current process
		<em>before</em> running the interrupt handler.
	<li> restores the state of the previous process to the CPU
		<em>after</em> the interrupt handler finishes.
	<li> runs the next process from the ready "queue"
		<em>after</em> the interrupt handler finishes.
	<li> None of the above
	</ol>
</li><br/>
<li> Interrupts for which the instructions <em>before</em> the program counter
	(PC) have <strong>all</strong> been completed and those
	appearing <em>after</em> the PC have NOT been completed, are called
	<ol class="answer_list">
	<li> precision interrupts.
	<li> precise interrupts.
	<li> imprecise interrupts.
	<li> ambiguous interrupts.
	<li> None of the above
	</ol>
</li><br/>
<li> Interrupts for which some instructions <em>before</em> the PC may NOT
	have completed, while some instructions <em>after</em> the PC may
	been have completed, are called
	<ol class="answer_list">
	<li> precision interrupts.
	<li> precise interrupts.
	<li> imprecise interrupts.
	<li> ambiguous interrupts.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following are <strong>not</strong> goals of I/O software?
	<ol class="answer_list">
	<li> Handling errors at the lowest level possible.
	<li> Making programs independent of the actual device(s) used.
	<li> Supporting device names that are independent of the actual
		device(s) used.
	<li> When practical, using buffering for block device communications.
	<li> None of the above
	</ol>
</li><br/>
<li> Asynchronous I/O 
	<ol class="answer_list">
	<li> can be made to look synchronous by the OS.
	<li> is often easier to program with.
	<li> is generally enabled by Direct Memory Access (DMA).
	<li> typically enables other actions to be performed while the I/O
		is occurring.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O uses the CPU to directly perform the reading/writing of
	data to/from devices?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O is the simplest in design, but prevents the CPU from
	doing other work while I/O is occurring?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O may require the copying of data from a user's address
	space to the kernel address space?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O often reads/writes data from/to a device a single
	byte at a time using a tight loop run by the CPU?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O software uses polling to determine whether or not
	a device is ready for the piece of data to be read/written?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O blocks the reading/writing process so that no time is
	spent busy waiting?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of I/O makes the most efficient use of the CPU resource when
	large chunks of data <strong>must</strong> be read/written?
	<ol class="answer_list">
	<li> Directed I/O
	<li> Programmed I/O
	<li> Interrupt-Driven I/O
	<li> I/O via DMA
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following are part of the 4 layers of I/O software?
	<ol class="answer_list">
	<li> Device controller firmware
	<li> Interrupt handlers
	<li> Device drivers
	<li> User level I/O software
	<li> None of the above
	</ol>
</li><br/>
<li> An easy and efficient way to deal with I/O interrupts from devices is to
	<ol class="answer_list">
	<li> <strong>always</strong> use Direct Memory Access (DMA) for I/O
		so that interrupts aren't generated.
	<li> have the device driver block when I/O is initiated, so the
		interrupt handler simply unblocks the driver when the interrupt
		is received.
	<li> let the device driver run as part of the OS so that a full
		context switch isn't necessary when an interrupt unblocks a
		device driver.
	<li> combine the interrupt handler and device driver into a single
		user level system call so that <strong>all</strong> parts of
		the I/O software stack are run as a single unit.
	<li> None of the above
	</ol>
</li><br/>
<li> Device driver code, particularly in the consumer market, is most commonly
	<ol class="answer_list">
	<li> installed on the device controller.
	<li> compiled into the OS.
	<li> dynamically loaded into the OS.
	<li> no longer needed as all device manufacturers are moving to a
		single standard API.
	<li> None of the above
	</ol>
</li><br/>
<li> The device dependent code for performing I/O resides in which I/O software
	layer(s)?
	<ol class="answer_list">
	<li> Device controller firmware
	<li> Interrupt handlers
	<li> Device drivers
	<li> User level I/O software
	<li> None of the above
	</ol>
</li><br/>
<li> Devices drivers are
	<ol class="answer_list">
	<li> normally provided by the device manufacturer, but usually only
		for the most popular OSs.
	<li> <strong>always</strong> written to block during an I/O operation.
	<li> often run as part of the OS kernel, to avoid the overhead of a
		full context switch.
	<li> coded using static variables to reduce the size of the run-time
		stack, since the device driver only services one request at
		a time.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following help make I/O software independent of the specific
	device being used?
	<ol class="answer_list">
	<li> Using file names as the names for devices.
	<li> Controlling access to devices through file ownership and access
		permissions.
	<li> Adherence to a common and uniform Application Programming
		Interface (API).
	<li> Enable user programs, via system calls, to access specialized
		features of devices via manufacturer specific control codes.
	<li> None of the above
	</ol>
</li><br/>
<li> Device independent capabilities generally include mechanisms to control:
	<ol class="answer_list">
	<li> interrupt delay and timeouts.
	<li> buffering.
	<li> error reporting.
	<li> device acquisition and release.
	<li> the device data block size.
	<li> None of the above
	</ol>
</li><br/>
<li> Spooling (simultaneous peripheral operations on-line) is a technique that
	<ol class="answer_list">
	<li> uses buffering to prevent slow devices (e.g., printers) from
		making the CPU busy wait.
	<li> allows non-shareable devices to be used serial, but appear to
		user software as if they are being shared.
	<li> is only used in conjunction with printing devices.
	<li> uses a single program to operate a non-shareable device.
	<li> None of the above
	</ol>
</li><br/>
<li> System calls (e.g., read, write) and library routines (e.g., fprintf)
	are provided by which I/O software layer.
	<ol class="answer_list">
	<li> Device controller firmware
	<li> Interrupt handlers
	<li> Device drivers
	<li> User level I/O software
	<li> None of the above
	</ol>
</li><br/>

<li> RAID systems rely on using multiple disks in order to
	<strong>always</strong> be able to
	<ol class="answer_list">
	<li> recover from a single disk failure.
	<li> determine whether or not a (single bit) error has occured (e.g.,
		corrupted data on one of the disks).
	<li> detect and correct a (single bit) error (e.g., corrupted data on
		one of the disks).
	<li> provide faster read and write performance based on data stripping.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following RAID levels support faster data access by using data
	stripping?
	<ol class="answer_list">
	<li> Level 0
	<li> Level 1
	<li> Level 2
	<li> Level 3
	<li> Level 4
	<li> Level 5
	<li> Level 6
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following RAID levels support faster read performance without
	the use of stripping?
	<ol class="answer_list">
	<li> Level 0
	<li> Level 1
	<li> Level 2
	<li> Level 3
	<li> Level 4
	<li> Level 5
	<li> Level 6
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following RAID levels support detection of at least one
	drive/bit error?
	<ol class="answer_list">
	<li> Level 0
	<li> Level 1
	<li> Level 2
	<li> Level 3
	<li> Level 4
	<li> Level 5
	<li> Level 6
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following RAID levels support the correction (or recovery)
	of data when two drives fail simultaneously?
	<ol class="answer_list">
	<li> Level 0
	<li> Level 1
	<li> Level 2
	<li> Level 3
	<li> Level 4
	<li> Level 5
	<li> Level 6
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following RAID levels only require modifications to
	one drive for a write operation?
	<ol class="answer_list">
	<li> Level 0
	<li> Level 1
	<li> Level 2
	<li> Level 3
	<li> Level 4
	<li> Level 5
	<li> Level 6
	<li> None of the above
	</ol>
</li><br/>
<li> RAID implemented in software is
	<ol class="answer_list">
	<li> typically more expensive than hardware RAID implementations.
	<li> <strong>always</strong> able to support the OS boot process. 
	<li> about the same speed as hardware RAID implementations.
	<li> provided by many OSs.
	<li> None of the above
	</ol>
</li><br/>
<li> RAID implemented in hardware is
	<ol class="answer_list">
	<li> typically more expensive than software RAID implementations.
	<li> <strong>always</strong> able to support the OS boot process. 
	<li> generally faster than software RAID implementations.
	<li> commonly found on most consumer/commodity computer systems.
	<li> None of the above
	</ol>
</li><br/>

<li> On a keyboard, an interrupt is generated
	<ol class="answer_list">
	<li> only when a key is pressed.
	<li> only when a key is released.
	<li> every N milliseconds (as configured by the device driver) for as
		long as the key is held down.
	<li> once when the key is pressed, and once when it is released.
	<li> None of the above
	</ol>
</li><br/>
<li> A device driver for supporting keyboards that passes along every typed
	character (including backspacing) supports input in
	<ol class="answer_list">
	<li> canonical mode.
	<li> non-canonical mode.
	<li> half-duplex mode.
	<li> full-duplex mode.
	<li> None of the above
	</ol>
</li><br/>
<li> Which device driver mode for supporting keyboards works
	<strong>best</strong> for full screen editors (e.g., vi, emacs)?
	<ol class="answer_list">
	<li> canonical mode.
	<li> non-canonical mode.
	<li> half-duplex mode.
	<li> full-duplex mode.
	<li> None of the above
	</ol>
</li><br/>
<li> Which device driver mode for supporting keyboards works
	<strong>best</strong> for command line programs (e.g., shell)?
	<ol class="answer_list">
	<li> canonical mode.
	<li> non-canonical mode.
	<li> half-duplex mode.
	<li> full-duplex mode.
	<li> None of the above
	</ol>
</li><br/>
<li> The device driver approach for supporting keyboards that is character
	oriented is also called
	<ol class="answer_list">
	<li> canonical mode.
	<li> non-canonical mode.
	<li> half-duplex mode.
	<li> full-duplex mode.
	<li> raw mode.
	<li> cooked mode.
	<li> None of the above
	</ol>
</li><br/>
<li> The device driver approach for supporting keyboards that is line
	oriented is also called
	<ol class="answer_list">
	<li> canonical mode.
	<li> non-canonical mode.
	<li> half-duplex mode.
	<li> full-duplex mode.
	<li> raw mode.
	<li> cooked mode.
	<li> None of the above
	</ol>
</li><br/>
<li> For a mouse, an interrupt is generated
	<ol class="answer_list">
	<li> each time a button is pressed.
	<li> each time a button is released.
	<li> every 40 milliseconds (as configured by the device driver)
		regardless of what the user does with the mouse.
	<li> whenever the mouse has traveled a predetermined minimum distance.
	<li> None of the above
	</ol>
</li><br/>
<li> Mouse input, unlike keyboard input, 
	<ol class="answer_list">
	<li> only supports raw mode.
	<li> only supports cooked mode.
	<li> supports both raw and cooked modes.
	<li> doesn't support either raw or cooked modes, but uses echoing
		instead. 
	<li> None of the above
	</ol>
</li><br/>
<li> While touch screens operate much the same way that mice do, a notable
	difference is that
	<ol class="answer_list">
	<li> mice have button(s) that can be pressed and released, whereas
		touch screens do <strong>not</strong> have a comparable
		capability.
	<li> mice suffer fromm the ghosting problem, whereas touch screens
		do <strong>not</strong>.
	<li> touch screens supply the absolute position of a touch instead of
		the relative position which mice provide.
	<li> touch screens can support multitouch whereas mice do
		<strong>not</strong> have a comparable capability.
	<li> None of the above
	</ol>
</li><br/>
<li> In order to support multitouch, touch screens should provide a continuous
	stream of position data 
	<ol class="answer_list">
	<li> in order to create the relative position data that it needs. 
	<li> to avoid the ghosting problem (i.e., the touch position is
		ambiguous).
	<li> otherwise data reported at discrete time increments will be
		understood as multiple separate touches (e.g., mouse clicks).
	<li> to prevent echoing, in which a single touch point may look like
		multiple touches.
	<li> None of the above
	</ol>
</li><br/>
<li> Text (or terminal) windows use
	<ol class="answer_list">
	<li> special character sequences to position the cursor and perform
		text insertion and deletion.
	<li> mixed sized text and different font styles to support document
		processing (e.g., Microsoft Word).
	<li> terminal capability (termcap) libraries to provide device
		independent support.
	<li> the raw scan codes from the keyboard, which is why they were
		commonly used on early computers.
	<li> None of the above
	</ol>
</li><br/>
<li> X11 is a windowing system
	<ol class="answer_list">
	<li> commonly used by Linux based systems.
	<li> allows programs to be run on one computer, with the display and
		interaction occurring on another computer.
	<li> uses a client-server model to separate the program's GUI operation
		from the rest of the program.
	<li> that is event driven. 
	<li> None of the above
	</ol>
</li><br/>
<li> Microsoft Windows and X11 differ in that
	<ol class="answer_list">
	<li> X11 is implemented as part of the OS kernel.
	<li> X11 user programs <strong>must</strong> explicitly coordinate the
		communication between the client and server components.
	<li> Microsoft Windows is portable and relatively easy to maintain.
	<li> Microsoft Windows combines the windowing and GUI elements together
		within the OS.
	<li> None of the above
	</ol>
</li><br/>
<li> Graphical User Interfaces (GUIs) are characterized by their use of
	<ol class="answer_list">
	<li> Windows
	<li> Icons
	<li> Menus
	<li> Pointing devices
	<li> None of the above
	</ol>
</li><br/>
<li> Unlike text windows, graphical user interfaces (GUIs)
	<ol class="answer_list">
	<li> are slower since they <strong>must</strong> use Programmed I/O.
	<li> require a graphics processing unit (GPU) be part of the hardware.
	<li> leverage a common graphical API (e.g., OpenGL) to make the
		software more portable.
	<li> <strong>must</strong> use the integerated graphics provided by
		the computer hardware (on the motherboard).
	<li> None of the above
	</ol>
</li><br/>

<li> Computer clocks and timers controlled by the OS are used to
	<ol class="answer_list">
	<li> support preemptive process/thread scheduling.
	<li> control the speed at which the CPU performs instructions.
	<li> time order independent (serializable) events.
	<li> prevent any simultaneous actions occurring within the computer.
	<li> None of the above
	</ol>
</li><br/>
<li> The computer clock hardware uses a crystal (tuned to a specific frequency) 
	to
	<ol class="answer_list">
	<li> cause a special register to count down to 0, which then
		generates an interrupt.
	<li> cause a special register to count the number of ticks since the
		epoch.
	<li> directly generate timing interrupts for the computer.
	<li> None of the above
	</ol>
</li><br/>
<li> Programmable clocks
	<ol class="answer_list">
	<li> can be used as a complete replacement for clock hardware.
	<li> are able to generate periodic clock interrupts.
	<li> read the special clock register value, and perform specific
		actions when particular values are reached.
	<li> alter the starting value of the special clock register to
		control the interval between clock generated interrupts.
	<li> None of the above
	</ol>
</li><br/>
<li> The calendar date and "wall" clock time are calculated
	<ol class="answer_list">
	<li> based on the number of clock ticks (usually in microseconds)
		since a specific starting date and time.
	<li> by keeping track of the years, days (0-365), hours (0-24),
		minutes (0-60), and seconds (0-60)
		that have elapsed based on regular clock interrupts. 
	<li> on a daily (24 hour clock) basis at midnight, with the time each
		day kept as a count of the microseconds since the last day.
	<li> by contacting an internet based time keeping service whenever
		the current date and time is needed.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> a usual responsibility
	of a clock (device) driver?
	<ol class="answer_list">
	<li> preventing processes from running too long.
	<li> collecting profiling and statistics information on processes.
	<li> maintaining the time of day.
	<li> providing watchdog timers.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are functionality often supported by
	the clock (device) driver?
	<ol class="answer_list">
	<li> Polling of some input devices (e.g., mice, touch screens).
	<li> CPU accounting.
	<li> Software profiling.
	<li> Watchdog timers.
	<li> None of the above
	</ol>
</li><br/>
<li> To keep track of the time of day, the OS typically
	<ol class="answer_list">
	<li> keeps track of the year, the day within the year, and
		the number of milliseconds within the current day.
	<li> keeps track of the year and counts the number of microseconds
		 within the current year.
	<li> keeps track of the year and counts the number of clock ticks
		 within the current year.
	<li> counts the number of clock ticks since the epoch.
	<li> None of the above
	</ol>
</li><br/>
<li> Watchdog timers often use a direct procedure call rather than causing a
	signal because
	<ol class="answer_list">
	<li> they are just another type of clock, and usually call an alarm
		procedure that in turn generates the signal.
	<li> as a special type of alarm, they are only used within the same
		process.
	<li> for user processes, it reduces the lag between the timer firing
		and the associated work being performed.
	<li> it's faster and the call is for kernel processes so the watchdog
		doesn't need to do a context switch.
	<li> None of the above
	</ol>
</li><br/>
<li> Alarms are set using a system call and are usually implemented as a
	<ol class="answer_list">
	<li> sorted queue of times when an interrupt should be generated.
	<li> special register that holds the next alarm time. However, only
		one alarm can be set/active at a time.
	<li> separate process that the clock (device) driver signals on a
		periodic basis.
	<li> set of count down registers, with a separate register
		corresponding to each alarm.
	<li> None of the above
	</ol>
</li><br/>
<li> Alarms can be implemented using either absolute or relative times.
	<ol class="answer_list">
	<li> Absolute times are more accurate since using relative times allows
		small inaccuracies in alarm activations to grow over time.
	<li> Relative times are preferred since even the 64 bit registers of
		modern computers are <strong>not always</strong> large enough
		to hold the appropriate absolute time relative to the epoch.
	<li> Relative times provide greater precision and accuracy than
		absolute times.
	<li> The are no advantages of using either absolute or relative times
		over the other for implementing alarms.
	<li> None of the above
	</ol>
</li><br/>
<li> A simple but less accurate way to implement CPU accounting for processes
	is to have a single counter associated with each process in the
	proc_table, incrementing the counter for the currently running process
	whenever
	<ol class="answer_list">
	<li> an interrupt handler is started. The counter value divided by the
		sum of <strong>all</strong> such values, is the proportion of
		the CPU the process has used.
	<li> the process uses its full time quantum. Charging the process 1.5
		times the counter value.
	<li> the process becomes blocked. Charging the process 0.5 times the
		counter value.
	<li> a clock tick occurs.  Charging the currently running process for
		the full clock tick.
	<li> None of the above
	</ol>
</li><br/>
<li> If a timer with an additional, but different, frequency than the main
	system timer is needed, and accuracy to within 20 microseconds is
	sufficient, which of the following is an efficient solution?
	<ol class="answer_list">
	<li> Use an ALARM system call that resets every time it is triggered. 
	<li> Have a separate process poll the system clock, triggering an
		event when the desired (recurring) time interval is reached.
	<li> Use a "soft timer" based on the frequent running of the OS
		kernel to check for the desired time interval.
	<li> Adjust the starting value of the programmable clock to correspond
		to the alternate frequency.
	<li> None of the above
	</ol>
</li><br/>

<li> The tendency for two or more computer clocks in the same location to report
	different times after a long period is
	<ol class="answer_list">
	<li> the clock synchronization dilemma.
	<li> due primarily to the effects of general relativity.
	<li> a result of clock drift.
	<li> mostly caused by small variations in the frequency of AC power
		used.
	<li> None of the above
	</ol>
</li><br/>
<li> Computer based clocks are typically accurate to between
	<ol class="answer_list">
	<li> 1 - 10 ppm (about 0.5 - 5.25 minutes / year)
	<li> 10 ppb - 5 ppm (about 0.315 - 155 seconds / year)
	<li> 1 ppb - 2 ppm (about 0.0315 -  60 seconds / year)
	<li> 1 - 5 ppb (about 0.0315 - 0.1575 seconds / year)
	<li> None of the above
	</ol>
</li><br/>
<li> Due to General Relativity clocks will have different speeds based on differences
	in their their
	<ol class="answer_list">
	<li> velocity.
	<li> acceleration.
	<li> mass. 
	<li> gravitational field.
	<li> None of the above
	</ol>
</li><br/>
<li> Solving the clock synchronization problem (so the clocks read the exact
	same time) for computers in different locations is
	<ol class="answer_list">
	<li> <strong>not</strong> possible.
	<li> simply a matter of having more accurate clocks.
	<li> possible by using a single shared clock.
	<li> doable by using multiple highly accurate clocks
		(e.g., GPS satellites) with computers periodically
		averaging the set of times they get.
	<li> None of the above
	</ol>
</li><br/>
<li> Given two events that occurred on different computer systems, it is
	<ol class="answer_list">
	<li> <strong>always</strong> possible to determine which event
		occurred before the other.
	<li> possible to determine which event occurred before the other,
		but only if the computer systems have recently (e.g., within
		the past minute) synchronized their clocks.
	<li> sometimes possible, but only when there is another event on a third
		computer system to which each of these events can be compared.
	<li> <strong>never</strong> possible to determine which event occurred
		before the other.
	<li> None of the above
	</ol>
</li><br/>
<li> All events occurring on different computer systems, which regularly
	exchanged messages, can
	<ol class="answer_list">
	<li> <strong>always</strong> be fully ordered.
	<li> <strong>always</strong> be partially ordered, and sometimes fully
		ordered (but only if no more than 5 events occur on a system
		between the exchange of messages).
	<li> <strong>always</strong> be partially ordered, but
		<strong>never</strong> fully ordered.
	<li> <strong>never</strong> be ordered.
	<li> None of the above
	</ol>
</li><br/>
<li> A polytree can be used to represent events occurring on different communicating
	computer systems, where
	<ol class="answer_list">
	<li> nodes correspond to events.
	<li> nodes correspond to computer systems.
	<li> directed edges correspond to messages (pointing from the sender to
		the receiver).
	<li> undirected edges correspond to the network connections between the
		different computer systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Lamport timestamps
	<ol class="answer_list">
	<li> enable the physical clocks of different computers to be
		synchronized sufficiently well so that the physical clock
		times can be used to fully order <strong>all</strong> events. 
	<li> provide a logical clock, which yields a time that can be used just
		like time from a physical clock, but that also enables all
		events to be fully ordered.
	<li> provide a logical clock, which yields a time that can be used to
		fully order <strong>all</strong> events, but which has no real
		correspondence to any of the physical clocks.
	<li> provide a logical clock, which yields a time that can be used to
		partially order events, but which has no real correspondence
		to any of the physical clocks.
	<li> None of the above
	</ol>
</li><br/>
<li> The algorithm for Lamport timestamps requires that each computer
	<ol class="answer_list">
	<li> maintain a clock-like counter which is incremented every time the
		computer's clock increments.
	<li> maintain a clock-like counter which is incremented before each
		event.
	<li> send its clock-like counter as part of every message sent to
		another computer.
	<li> periodically ask other computers for their clock-like counter
		values, setting its value to the lower of the value received
		and its current value.
	<li> None of the above
	</ol>
</li><br/>
<li> Lamport timestamps establishes a "happened-before" relationship on events,
	such that
	<ol class="answer_list">
	<li> if "a" happened-before "b", then the count value associated with
		event "a" is less than the count value associated
		with event "b".
	<li> if "a" happened-before "b", then the count value associated with
		event "a" is less than or equal to the count value associated
		with event "b".
	<li> if the count value associated with event "a" is less
		than the count value associated with event "b", then "a"
		happened-before "b".
	<li> if the count value associated with event "a" is less than or
		equal to the count value associated with event "b", then "a"
		happened-before "b".
	<li> None of the above
	</ol>
</li><br/>
<li> The vector clock algorithm is similar to the algorithnm for Lamport
	timestamps except that
	<ol class="answer_list">
	<li> each process (or computer) keeps a list of logical clocks
		(counters) corresponding to each process (computer),
		<strong>not</strong> just one clock for itself.
	<li> messages do <strong>not</strong> include any logical clock
		(counter) values when sent.
	<li> <strong>all</strong> logical clock (counter) values in a process'
		(computer's) list of counter values are increased to the
		maximum logical clock value in a received message.
	<li> <strong>all</strong> logical clock (counter) values in a process'
		(computer's) list of counter values are increased by one
		when a message is received.
	<li> None of the above
	</ol>
</li><br/>
<li> The vector clock algorithm establishes a relationship on events, such that
	<ol class="answer_list">
	<li> if "a" happened-before "b", then [HTML]<strong>all</strong>[/HTML]
		of the count values in the vector for event "a" are less than
		or equal to the corresponding count values in the vector
		associated with event "b".
	<li> if "a" happened-before "b", then most (but not necessarily all)
		of the count values in the vector for event "a" are less than
		the corresponding count values in the vector associated with
		event "b".
	<li> if [HTML]<strong>all</strong>[/HTML] of the count values in the
		vector associated with event "a" are less than or equal to the
		corresponding count values in the vector associated with event
		"b", then "a" happened-before "b".
	<li> if some of the count values in the vector associated with event
		"a" are less (and some are greater) than the corresponding
		count values in the vector associated with event "b", then
		"a" and "b" are concurrent.
	<li> None of the above
	</ol>
</li><br/>
<li> The vector clock algorithm, when compared to Lamport timestamps
	<ol class="answer_list">
	<li> provides exactly the same "happened-before" relationship.
	<li> provides a more powerful "happened-before" relationship that
		works in both directions (i.e., it is an "if and only if"
		relationship).
	<li> provides an additional "concurrent" relationship for when some of
		the corresponding clock vector entries for events "a" and "b"
		are less and some are greater than one another.
	<li> provides a complete ordering on all events.
	<li> None of the above
	</ol>
</li><br/>

<li> While there are multiple date formats commonly used throughout the world,
	<ol class="answer_list">
	<li> no country uses more than one date format.
	<li> there is no ambiguity between the different formats used within
		any single country.
	<li> there is no ambiguity between any of the formats used throughout
		the world.
	<li> the year is <strong>always</strong> the last part of date.
	<li> None of the above
	</ol>
</li><br/>
<li> The format(s) commonly used for indicating the time of day
	<ol class="answer_list">
	<li> <strong>all</strong> unambiguously indicate the time of day,
		so there is no confusion within the same format or between
		different formats.
	<li> are ambiguous in some cases, where one written time could be
		interpreted multiple ways (even within the same format).
	<li> can be confused with one another such that a time written in
		one format could be interpreted as a different part of the
		day in another format.
	<li> None of the above
	</ol>
</li><br/>
<li> The international standard (ISO 8601) for indicating the date (and time)
	<ol class="answer_list">
	<li> is based on the Gregorian calendar.
	<li> gives the most significant part of the date (and time) first
		(reading left to right).
	<li> gives the least significant part of the date (and time) first
		(reading left to right).
	<li> uses zero padding for everything except the year.
	<li> None of the above
	</ol>
</li><br/>
<li> While the international standard (ISO 8601) for indicating the date
	(and time) does <strong>not</strong> require their use
	<ol class="answer_list">
	<li> the three letter month abbreviation is recommended both for
		readability and to reduce possible confusion with other formats.
	<li> zero padding of numbers (e.g., months, days, hours, minutes) is
		recommended for consistency of date and time lengths.
	<li> "BCE" and "CE" can be used to clarify the era for the year.
	<li> hyphens (-) and colons (:) are recommended for both readability and
		to reduce possible confusion with other formats.
	<li> None of the above
	</ol>
</li><br/>
<li> Drawback(s) of the Gregorian calendar include:
	<ol class="answer_list">
	<li> dates between 1582 and about 1755 can be unrealiable.
	<li> many people do <strong>not</strong> know the leap year rules
		(e.g., why 2000 was a leap year but 2100 will not be).
	<li> much of the world still does <strong>not</strong> use it for civil,
		official, or administrative purposes.
	<li> it is only used in predominantly christian countries.
	<li> None of the above
	</ol>
</li><br/>
<li> Julian dates are used by astronomers because
	<ol class="answer_list">
	<li> the day starts at noon rather than at midnight.
	<li> the leap year rules are simpler (1 extra day every 7 years)
		than in the Gregorian calendar.
	<li> observations by early astronomers were done using this calendar,
		thus they didn't change when the Gregorian calendar was
		introduced.
	<li> simple subtraction indicates the time difference between two
		"dates".
	<li> None of the above
	</ol>
</li><br/>
<li> The epoch (first day) for Julian dates was
	<ol class="answer_list">
	<li> 1 Jan 4713 BCE
	<li> 25 Dec 1 CE
	<li> 1 Jan 1970 CE
	<li> 1 Jan 1980 CE
	<li> None of the above
	</ol>
</li><br/>

<li> In the context of a computer system, the following are considered
	resources:
	<ol class="answer_list">
	<li> I/O devices
	<li> CPU
	<li> Memory
	<li> Software
	<li> None of the above
	</ol>
</li><br/>
<li> A preemptable resource is one that when temporarily taken away from a
	process
	<ol class="answer_list">
	<li> will <strong>not</strong> cause a problem for the preempted
		process.
	<li> may sometimes cause a problem for the preempted process,
		including possible process failure.
	<li> <strong>always</strong> results in failure of the preempted
		process.
	<li> None of the above
	</ol>
</li><br/>
<li> A non-preemptable resource is one that when taken away (even if only
	temporarily) from a process
	<ol class="answer_list">
	<li> will <strong>not</strong> cause a problem for the preempted
		process.
	<li> may sometimes cause a problem for the preempted process,
		including possible process failure.
	<li> <strong>always</strong> results in failure of the preempted
		process.
	<li> None of the above
	</ol>
</li><br/>
<li> The typical interaction pattern between a process and a resource is
	<ol class="answer_list">
	<li> detected, requested, used, released.
	<li> requested, used, released.
	<li> requested, granted, used, released.
	<li> granted, used, released.
	<li> None of the above
	</ol>
</li><br/>
<li> Software (e.g., blocks of code) can be viewed as a non-premeptable resource
	<ol class="answer_list">
	<li> because they in turn use resources (e.g., CPU, memory).
	<li> when mutex semaphores are used to controlled access.
	<li> since it can access the system bus.
	<li> if it is part of a user process that the OS schedules to run
		in the CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following conditions <strong>must</strong> hold in order for
	deadlock to exist?
	<ol class="answer_list">
	<li> Only one instance of each type of resource exists.
	<li> Mutually exclusive use of resources.
	<li> Resources are controlled and allocated only by the OS.
	<li> Processes can hold resources while requesting additional resources.
	<li> Process scheduling <strong>must</strong> be non-preemptive.
	<li> Granted resources cannot be preempted.
	<li> All resources within the system have been allocated.
	<li> Circular wait condition.
	<li> All processes within the system are in the blocked state.
	<li> None of the above
	</ol>
</li><br/>
<li> A set of processes is deadlocked when
	<ol class="answer_list">
	<li> <strong>all</strong> of the processes in the set are in the
		blocked state.
	<li> every resource in the system has been granted to a process in
		the set.
	<li> each process in the set is waiting for an event that only another
		process in the set can cause.
	<li> no process in the set has been granted a preemptable resource. 
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a strategy for dealing
	with deadlocks?
	<ol class="answer_list">
	<li> Prevention
	<li> Dynamic avoidance
	<li> Detection and recovery
	<li> Ignore the problem
	<li> None of the above
	</ol>
</li><br/>
<li> Deadlock modeling represents resources (squares) and processes (circles)
	as a graph with
	<ol class="answer_list">
	<li> arrows pointing from a process to a resource represent an
		(ungranted) resource request.
	<li> arrows pointing from a resource to a process represent a
		granted resource.
	<li> arrows pointing in both directions between a process and a
		resource represent a granted dedicated non-preemptable
		resource.
	<li> undirected lines (no arrows) between a process and a resource
		represent a granted preemptable resource.
	<li> None of the above
	</ol>
</li><br/>
<li> In a deadlock model graph, a deadlock exists when
	<ol class="answer_list">
	<li> at least one process has more than two (2) arrows pointing to it.
	<li> at least one resource has more than two (2) arrows pointing to it.
	<li> there is a directed cycle in the graph.
	<li> the graph forms a tree.
	<li> None of the above
	</ol>
</li><br/>
<li> The following resource graph indicates that:<br/>
	<img src="/eckart/classes/cpsc3125/questions/ResourceGraph_1.png" width="406" height="384" alt="Resource Graph" />
	<ol class="answer_list">
	<li> No deadlock exists.
	<li> No deadlock exists, but there is a livelock on "T".
	<li> A deadlock exists involving the processes and resources
		denoted by: B, T, C, S.
	<li> A deadlock exists involving the processes and resources
		denoted by: D, R, B, T.
	<li> None of the above
	</ol>
</li><br/>
<li> The following resource graph indicates that:<br/>
	<img src="/eckart/classes/cpsc3125/questions/ResourceGraph_2.png" width="383" height="365" alt="Resource Graph" />
	<ol class="answer_list">
	<li> No deadlock exists.
	<li> No deadlock exists, but there is a livelock on "T".
	<li> A deadlock exists involving the processes and resources
		denoted by: A, T, D, U.
	<li> A deadlock exists involving the processes and resources
		denoted by: A, R, B, S, C, T.
	<li> None of the above
	</ol>
</li><br/>

<li> Which strategy for dealing with deadlock is used by most systems (e.g.,
	Linux, Mac OS X, Windows)?
	<ol class="answer_list">
	<li> Prevention
	<li> Dynamic avoidance
	<li> Detection and recovery
	<li> Ignore the problem
	<li> None of the above
	</ol>
</li><br/>
<li> Determining whether or not a deadlock <em>actually</em> exists in the
	system requires
	<ol class="answer_list">
	<li> building the deadlock model graph and checking for cycles.
	<li> checking whether or not <strong>all</strong> processes are in
		the blocked state.
	<li> periodically checking how long blocked processes have been blocked.
	<li> periodically checking whether there are any processes in the ready
		state.
	<li> None of the above
	</ol>
</li><br/>
<li> Deadlock detection is
	<ol class="answer_list">
	<li> fast and easy to run, so it should be done every time a resource is
		allocated.
	<li> fast and easy to run, so it should be done every time a new process
		is created.
	<li> rather slow and costly to run, so it should be run only when a
		process has been blocked for a long time (assuming there's no
		urgent need to detect a possible deadlock).
	<li> rather slow and costly to run, so it should be run only when
		there is no user process in the ready state. That is, it
		should be run as the idle process.
	<li> None of the above
	</ol>
</li><br/>
<li> If a deadlock has been detected, recovery can be accomplished by
	<ol class="answer_list">
	<li> waiting until a process in the deadlock releases
		<strong>all</strong> its resources.
	<li> rolling back a process in the deadlock to a checkpoint before it
		requested a resource participating in the deadlock.
	<li> terminating a process in the deadlock (thus releasing
		<strong>all</strong> of its granted resources).
	<li> change <strong>all</strong> of the processes in the deadlock
		to the ready state.
	<li> None of the above
	</ol>
</li><br/>
<li> When recovering from a deadlock by terminating a process that was
	participating in a deadlock
	<ol class="answer_list">
	<li> give preference to processes with the least amount of
		accumulated CPU time is often <strong>best</strong>.
	<li> change <strong>all</strong> remaining blocked processes to the
		ready state after terminating the offending process.
	<li> recheck to determine if any additional deadlocks still exist.
	<li> do <strong>not</strong> allow any other processes in the deadlock
		to acquire the resources released by the terminating process.
	<li> None of the above
	</ol>
</li><br/>

<li> Dynamic deadlock avoidance assumes that
	<ol class="answer_list">
	<li> it is possible to preempt an otherwise non-preemptable resource
		if deadlock would otherwise occur.
	<li> the resource needs of a process can be changed by the OS.
	<li> processes can be terminated if their resource request would 
		cause a deadlock.
	<li> there exists a resource allocation that would allow
		<strong>all</strong> current processes to complete.
	<li> None of the above
	</ol>
</li><br/>
<li> An <em>UNsafe state</em> with respect to dynamic avoidance of deadlock is
	one in which
	<ol class="answer_list">
	<li> there exists one or more resource allocation scenarios that
		allow some (but not all) processes to complete.
	<li> there exists at least one resource allocation scenario that
		would prevent <strong>all</strong> processes from completing.
	<li> no resource allocation scenario guarantees <strong>all</strong>
		processes will complete.
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>safe state</em> with respect to dynamic avoidance of deadlock is
	one in which
	<ol class="answer_list">
	<li> <strong>all</strong> resource allocation scenarios allow
		<strong>all</strong> processes to complete.
	<li> there exists one or more resource allocation scenarios that
		allow some (but not all) processes to complete.
	<li> there exists at least one resource allocation scenario that
		would allow <strong>all</strong> processes to complete.
	<li> None of the above
	</ol>
</li><br/>
<li> If a set of processes and resources is in an <em>UNsafe state</em> with
	respect to deadlock, then deadlock
	<ol class="answer_list">
	<li> is guaranteed to occur eventually.
	<li> will occur if <strong>all</strong> processes use their maximum
		required resources.
	<li> may still be avoided if some processes use less than their
		maximum required resources.
	<li> can <strong>always</strong> be avoided by withholding additional
		resources from one or more processes.
	<li> None of the above
	</ol>
</li><br/>
<li> The Banker's algorithm for dynamic deadlock avoidance requires that
	<ol class="answer_list">
	<li> there <strong>always</strong> be sufficient
		resources available to meet <strong>all</strong> processes'
		maximum requests simultaneously.
	<li> there <strong>always</strong> be sufficient
		resources available to meet at least one process' maximum
		request.
	<li> the minimum resource needs of <strong>all</strong>
		processes be known beforehand.
	<li> the maximum resource needs of <strong>all</strong>
		processes be known beforehand.
	<li> None of the above
	</ol>
</li><br/>
<li> The Banker's algorithm for dynamic deadlock avoidance is most applicable
	to batch processing because
	<ol class="answer_list">
	<li> preemptive scheduling is <strong>not</strong> generally used.
	<li> virtual memory is <strong>never</strong> used.
	<li> the running time for each process is known from previous
		executions.
	<li> the processes and their maximum resource needs are typically
		known beforehand.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to the Banker's algorithm, assuming the bank started with
	100 units of resource, what is true about the system given the
	following situation?
	<blockquote>
	<table style="text-align: center">
	<tr><th></th><th colspan="2">Allocation</th></tr>
	<tr><th>Process</th><th>Maximum<br/>Possible</th><th>Current</th></tr>
	<tr><td>A</td><td>10</td><td>5</td></tr>
	<tr><td>B</td><td>20</td><td>10</td></tr>
	<tr><td>C</td><td>30</td><td>15</td></tr>
	<tr><td>D</td><td>40</td><td>20</td></tr>
	</table>
	</blockquote>
	<ol class="answer_list">
	<li> The system cannot become deadlocked.
	<li> The system is in a safe state.
	<li> The system is in an UNsafe state.
	<li> The system is deadlocked if <strong>all</strong>
		processes need even one additional resource.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to the Banker's algorithm, assuming the bank started with
	60 units of resource, what is true about the system given the
	following situation?
	<blockquote>
	<table style="text-align: center">
	<tr><th></th><th colspan="2">Allocation</th></tr>
	<tr><th>Process</th><th>Maximum<br/>Possible</th><th>Current</th></tr>
	<tr><td>A</td><td>10</td><td>5</td></tr>
	<tr><td>B</td><td>20</td><td>10</td></tr>
	<tr><td>C</td><td>30</td><td>15</td></tr>
	<tr><td>D</td><td>40</td><td>20</td></tr>
	</table>
	</blockquote>
	<ol class="answer_list">
	<li> The system cannot become deadlocked.
	<li> The system is in a safe state.
	<li> The system is in an UNsafe state.
	<li> The system is deadlocked if <strong>all</strong>
		processes need even one additional resource.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to the Banker's algorithm, assuming the bank started with
	50 units of resource, what is true about the system given the
	following situation?
	<blockquote>
	<table style="text-align: center">
	<tr><th></th><th colspan="2">Allocation</th></tr>
	<tr><th>Process</th><th>Maximum<br/>Possible</th><th>Current</th></tr>
	<tr><td>A</td><td>10</td><td>5</td></tr>
	<tr><td>B</td><td>20</td><td>10</td></tr>
	<tr><td>C</td><td>30</td><td>15</td></tr>
	<tr><td>D</td><td>40</td><td>20</td></tr>
	</table>
	</blockquote>
	<ol class="answer_list">
	<li> The system cannot become deadlocked.
	<li> The system is in a safe state.
	<li> The system is in an UNsafe state.
	<li> The system is deadlocked if <strong>all</strong>
		processes need even one additional resource.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following prevents deadlock by negating the exclusive use
	of resources?
	<ol class="answer_list">
	<li> The use of two-phase locking.
	<li> Use spooling and a spool daemon, with the spool daemon using only
		a single resource.
	<li> Allow a process to hold only one resource at a time.
	<li> Requiring that new resource requests by a process occur in
		increasing resource priority order.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following prevents deadlock by negating the holding of
	resource while making additional requests?
	<ol class="answer_list">
	<li> Requiring that new resource requests by a process occur in
		increasing resource priority order.
	<li> Allow a process to hold only one resource at a time.
	<li> Use spooling and a spool daemon, with the spool daemon using only
		a single resource.
	<li> The use of two-phase locking.
	<li> None of the above
	</ol>
</li><br/>
<li> Preventing deadlock by negating the non-preemptability of resources
	works for resources
	<ol class="answer_list">
	<li> that can be spooled, thus ensuring that the resource
		<strong>never</strong> needs to be preempted.
	<li> which are only assigned using two-phase locking.
	<li> like the CPU, but <strong>not</strong> for <strong>all</strong>
		types of resources.
	<li> assigned in strictly decreasing order resource priority order.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following prevents deadlock by negating the circular
	wait condition?
	<ol class="answer_list">
	<li> Use spooling and a spool daemon, with the spool daemon using only
		a single resource.
	<li> Allow a process to hold only one resource at a time.
	<li> Requiring that new resource requests by a process occur in
		increasing resource priority order.
	<li> The use of two-phase locking.
	<li> None of the above
	</ol>
</li><br/>

<li> Starvation occurs when a process is <strong>not</strong> deadlocked,
	<ol class="answer_list">
	<li> but is consistently unable to obtain a needed resource, often
		because the resource keeps being assigned to other processes.
	<li> and is assigned the needed resource, but the resource is
		continually preempted.
	<li> but continually acquires and returns the same resource over and
		over.
	<li> but waits for a signal from another process which
		<strong>never</strong> comes.
	<li> None of the above
	</ol>
</li><br/>
<li> Livelock occurs when a set of two or more processes
	<ol class="answer_list">
	<li> experience a combination of both deadlock and starvation.
	<li> are unable to get <strong>all</strong> their needed resources,
		and continually give back those resources that were granted
		and retry getting <strong>all</strong> of them
		<strong>without</strong> success.
	<li> are waiting for signals from each other before continuing
		their remaining computations.
	<li> is only missing one of the 4 conditions necessary for deadlock.
	<li> None of the above
	</ol>
</li><br/>
<li> A communication deadlock can occur when
	<ol class="answer_list">
	<li> messages are lost in transit.
	<li> a process is starved for a communication channel. 
	<li> <strong>all</strong> the processes that are trying to
		communicate are in livelock.
	<li> two-phase locking is used to allocate communicate channels.
	<li> None of the above
	</ol>
</li><br/>
<li> Communication deadlocks can often (though <strong>not always</strong>)
	be avoid by using
	<ol class="answer_list">
	<li> multiple communication channels.
	<li> a single message packet that is passed between the processes.
	<li> two-phase locking.
	<li> message acknowledgements in combination with timeouts (when
		acknowledgements are <strong>not</strong> promptly received).
	<li> None of the above
	</ol>
</li><br/>
<li> Two-phase locking is characterized by
	<ol class="answer_list">
	<li> assigning resources priorities and requiring processes to request
		resources in increasing priority order.
	<li> requiring that <strong>all</strong> resources be preemptable.
	<li> repeatedly trying to get <strong>all</strong> needed resources,
		returning those granted if some requests went unsatisfied,
		until <strong>all</strong> the resources are finally acquired.
	<li> having processes make two requests (in quick succession)
		to acquire a resource.
	<li> None of the above
	</ol>
</li><br/>

<li> The OS mechanisms used to provide security are called
	<ol class="answer_list">
	<li> threats.
	<li> shields.
	<li> security perimeter.
	<li> protection mechanisms.
	<li> None of the above
	</ol>
</li><br/>
<li> Any set of actions taken to gain unauthorized access to a system is
	called a (an)
	<ol class="answer_list">
	<li> vulnerability.
	<li> exploit.
	<li> 0-day attack.
	<li> script.
	<li> None of the above
	</ol>
</li><br/>
<li> Someone who enjoys the creativity and challenge of solving problems is
	called a
	<ol class="answer_list">
	<li> maker.
	<li> hacker.
	<li> cracker.
	<li> white hat.
	<li> None of the above
	</ol>
</li><br/>
<li> Someone who attempts to gain unauthorized access to a system is called a
	<ol class="answer_list">
	<li> black hat.
	<li> cracker.
	<li> script kiddie.
	<li> hacker.
	<li> None of the above
	</ol>
</li><br/>
<li> A software bug that can be used to subvert the security of a system is
	called a
	<ol class="answer_list">
	<li> security hole.
	<li> vulnerability.
	<li> exploit.
	<li> threat.
	<li> None of the above
	</ol>
</li><br/>
<li> The difference(s) between active and passive intruders are that
	<ol class="answer_list">
	<li> passive intruders use an existing users credentials for access
		while active intruders use vulnerabilities to break in.
	<li> passive intruders rely on social engineering whereas active
		intruders use technical exploits.
	<li> active intruders use acquired data for personal gain while
		passive intruders use the data to expose the illegal
		activities of others.
	<li> active intruders write/change data whereas passive intruders
		only read data.
	<li> None of the above
	</ol>
</li><br/>
<li> A passive intruder <strong>must</strong> have
	<ol class="answer_list">
	<li> the credentials of an authorized user to access data.
	<li> electronic access to the computer system.
	<li> physical access to the computer system.
	<li> an exploit for a known vulnerability.
	<li> None of the above
	</ol>
</li><br/>
<li> An active intruder <strong>must</strong> have
	<ol class="answer_list">
	<li> the credentials of an authorized user to access data.
	<li> electronic access to the computer system.
	<li> physical access to the computer system.
	<li> an exploit for a known vulnerability.
	<li> None of the above
	</ol>
</li><br/>
<li> Determining who can see what data (e.g., privacy) is known as information
	<ol class="answer_list">
	<li> availability.
	<li> accountability.
	<li> confidentiality.
	<li> integrity.
	<li> None of the above
	</ol>
</li><br/>
<li> Preventing the change or removal of data by unauthorized people is
	information
	<ol class="answer_list">
	<li> availability.
	<li> accountability.
	<li> confidentiality.
	<li> integrity.
	<li> None of the above
	</ol>
</li><br/>
<li> Ensuring that unauthorized people are <strong>not</strong> able to make
	a system unusable or reduce its performance is called information
	<ol class="answer_list">
	<li> availability.
	<li> accountability.
	<li> confidentiality.
	<li> integrity.
	<li> None of the above
	</ol>
</li><br/>
<li> The loss of data availability requires
	<ol class="answer_list">
	<li> the loss of data accountability.
	<li> the loss of data integrity.
	<li> a loss of data confidentiality.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of attack would be the hardest to detect and defend against?
	<ol class="answer_list">
	<li> A passive attack by an outsider.
	<li> A passive attack by an insider.
	<li> An active attack by an outsider.
	<li> An active attack by an insider.
	<li> None of the above
	</ol>
</li><br/>
<li> What term is used to describe unskilled persons that use software
	developed by others to gain unauthorized access to computer systems
	and data?
	<ol class="answer_list">
	<li> Script-kiddie
	<li> Black Hat
	<li> Cracker
	<li> Hacker
	<li> None of the above
	</ol>
</li><br/>
<li> A group of networked and compromised computers that are used to conduct
	illegal activities on a massive scale is called
	<ol class="answer_list">
	<li> cyberwarfare.
	<li> a botnet.
	<li> port scanning.
	<li> a script-kiddie.
	<li> None of the above
	</ol>
</li><br/>
<li> Port scanning is a common method used to
	<ol class="answer_list">
	<li> discover computer systems and their potential vulnerabilities.
	<li> form a group of networked computers into a botnet.
	<li> exploit the known vulnerabilities of a system.
	<li> conduct a passive insider attack on a system.
	<li> None of the above
	</ol>
</li><br/>
<li> A computer system that meets a formally stated set of security requirements
	is called a
	<ol class="answer_list">
	<li> secure system.
	<li> trusted computer base.
	<li> trusted system.
	<li> validated system.
	<li> None of the above
	</ol>
</li><br/>
<li> The set of hardware and software that provides the basis for a system
	that meets a formally stated set of security requirements is called a
	<ol class="answer_list">
	<li> secure system.
	<li> trusted computer base.
	<li> trusted system.
	<li> validated system.
	<li> None of the above
	</ol>
</li><br/>
<li> A trusted computing base (TCB) is comprised of
	<ol class="answer_list">
	<li> most (but not necessarily all) of the computer hardware.
	<li> <strong>all</strong> the hardware and software of a computer
		system.
	<li> the human processes and procedures used to operate the computer
		system.
	<li> the core part of the OS (e.g., microkernel).
	<li> None of the above
	</ol>
</li><br/>

<li> A domain is a set of object-right pairs, in which the
	<ol class="answer_list">
	<li> object represents a resource (e.g., file, cpu, disk, software).
	<li> object represents users and groups of users.
	<li> right represents the permission to perform a specific operation.
	<li> right represents one of ADD, DELETE, MODIFY.
	<li> None of the above
	</ol>
</li><br/>
<li> When users (and processes) only have access to the resources (and
	operations) necessary to complete a task, this is known as the
	<ol class="answer_list">
	<li> lowest access protocol.
	<li> least common denominator.
	<li> principle of necessary privilege.
	<li> principle of least authority.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to file objects in a POSIX system, the rights are the
	<ol class="answer_list">
	<li> add, remove, modify operations allowed for the file owner.
	<li> add, remove, modify operations allowed for the file owner
		and everyone else.
	<li> read, write, and execute operations allowed for the file owner.
	<li> read, write, and execute operations allowed for the file owner,
		the file's group, and everyone else.
	<li> None of the above
	</ol>
</li><br/>
<li> The SETUID bit associated with files on a POSIX system are used to
	<ol class="answer_list">
	<li> execute the program file with the identity of the file's owner,
		regardless of who executes the program file.
	<li> ensure that the file is owned by the file's creator.
	<li> enable the owner of a file to execute the program it holds.
	<li> enable anyone to execute the program file.
	<li> None of the above
	</ol>
</li><br/>
<li> The SETUID bit associated with files on a POSIX system can create a
	secruity risk
	<ol class="answer_list">
	<li> because it enables someone other than the owner of a file to
		delete or change the file.
	<li> if the file owner has the same privileges as the superuser
		(e.g., root or administrator accounts).
	<li> since it allows users to change their identity whenever they
		wish.
	<li> when the file contains non-executable, but sensitive data.
	<li> None of the above
	</ol>
</li><br/>
<li> If the protection matrix is drawn like the below, what information is
	stored within the matrix locations? 
<table border="1">
<tr>
	<th colspan="2" rowspan="2"></th>
	<th colspan="8">Objects</th>
</tr>
<tr>
	<th>File 1</th>
	<th>File 2</th>
	<th>...</th>
	<th>File N</th>
	<th>Domain 1</th>
	<th>Domain 2</th>
	<th>...</th>
	<th>Domain D</th>
</tr>
<tr>
	<th rowspan="4" style="vertical-align: middle">Subjects</th>
	<th>Domain 1</th>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
</tr>
<tr>
	<th>Domain 2</th>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
</tr>
<tr>
	<th>...</th>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
</tr>
<tr>
	<th>Domain D</th>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
	<td></td>
</tr>
</table>
	<ol class="answer_list">
	<li> Usernames that are the owners of the object within the subject
		domain.
	<li> 1 if the object is owned by the subject, and 0 otherwise.
	<li> Operations the subject is allowed to perform on the object.
	<li> Group names that contain both the (user) subject and which are
		group owners of the object.
	<li> None of the above
	</ol>
</li><br/>
<li> The objects within the protection matrix are most often associated with
	<ol class="answer_list">
	<li> users.
	<li> groups of users.
	<li> processes.
	<li> files.
	<li> None of the above
	</ol>
</li><br/>
<li> The subjects within the protection matrix are most often associated with
	<ol class="answer_list">
	<li> users.
	<li> groups of users.
	<li> processes.
	<li> files.
	<li> None of the above
	</ol>
</li><br/>
<li> Domains can appear both as objects and subjects within the protection
	matrix in order to
	<ol class="answer_list">
	<li> allow domains to add, remove, or modify themselves.
	<li> allow domains to add, remove, or modify other domains.
	<li> support switching from one domain to another domain.
	<li> enable domains to create new domains. 
	<li> None of the above
	</ol>
</li><br/>
<li> Because the protection matrix can be quite large, most systems don't
	store it as a full matrix. When the matrix is stored as sparse
	rows, it is called
	<ol class="answer_list">
	<li> an Object Permissions List.
	<li> an Access Control List.
	<li> a Capability List.
	<li> a Subject Permissions List.
	<li> None of the above
	</ol>
</li><br/>
<li> Because the protection matrix can be quite large, most systems don't
	store it as a full matrix. When the matrix is stored as sparse
	columns, it is called
	<ol class="answer_list">
	<li> an Object Permissions List.
	<li> an Access Control List.
	<li> a Capability List.
	<li> a Subject Permissions List.
	<li> None of the above
	</ol>
</li><br/>
<li> POSIX systems typically store the protection matrix (using 9 bits to
	store the permissions for 3 domain categories) as a version of
	<ol class="answer_list">
	<li> an Object Permissions List.
	<li> an Access Control List.
	<li> a Capability List.
	<li> a Subject Permissions List.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following corresponds most closely to the storage of a
	protection matrix using a Capability List?
	<ol class="answer_list">
	<li>
<table border="1">
<tr>
<th>Write</th>
<td>File 2: Domain 3, Domain 4</td>
<td>File 5: Domain 7</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>File 2</th>
<td>Domain 2: Write</td>
<td>Domain D: Read/Write</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>Domain 2</th>
<td>File 2: Write</td>
<td>File N: Execute</td>
<td>Domain D: Enter</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>File 3</th>
<td>Read: Domain 4, Domain 7</td>
<td>Write: Domain 5</td>
</tr>
</table>
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following corresponds most closely to the storage of a
	protection matrix using an Access Control List?
	<ol class="answer_list">
	<li>
<table border="1">
<tr>
<th>Write</th>
<td>File 2: Domain 3, Domain 4</td>
<td>File 5: Domain 7</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>File 2</th>
<td>Domain 2: Write</td>
<td>Domain D: Read/Write</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>Domain 2</th>
<td>File 2: Write</td>
<td>File N: Execute</td>
<td>Domain D: Enter</td>
</tr>
</table>
	<li>
<table border="1">
<tr>
<th>File 3</th>
<td>Read: Domain 4, Domain 7</td>
<td>Write: Domain 5</td>
</tr>
</table>
	<li> None of the above
	</ol>
</li><br/>
<li> Additional advantage(s) that Capability Lists have over Access Control
	Lists is that they are
	<ol class="answer_list">
	<li> themselves objects and can thus be referenced by other
		Capability Lists.
	<li> able to more easily add, delete, or change the rights to an
		object by a specific subject.
	<li> more efficient in allowing a process to exercise a specific right.
	<li> able to revoke <strong>all</strong> rights to an object (by
		<strong>all</strong> subjects) more quickly and easily.
	<li> None of the above
	</ol>
</li><br/>
<li> Since Capability Lists are associated with (and "owned" by) subjects
	(i.e., users), unlike Access Control Lists, they <strong>must</strong>
	be protected from tampering otherwise
	<ol class="answer_list">
	<li> the subject of that Capability List could grant themselves
		additional capabilities.
	<li> anyone on the system could remove capabilities from other users.
	<li> anyone on the system could remove subjects, even those they
		didn't own.
	<li> Capability Lists wouldn't be able to reference other Capability
		Lists.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following ways in which Capability Lists are protected from
	tampering?
	<ol class="answer_list">
	<li> Each user's login shell prevents that user from accessing their
		own Capability List, even though they own it.
	<li> Special hardware tag bits in memory to indicate which data are
		part of a capability list.
	<li> The Capability Lists are kept in the OS with no direct access by
		subjects.
	<li> A one-way hash function is used to prevent alteration, enabling
		the Capability lists to be stored in user space.
	<li> None of the above
	</ol>
</li><br/>
<li> If groups of subjects/users are <strong>not</strong> supported,
	then Access Control Lists
	<ol class="answer_list">
	<li> can become very large if rights are granted to every user.
	<li> <strong>must</strong> collect objects together into groups instead.
	<li> are associated with each subject (to make access faster) rather
		than with the corresponding object. 
	<li> become extremely slow to use, so Capability Lists should be used
		instead.
	<li> None of the above
	</ol>
</li><br/>
<li> Access Control Lists enable rights to an object by a specific subject to
	be easily
	<ol class="answer_list">
	<li> added.
	<li> modified.
	<li> deleted.
	<li> None of the above
	</ol>
</li><br/>

<li> The protection matrix indicates what subjects <em>can</em> do,
	<strong>not</strong> what they are authorized to do. Authorization is
	<ol class="answer_list">
	<li> determined in combination with the password file.
	<li> enforced by the OS kernel.
	<li> enforced by the file system.
	<li> a management policy, <strong>not</strong> part of the OS.
	<li> None of the above
	</ol>
</li><br/>
<li> Protection commands are used by processes to
	<ol class="answer_list">
	<li> enforce the rights recorded in the protection matrix.
	<li> prevent changes to the protection matrix.
	<li> change the protection matrix.
	<li> create the Access Control List or Capability List that
		corresponds to the protection matrix.
	<li> None of the above
	</ol>
</li><br/>
<li> The protection commands for "Create Subject" and "Delete Subject"
	<ol class="answer_list">
	<li> add/remove a row from the protection matrix.
	<li> add/remove a column from the protection matrix.
	<li> modify an entry in the protection matrix.
	<li> None of the above
	</ol>
</li><br/>
<li> The protection commands for "Create Object" and "Delete Object"
	<ol class="answer_list">
	<li> add/remove a row from the protection matrix.
	<li> add/remove a column from the protection matrix.
	<li> modify an entry in the protection matrix.
	<li> None of the above
	</ol>
</li><br/>
<li> The protection commands for "Insert Right" and "Remove Right"
	<ol class="answer_list">
	<li> add/remove a row from the protection matrix.
	<li> add/remove a column from the protection matrix.
	<li> modify an entry in the protection matrix.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of access control allows the owner of an object to determine
	who should have access to it?
	<ol class="answer_list">
	<li> Selective access control
	<li> Discretionary access control
	<li> Proscribed access control
	<li> Mandatory access control
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of access control uses organizational rules to determine which
	subjects have rights to various objects?
	<ol class="answer_list">
	<li> Selective access control
	<li> Discretionary access control
	<li> Proscribed access control
	<li> Mandatory access control
	<li> None of the above
	</ol>
</li><br/>
<li> Within an organizational structure in which <strong>all</strong>
	objects and subjects are assigned "security levels", the
	Biba Model of mandatory access control
	<ol class="answer_list">
	<li> restricts subjects to read only files which have an equal or
		higher security level than they do.
	<li> restricts subjects to read only files which have an equal or
		lower security level than they do.
	<li> restricts subjects to write only to files which have an equal or
		higher security level than they do.
	<li> restricts subjects to write only to files which have an equal or
		lower security level than they do.
	<li> None of the above
	</ol>
</li><br/>
<li> Within an organizational structure in which <strong>all</strong>
	objects and subjects are assigned "security levels", the
	Bell-LaPadula Model of mandatory access control
	<ol class="answer_list">
	<li> restricts subjects to read only files which have an equal or
		higher security level than they do.
	<li> restricts subjects to read only files which have an equal or
		lower security level than they do.
	<li> restricts subjects to write only to files which have an equal or
		higher security level than they do.
	<li> restricts subjects to write only to files which have an equal or
		lower security level than they do.
	<li> None of the above
	</ol>
</li><br/>
<li> Which security model is designed to allow managers to actively direct
	their subordinates?
	<ol class="answer_list">
	<li> Bell-LaPadula Model
	<li> Biba Model
	<li> None of the above
	</ol>
</li><br/>
<li> Which security model is designed to keep (military) secrets?
	<ol class="answer_list">
	<li> Bell-LaPadula Model
	<li> Biba Model
	<li> None of the above
	</ol>
</li><br/>
<li> The question of how to keep information that a server process gets from
	a client process from being passed on to a third party is known as
	<ol class="answer_list">
	<li> steganography.
	<li> the encryption conundrum.
	<li> the confinement problem.
	<li> a security sieve.
	<li> None of the above
	</ol>
</li><br/>
<li> Covert channels are
	<ol class="answer_list">
	<li> difficult to stop simply because they use subtle and "underhanded"
		ways to pass along information.
	<li> frequently used to allow remote systems to communicate.
	<li> noisy but enable information to be reliable sent.
	<li> designed to ensure the integrity of transmitted data.
	<li> None of the above
	</ol>
</li><br/>
<li> Hiding information within other unrelated information is called
	<ol class="answer_list">
	<li> semiology.
	<li> steganography.
	<li> cryptography.
	<li> cartography.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are design principles for security?
	<ol class="answer_list">
	<li> System designs should be private.
	<li> The default should be <strong>no</strong> access.
	<li> Check for the current rights to a resource upon each use,
		<strong>not</strong> just those it had when the resource
		was acquired.
	<li> Only give new processes the privileges of their parent.
	<li> Protection mechanisms should be built into the OS.
	<li> Security schemes <strong>must</strong> be psychologically
		acceptable.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are design principles for security?
	<ol class="answer_list">
	<li> System designs should be public.
	<li> The default read/write access to files should be given only for
		those owned by the process owner.
	<li> Check for the current rights to a resource upon each use,
		<strong>not</strong> just those it had when the resource
		was acquired.
	<li> Give each process the least privilege possible.
	<li> Protection mechanisms belong in a single process, separate from
		the OS, to ensure it isn't compromised if the OS is.
	<li> Security schemes <strong>must</strong> be psychologically
		acceptable.
	<li> None of the above
	</ol>
</li><br/>

<li> With respect to cryptography, the idea that encryption algorithms should
	be public, so that the key is the sole means for ensuring secrecy,
	is called
	<ol class="answer_list">
	<li> security by obscurity.
	<li> saltiness.
	<li> Kerckhoffs' principle.
	<li> algorithmic strength.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to cryptography, the computational effort needed to convert
	encrypted text back into its original form <strong>without</strong>
	knowing the encryption key(s) is known as
	<ol class="answer_list">
	<li> security by obscurity.
	<li> the an algorithm's saltiness.
	<li> Kerckhoffs' principle.
	<li> the encryption algorithm's strength.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to cryptography, the encrypted version of the text (or data)
	is called
	<ol class="answer_list">
	<li> the encryption key.
	<li> plaintext.
	<li> ciphertext.
	<li> salt.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to cryptography, the original text (or data) that was intended
	for direct consumption by a human (or computer program) is called
	<ol class="answer_list">
	<li> the encryption key.
	<li> plaintext.
	<li> ciphertext.
	<li> salt.
	<li> None of the above
	</ol>
</li><br/>
<li> When the key to encrypt text/data is <em>different</em> from the key
	to decrypt the encrypted text/data, this is called
	<ol class="answer_list">
	<li> symmetric key cryptography.
	<li> asymmetric key cryptography.
	<li> secret key cryptography.
	<li> one-way hash function cryptography. 
	<li> None of the above
	</ol>
</li><br/>
<li> When the key to encrypt text/data is the <em>same</em> as the key
	to decrypt the encrypted text/data, this is called
	<ol class="answer_list">
	<li> symmetric key cryptography.
	<li> asymmetric key cryptography.
	<li> secret key cryptography.
	<li> one-way hash function cryptography. 
	<li> None of the above
	</ol>
</li><br/>
<li> Advantage(s) of secret key over public key cryptography are
	<ol class="answer_list">
	<li> the algorithms are generally quite fast, handling large amounts
		of text/data efficiently.
	<li> it can be used as the basis for digital signatures.
	<li> it requires the secure exchange of keys.
	<li> that it's harder to break since the algorithms are kept secret.
	<li> None of the above
	</ol>
</li><br/>
<li> Disadvantage(s) of secret key over public key cryptography are
	<ol class="answer_list">
	<li> the algorithms are generally quite slow.
	<li> it can be used as the basis for digital signatures.
	<li> it requires the secure exchange of keys.
	<li> that it's easier to break since the algorithms are kept secret.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are secret key cryptography algorithms currently
	recommended for use by the National Institute for Standards
	and Technology (NIST)?
	<ol class="answer_list">
	<li> Twofish
	<li> RSA
	<li> Triple DES
	<li> Advanced Encryption Standard (AES)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are secret key cryptography algorithms currently
	recommended for use by the National Institute for Standards
	and Technology (NIST) for protecting US government TOP SECRECT
	documents?
	<ol class="answer_list">
	<li> Twofish
	<li> RSA
	<li> Triple DES
	<li> Advanced Encryption Standard (AES)
	<li> None of the above
	</ol>
</li><br/>
<li> For public key cryptography, the 2 keys required are
	<ol class="answer_list">
	<li> are randomly chosen by the person send and the person receiving
		the message.
	<li> the same key.
	<li> bit-wise palindromes of one another.
	<li> related to one another by a mathematical property.
	<li> None of the above
	</ol>
</li><br/>
<li> For Alice and Bob to have a two-way communication with one another using
	public key cryptography, how many separate keys <strong>must</strong>
	be created and used?
	<ol class="answer_list">
	<li> 2 keys created, and both keys are used.
	<li> 2 keys created, but only one key is used.
	<li> 4 keys created, but only two keys are used.
	<li> 4 keys created, and <strong>all</strong> keys are used.
	<li> None of the above
	</ol>
</li><br/>
<li> Advantage(s) of public key over secret key cryptography are
	<ol class="answer_list">
	<li> the algorithms are generally quite fast, handling large amounts
		of text/data efficiently.
	<li> it can be used as the basis for digital signatures.
	<li> it requires the secure exchange of keys.
	<li> that it's harder to break since the algorithms are kept secret.
	<li> None of the above
	</ol>
</li><br/>
<li> Disadvantage(s) of public key over secret key cryptography are
	<ol class="answer_list">
	<li> the algorithms are generally quite slow.
	<li> it can be used as the basis for digital signatures.
	<li> it requires the confidential exchange of keys.
	<li> that it's harder to break since the algorithms are kept secret.
	<li> None of the above
	</ol>
</li><br/>
<li> For Bob to send confidential messages to Alice, match the correct
	choice of person, action, and information in the following sequence
	of steps. You should restrict your choice to one of the two options
	following each blank.
	[Note: Some options may be used more than once, or not at all.]
	<ul class="bullet_list">
	<li> ___A___ [ Alice | Bob ] creates a public-private key pair.
	<li> ___B___ [ Alice | Bob ] publishes the ___C___ [ public | private ]
		key to the world.
	<li> ___D___ [ Alice | Bob ] uses ___E___'s [ Alice | Bob ]
		___F___ [ public | private ] key to
		___G___ [ encrypt | decrypt ] a message.
	<li> ___H___ [ Alice | Bob ] sends the ___I___ed [ encrypt | decrypt ]
		message to ___J___ [ Alice | Bob ].
	<li> ___K___ [ Alice | Bob ] receives the
		___L___ed [ encrypt | decrypt ] message from
		___M___ [ Alice | Bob ].
	<li> ___N___ [ Alice | Bob ] ___O___s [ encrypt | decrypt ]
		the message using the ___P___ [ public | private ] key.
	<li> Alice reads the message.
	</ul>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> Alice
		<li> Bob
		<li> public
		<li> private
		<li> encrypt
		<li> decrypt
		</ol>
	</td></tr></table>
</li><br/>
<li> Many available cryptographically secured communication systems (e.g.,
	PGP, GPG) use both secret key and public-private key algorithms because 
	<ol class="answer_list">
	<li> the public-private keys are used to exchange the secret keys since
		secret key algorithms are used to provide secrecy for the
		actual text/data (due to their greater speed and efficiency).
	<li> the secret keys are used to exchange the public-private keys since
		public-private key algorithms are used to provide secrecy for
		the actual text/data (due to their greater speed and
		efficiency).
	<li> the public-private keys are used to encrypt data for sending
		while the secret keys are used to decrypt data for reading.
	<li> the secret keys are used to encrypt data for sending while the
		public-private keys are used to decrypt data for reading.
	<li> None of the above
	</ol>
</li><br/>
<li> One-Way hash functions
	<ol class="answer_list">
	<li> encrypt their input, using only a secret key algorithm.
	<li> encrypt their input, using only a public key algorithm.
	<li> encrypt their input, using either a secret or public key algorithm.
	<li> create a digest of their input which is often shorter than
		the original text.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary difference(s) between encryption algorithms and one-way
	hash functions are
	<ol class="answer_list">
	<li> encryption algorithms are faster and more efficient.
	<li> encryption algorithms are reversible.
	<li> one-way hash functions <strong>must</strong> use a non-secret salt.
	<li> different inputs to one-way hash functions <strong>always</strong>
		provide different results.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are one-way hash functions are that currently
	recommended for use by the National Institute for Standards
	and Technology (NIST)?
	<ol class="answer_list">
	<li> RSA
	<li> SHA512
	<li> MD5
	<li> Twofish
	<li> None of the above
	</ol>
</li><br/>
<li> Which technologies (in combination) are used to provide non-repudiation
	for public communications (i.e., digital signatures)?
	<ol class="answer_list">
	<li> Symmetric key cryptography
	<li> Asymmetric key cryptography
	<li> One-way hash functions
	<li> Salt values.
	<li> None of the above
	</ol>
</li><br/>
<li> For Alice to digitally sign a document, and for Bob to subsequently
        confirm that signature for the given document, match the correct
	choice of person, action, and information in the following sequence
	of steps.  You should restrict your choice to one of the two options
	following each blank.
	[Note: Some options may be used more than once, or not at all.]
	<ul class="bullet_list">
	<li> ___A___ [ Alice | Bob ] creates a public-private key pair,
		publishing the public key to the world.
	<li> ___B___ [ Alice | Bob ] uses a one-way hash to create a
		message digest of the document.
	<li> ___C___ [ Alice | Bob ] uses their ___D___ [ public | private ]
		key to ___E___ [ encrypt | decrypt ] the
		___F___ [ message digest | document ], creating the
		signature block.
	<li> The signature block is then appended to the
		___G___ [ message digest | document ].
	<li> ___H___ [ Alice | Bob ] obtains ___I___'s [ Alice | Bob ]
		___J___ [ public | private ] key.
	<li> ___K___ [ Alice | Bob ] strips the signature block from the
		signed document.
	<li> ___L___ [ Alice | Bob ] ___M___s [ encrypt | decrypt ] the
		signature block to obtain a message digest (MDsigned) using
		___N___'s [ Alice | Bob ] ___O___ [ public | private ] key.
	<li> ___P___ [ Alice | Bob ] creates a second message digest (MDdoc).
	<li> ___Q___ [ Alice | Bob ] compares MDsigned with MDdoc,
		and if they are then this document was signed by Alice.
	</ul>
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> Alice
		<li> Bob
		<li> public
		<li> private
		<li> encrypt
		<li> decrypt
		<li> message digest
		<li> document
		</ol>
	</td></tr></table>
</li><br/>
<li> For the purpose of non-repudiation, keeping a document and signature
	block together is
	<ol class="answer_list">
	<li> unnecessary since the document can <strong>always</strong> be
		regenerated from the signature block.
	<li> helpful, but <strong>not</strong> necessary since the signature
		block can be recreated from the document.
	<li> necessary since the document <strong>must</strong> be used to
		create the message digest needed for confirmation.
	<li> necessary since the message digest for the document
		<strong>must</strong> be created using the salt from
		the signature block.
	<li> None of the above
	</ol>
</li><br/>
<li> An important aspect of public key cryptography that allows Alice to use
	the same public-private key pair to create digital signatures
	<em>and</em> to allow anyone else (e.g., Bob) to send her secure
	communications that no one else can read is due to which of the
	following characteristics of asymmetric algorithms like RSA?
	<ol class="answer_list">
	<li> the private key can be used to decrypt information encrypted using
		the private key.
	<li> the public key can be used to decrypt information encrypted using
		the public key.
	<li> the private key can be used to decrypt information encrypted using
		the public key.
	<li> the public key can be used to decrypt information encrypted using
		the private key.
	<li> None of the above
	</ol>
</li><br/>
<li> While retaining their non-repudiability characteristic, digital
	signatures can
	<ol class="answer_list">
	<li> be used to sign a document, but only by one person.
	<li> be used to sign multiple documents, but only by one person.
	<li> be used to have multiple people sign a document, by having
		each sign the previous combined document and signature
		block in succession (so long as the signing order is
		communicated).
	<li> be used to have muiltiple people sign a document, by having
		the primary signer use the private keys of the other signers
		to encrypt the message digest once with each key to create
		the signature block.
	<li> None of the above
	</ol>
</li><br/>
<li> Why is Alice's publishing of her public key on an unsecured web site
	insufficient to ensure confidentiality?
	<ol class="answer_list">
	<li> Because anyone could use her public key to read encrypted messages
		that others are sending to Alice.
	<li> Because anyone could digitally sign documents using Alice's
		public key, and thus could pretend to be Alice
		<strong>without</strong> anyone finding out.
	<li> Because a third party, Mallory, could make a random change to her
		published key making sure that Alice was unable to read
		any messages sent to her (encrypted with the altered public
		key).
	<li> Because a third party, Mallory, could change her published key
		to be that of his own public key, thus enabling a
		man-in-the-middle attack for encrypted communications sent
		to Alice.
	<li> None of the above
	</ol>
</li><br/>
<li> How does using a trusted third party certificate authority (CA) when
	publishing a public key help to ensure the secrecy of communications?
	<ol class="answer_list">
	<li> The digital signature of the CA provides assurance that the
		public key is unaltered.
	<li> The CA hosts a well secured and trusted web site to which the
		public key is published - thus avoiding the problems of using
		an unsecured web site.
	<li> The CA serves as an intermediary for <strong>all</strong>
		encrypted messages sent to Alice, and the CA decrypts the
		message on Alice's behalf to ensure that her public key
		was used to encrypt it.
	<li> The CA vouches for the authenticity of the public key (i.e., it
		belongs to the claimed owner).
	<li> None of the above
	</ol>
</li><br/>
<li> Key storage is an important consideration for secret keys when using
	symmetric cryptography because
	<ol class="answer_list">
	<li> if the one copy of the secret key is altered or lost, then any
		information encrypted using that key will be lost.
	<li> knowing one secret key from the key store compromises
		<strong>all</strong> other secret keys in the key store.
	<li> if a third party discovers the secret key, than anything encrypted
		using it can be compromised.
	<li> if a third party discovers the public key (of the key pair),
		they could "fake" perfect digital signatures on documents.
	<li> None of the above
	</ol>
</li><br/>
<li> Key storage is an important consideration for private keys when using
	asymmetric cryptography because
	<ol class="answer_list">
	<li> if the one copy of the private key is altered or lost, then any
		information encrypted using the corresponding public key will
		be lost.
	<li> knowing one private key from the key store compromises
		<strong>all</strong> other private keys in the key store.
	<li> if a third party discovers the private key, than anything
		encrypted using the corresponding public key can be compromised.
	<li> if a third party discovers the private key,
		they could "fake" legitimate digital signatures on documents.
	<li> None of the above
	</ol>
</li><br/>
<li> The purpose of a Hardware Security Module (HSM) with respect to
	cryptography is to
	<ol class="answer_list">
	<li> render the stored keys unusable upon detection of tampering.
	<li> prevent unauthorized users from accessing the stored keys.
	<li> verify the digital signatures of the stored keys.
	<li> create public-private key pairs for asymmetric algorithms.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantage of using a Trusted Platform Module (TPM) over using only
	a Hardware Security Module (HSM) is that
	<ol class="answer_list">
	<li> because the TPM verifies the Certificate Authority's digital
		signatures, <strong>all</strong> stored keys can be trusted.
	<li> the TPM only allows authorized users to read stored keys back
		to their programs.
	<li> the TPM renders the stored keys unusable upon detection of
		tampering.
	<li> the TPM performs the encrypt/decrypt operations so that keys
		<strong>never</strong> leave the module.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantage(s) of using a Trusted Platform Module (TPM) that is fully
	integrated into the hardware and OS include
	<ol class="answer_list">
	<li> the ability to securely encrypt <strong>all</strong> data stored
		on installed HDDs and SSDs.
	<li> the inability to install unauthorized (e.g., unsigned) software.
	<li> a guarantee that any encrypted data on the system can
		<strong>always</strong> be accessed.
	<li> attestation to verify that the computer is currently
		authorized to run a particular software (or access specific
		data).
	<li> None of the above
	</ol>
</li><br/>

<li> The process and policies for enabling users to prove who they are is
	called
	<ol class="answer_list">
	<li> authorization.
	<li> attestation.
	<li> authentication.
	<li> dispensation.
	<li> None of the above
	</ol>
</li><br/>
<li> The types of factors typically used to identify users are
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> Passwords and challenge-response mechanisms are examples of which type(s)
	of factor(s) used for user identification?
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> Credit cards and USB security dongles are examples of which type(s) of
	factor(s) used for user identification?
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> Fingerprints and retina scans are examples of which type(s) of factor(s)
	used for user identification?
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> To preserve the confidentiality of user passwords,
	<ol class="answer_list">
	<li> only their one-way hash values should be stored.
	<li> they should be stored as encrypted values.
	<li> they should be stored as two separate parts, each part
		encrypted using a different key.
	<li> storing them as cleartext is okay, provided the password
		file is readable only by the superuser.
	<li> None of the above
	</ol>
</li><br/>
<li> Many users pick passwords that are (or are similar to) real words. A
	common mechanism to reduce the chances of a successful dictionary
	attack on passwords,
	<ol class="answer_list">
	<li> encrypts each password using a common symmetric key.
	<li> stores passwords in two or more pieces, making it harder for
		attackers to get the full password.
	<li> encrypts the password using itself as the symmetric key.
	<li> adds random "salt" characters to the password before it is hashed.
	<li> None of the above
	</ol>
</li><br/>
<li> While password schemes are easy to understand and implement, they
	are made less secure due to people's
	<ol class="answer_list">
	<li> using their password too often by repeatedly logging out and
		back in again.
	<li> writing down their passwords or storing them as plaintext
		in a computer file.
	<li> using passwords that are very similar to natural language words,
		making them vulnerable to dictionary attacks.
	<li> using shorter passwords that are easier to remember.
	<li> None of the above
	</ol>
</li><br/>
<li> One-time passwords are more secure
	<ol class="answer_list">
	<li> because <strong>all</strong> of the unused passwords
		<strong>must</strong> be stored so that both the system
		and the user know which password should be used next.
	<li> since easedropping on communications for passwords won't
		enable the easedropper to login using them.
	<li> because the entire set of passwords <strong>must</strong> be
		agreed upon (and shared) in advance.
	<li> since they are created only as/when they are needed.
	<li> None of the above
	</ol>
</li><br/>
<li> One-way hash chains which create one-time passwords for transmission over
	communication networks are based on:
	<ol class="answer_list">
	<li> the repeated application of a non-invertible function.
	<li> having a maximum number of one-time passwords needed.
	<li> a secret seed/password value known only to the user.
	<li> the number of times to chain is specified by the server.
	<li> None of the above
	</ol>
</li><br/>
<li> One-way hash chains create
	<ol class="answer_list">
	<li> a fixed (but potentially large) number of one-time passwords that
		both the server and user <strong>must</strong> store/remember.
	<li> an unlimited number of one-time passwords used by the server,
		but the user only has to remember a single "base" password.
	<li> a new one-time password each time they are used. The user's
		public key is used by the server to transmit the new password
		for each login attempt.
	<li> a fixed (but potentially large) number of one-time passwords
		used by the server, but the user only has to remember a
		single "base" password.
	<li> None of the above
	</ol>
</li><br/>
<li> In order to ensure effectiveness, one-way hash chains for passwords require the
	<ol class="answer_list">
	<li> server to calculate the last value in the hash chain before
		the user can login the first time.
	<li> user to know the one-way hash function so that they can
		apply it to the server's challenge to calculate their response.
	<li> user to enter their password directly into the server.
	<li> user to enter their password into a trusted client that
		can apply the hash function to the user's password.
	<li> None of the above
	</ol>
</li><br/>
<li> When users setup a mathematical equation (e.g., f(x) = x + 4 ) as the
	basis for the computer to identify them by giving the user an
	(probably random) input (e.g., 5) and checking the user's answer
	against what the computer calculated for the same input (e.g., 9),
	this is called
	<ol class="answer_list">
	<li> Computed Authentication.
	<li> Challenge-Response.
	<li> Formula Identification.
	<li> One-way Hash Chain.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are <strong>not</strong> general rules for
	securing login/password information?
	<ol class="answer_list">
	<li> Restrict read access to saved (and hashed) passwords.
	<li> Deny access as soon as an incorrect character in a password is
		typed.
	<li> Wait until <strong>all</strong> passwords and security questions
		are answered before granting/denying access.
	<li> Immediately deny access if an incorrect user name is given.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are general rules for securing login/password
	information?
	<ol class="answer_list">
	<li> Restrict read access to saved (and hashed) passwords.
	<li> Deny access as soon as an incorrect character in a password is
		typed.
	<li> Wait until <strong>all</strong> passwords and security questions
		are answered before granting/denying access.
	<li> Immediately deny access if an incorrect user name is given.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following factors for identifying users is a poor choice
	if only one factor is used due to its susceptibility to theft?
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following factors for identifying users has the worst
	reliability due to variability of accurately acquiring the
	factor data? 
	<ol class="answer_list">
	<li> human factors.
	<li> knowledge factors.
	<li> possession factors.
	<li> inherence factors.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following would <strong>not</strong> be useful countermeasures
	to guard against compromises in user identification?
	<ol class="answer_list">
	<li> Only allow users to login from specific locations.
	<li> Ensure handprint being scanned is the correct body temperature.
	<li> Use random phrases for voiceprint verification (perhaps picked
		from a large set of saved phrases).
	<li> Require the physical use of a credit card rather than entering
		the number on a web form.
	<li> None of the above
	</ol>
</li><br/>
<li> Using multiple factors when identifying users
	<ol class="answer_list">
	<li> increases the confidence of a correct identification.
	<li> <strong>never</strong> works in practice since users reject
		it as taking too long.
	<li> provides lower confidence of a correct identification than if
		only one factor were used.
	<li> <strong>always</strong> prevents unauthorized access.
	<li> None of the above
	</ol>
</li><br/>

<li> A <em>buffer overflow</em> software bug enables an attacker to
	<ol class="answer_list">
	<li> read information outside the boundary of a data structure.
	<li> write information outside the boundary of a data structure.
	<li> calculate values that are too large to hold within a register.
	<li> load in values from memory too large to hold within a register.
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>buffer overflow</em> software bug is only possible when
	<ol class="answer_list">
	<li> the program code is compiled directly into machine code.
	<li> the compiled code is part of the OS kernel.
	<li> the programming language run-time doesn't check variable
		boundaries (e.g., array bounds).
	<li> the computer system uses Direct Memory Access (DMA), which
		can copy large blocks of data.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Buffer overflow</em> bugs are <strong>not</strong> possible in code
	<ol class="answer_list">
	<li> run on a POSIX compliant system.
	<li> that doesn't use arrays.
	<li> written in Java and run on a compliant JVM.
	<li> that avoids using OS system calls.
	<li> None of the above
	</ol>
</li><br/>
<li> The <em>Heartbleed</em> attack, in the OpenSSL library, exploits which
	type of software bug?
	<ol class="answer_list">
	<li> Buffer Overflow
	<li> Format String
	<li> Dangling Reference
	<li> Integer Overflow
	<li> Command Injection
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> a helpful technique for
	avoiding/defending against a <em>buffer overflow</em> bug?
	<ol class="answer_list">
	<li> Stack Canary
	<li> Address Space Layout Randomization
	<li> Data Execution Prevention
	<li> Using strong run-time checked programming languages like Java.
	<li> None of the above
	</ol>
</li><br/>
<li> Writing a random (but known) value just below the return address in a
	function's run-time stack entry and checking it for changes before
	returning from the function describes which buffer overflow
	prevention/detection technique?
	<ol class="answer_list">
	<li> Stack Canary
	<li> Address Space Layout Randomization
	<li> Data Execution Prevention
	<li> Using strong run-time checked programming languages like Java.
	<li> None of the above
	</ol>
</li><br/>
<li> Preventing the execution of code stored in either the heap of the run-time
	stack of a running process/thread, describes which buffer overflow
	prevention/detection technique?
	<ol class="answer_list">
	<li> Stack Canary
	<li> Address Space Layout Randomization
	<li> Data Execution Prevention
	<li> Using strong run-time checked programming languages like Java.
	<li> None of the above
	</ol>
</li><br/>
<li> Locating the stack, heap, libraries, and other portions of the process
	layout in different parts of the address space each time the program
	is run, is an example which buffer overflow prevention/detection
	technique?
	<ol class="answer_list">
	<li> Stack Canary
	<li> Address Space Layout Randomization
	<li> Data Execution Prevention
	<li> Using strong run-time checked programming languages like Java.
	<li> None of the above
	</ol>
</li><br/>
<li> Format string bugs enable attacks that
	<ol class="answer_list">
	<li> are a special kind of null reference attack.
	<li> typically leverage unchecked user input.
	<li> can disclose unauthorized data by printing beyond the bounds
		of an array.
	<li> are based on a software bug peculiar to C, which enables a
		printf statement to change variable values.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is a helpful technique for
	avoiding/defending against a <em>format string</em> bug?
	<ol class="answer_list">
	<li> Stack Canary
	<li> Address Space Layout Randomization
	<li> Data Execution Prevention
	<li> Using strong run-time checked programming languages like Java.
	<li> None of the above
	</ol>
</li><br/>
<li> Dangling references are created when
	<ol class="answer_list">
	<li> a function returns a pointer to one of its local variables as
		its result.
	<li> the language run-time does <strong>not</strong> provide
		garbage collection.
	<li> dynamic (heap) allocated memory is <strong>not</strong> freed
		when it's no longer needed.
	<li> a pointer to a deallocated heap object is retained.
	<li> None of the above
	</ol>
</li><br/>
<li> Dangling references occur whenever
	<ol class="answer_list">
	<li> dynamic (heap) allocated memory is used.
	<li> a variable holds the address of unallocated memory.
	<li> dynamic (heap) allocated memory is <strong>not</strong> freed
		when it's no longer needed.
	<li> the language run-time does <strong>not</strong> provide
		garbage collection.
	<li> None of the above
	</ol>
</li><br/>
<li> A dangling reference bug can be exploited by
	<ol class="answer_list">
	<li> exhausting the amount of dynamic (heap) storage available.
	<li> simply writing to the deallocated storage at any time.
	<li> writing the dangling reference address to the storage it points
		to, thus creating a circular structure.
	<li> by changing an unrelated data structure when the deallocated
		storage pointed to by the dangling reference is part of
		a new dynamic (heap) allocation.
	<li> None of the above
	</ol>
</li><br/>
<li> The <strong>best</strong> defense(s) against dangling reference bugs is to
	<ol class="answer_list">
	<li> <strong>always</strong> fill newly allocated dynamic
		(heap) memory with zeros.
	<li> use automatic garbage collection.
	<li> disable the free/deallocate routine so that memory cannot be
		reused.
	<li> limit each object referenced by a pointer to be assigned at most
		twice.
	<li> None of the above
	</ol>
</li><br/>
<li> (De)referencing a null pointer as a function to call usually causes an
	error since there is no code at address 0 to run, but
	<ol class="answer_list">
	<li> if code is mapped to address 0 (e.g., by the <em>mmap</em> system
		call), then the mapped code will be run instead of crashing.
	<li> using garbage collection prevents dereferencing null pointers.
	<li> it is <strong>not</strong> possible to use a pointer as a
		function to call, except in assembly language, so this is
		seldom a problem.
	<li> this can be avoided by using a special (e.g., negative) value
		to mark uninitialized pointers.
	<li> None of the above
	</ol>
</li><br/>
<li> Integer Overflow bugs occur when
	<ol class="answer_list">
	<li> a number that is too large is loaded from memory into a register.
	<li> a number that is too large is stored from a register into memory.
	<li> the result of an integer operation (e.g., multiplication)
		is larger than can be stored in a register, and
		<strong>no</strong> error is generated.
	<li> the result of an integer operation (e.g., multiplication)
		is too large, writing beyond the intended variable space
		(a specific form of the buffer overflow bug).
	<li> None of the above
	</ol>
</li><br/>
<li> Command Injection occurs when
	<ol class="answer_list">
	<li> the input buffer overflows and writes new commands into the
		program code.
	<li> a null pointer is dereferenced and the 0th address has been mapped
		to unauthorized code.
	<li> a function call is passed a dangling reference as one of its
		parameters.
	<li> unsanitized user input is used as part of an executed action.
	<li> None of the above
	</ol>
</li><br/>
<li> SQL injection is one of the most common forms of command injection. For
	the following SQL code, which value of <em>$input_name</em> would
	cause a command injection <strong>without</strong> causing a SQL error?
<pre>
	SELECT BirthDate FROM PersonTable WHERE Name = '$input_name';
</pre>
	<ol class="answer_list">
	<li> TRUNCATE TABLE PersonTable;
	<li> 'TRUNCATE TABLE PersonTable'; --
	<li> Joe TRUNCATE TABLE PersonTable;
	<li> Joe'; TRUNCATE TABLE PersonTable; --
	<li> None of the above
	</ol>
</li><br/>
<li> Time of Check to Time of Use (TOCTOU) attacks
	<ol class="answer_list">
	<li> exploit the lack of semaphore use in most programs.
	<li> leverage the race condition between checking that a use will be
		valid and the actual use.
	<li> are impossible to prevent since there will
		<strong>always</strong> be a delay between checking the
		legality of something and then actually doing it.
	<li> None of the above
	</ol>
</li><br/>
<li> Time of Check to Time of Use (TOCTOU) attacks are enabled by
	<ol class="answer_list">
	<li> processing delays in the underlying file system(s).
	<li> the use of preemptive scheduling.
	<li> poor program design and coding.
	<li> improperly designed application programming interfaces (APIs).
	<li> None of the above
	</ol>
</li><br/>

<li> Insider attacks differ from attacks by outsiders in that the attacker has
	<ol class="answer_list">
	<li> superuser rights on the computer systems.
	<li> access to the environment.
	<li> unrestricted access to the network.
	<li> specific knowledge of the environment (both computer and human).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is a type of insider attack?
	<ol class="answer_list">
	<li> Logic Bomb
	<li> Rootkit
	<li> Spoofing
	<li> Denial of Service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is a type of insider attack?
	<ol class="answer_list">
	<li> Logic Bomb
	<li> Back Door
	<li> Worm
	<li> Denial of Service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> true of a logic bomb?
	<ol class="answer_list">
	<li> They can be set to activate at a particular time or when an event
		happens (or fails to happen).
	<li> It is immediately obvious when a logic bomb has "exploded".
	<li> Bombs can remove files or change encryption/decryption keys.
	<li> They are nearly always installed after an employee has
		left the organization (usually for revenge).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> true of a back door attack?
	<ol class="answer_list">
	<li> All back door attacks provide login access to the computer system
		or network.
	<li> Are crafted to allow privileged access to data or the computer
		system.
	<li> Some degree of special privilege or access is required to install
		a back door attack.
	<li> Back doors don't necessarily guarantee access, but may offer only
		a greater chance of gaining access.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is <strong>not</strong> true of a spoofing attack?
	<ol class="answer_list">
	<li> Spoofing attacks require some degree of privileged access to data
		or the computer system.
	<li> Are based on mimicking an existing interface in order to trick
		users into providing secret information.
	<li> Spoofing attacks are the most difficult attack to defend against.
	<li> Are a specialized form of back door attacks.
	<li> None of the above
	</ol>
</li><br/>
<li> Phishing attacks are a special type of spoofing attack that
	<ol class="answer_list">
	<li> can have success rates as high as 70%.
	<li> depend upon specialized and confidential information in order to
		convince others to provide computer access.
	<li> leverage a back door to gain credibility, leveraging that trust
		into more elevated access.
	<li> use social engineering techniques to gain information that leads
		to computer and data access.
	<li> None of the above
	</ol>
</li><br/>
<li> Defending against insider attacks
	<ol class="answer_list">
	<li> uses exactly the same techniques as defending against outside
		attacks.
	<li> involves separation of duties so that one person doesn't have
		too much control.
	<li> often uses code reviews so that it is harder to sneak in an
		obvious weakness/attack.
	<li> benefit from keeping personnel in the same position for
		many years to reduce social engineering opportunities.
	<li> None of the above
	</ol>
</li><br/>

<li> The types of vectors for conducting an outside attack are
	<ol class="answer_list">
	<li> Worm
	<li> Smurfing
	<li> Trojan Horse
	<li> Virus
	<li> None of the above
	</ol>
</li><br/>
<li> Non-replicating malware that is embedded in a (usually) useful program
	is called a
	<ol class="answer_list">
	<li> Worm
	<li> Rootkit
	<li> Trojan Horse
	<li> Virus
	<li> None of the above
	</ol>
</li><br/>
<li> Malware that replicates and spreads by attaching itself to other programs
	is called a
	<ol class="answer_list">
	<li> Worm
	<li> Rootkit
	<li> Trojan Horse
	<li> Virus
	<li> None of the above
	</ol>
</li><br/>
<li> Malware in the form of a self-replicating standalone program is called a
	<ol class="answer_list">
	<li> Worm
	<li> Rootkit
	<li> Trojan Horse
	<li> Virus
	<li> None of the above
	</ol>
</li><br/>
<li> Which types of viruses generally try to hide themselves within the
	interrupt vector?
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> Microkernels provide more protection than monolithic kernels against which
	type of virus?
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of virus is perhaps the most dangerous to have a compiler
	infected with?
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> A Macro virus is a special type of what kind of virus?
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> The virus that modifies the program that loads the OS and ultimately
	becomes memory-resident after the system is up and running, is a
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> A virus that infects processes but <strong>not</strong> program files is a
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> A virus that adds a copy of itself to an existing program, leaving the
	rest of the program untouched, is a
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> A virus that replaces code in an existing program with new code is a
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Companion virus.
	<li> Overwriting virus.
	<li> Boot Sector virus.
	<li> Parasitic virus.
	<li> Device Driver virus.
	<li> Macro virus.
	<li> Source Code virus.
	<li> None of the above
	</ol>
</li><br/>
<li> Malware that is particularly effective in concealing its existence, that
	strongly resists removal, and provides superuser access to the system
	is a
	<ol class="answer_list">
	<li> Memory-Resident virus.
	<li> Parasitic virus.
	<li> Rootkit.
	<li> Boot Sector virus.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of malware doesn't directly impact or provide access to
	the infected system but has these characteristics:
	hides to avoid detection; collects info from the infected
	host; communicates collected info to a remote server; and
	is difficult to remove?
	<ol class="answer_list">
	<li> Rootkit
	<li> Spyware
	<li> Botnet
	<li> Smurfer
	<li> None of the above
	</ol>
</li><br/>
<li> A keylogger is an example of (a)
	<ol class="answer_list">
	<li> Rootkit.
	<li> Spyware.
	<li> Botnet.
	<li> Smurfer.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is the most common type of rootkit?
	<ol class="answer_list">
	<li> Application
	<li> Library
	<li> Hypervisor
	<li> Firmware
	<li> None of the above
	</ol>
</li><br/>
<li> Attempts to make a targeted system/network unusable by flooding it with
	service requests is called a
	<ol class="answer_list">
	<li> Network sniffing attack.
	<li> Worm Armageddon.
	<li> Denial of Service (DoS) attack.
	<li> Trojan stampede.
	<li> None of the above
	</ol>
</li><br/>
<li> When the requests for a Denial of Service (DoS) attack come from multiple
	sources, this is an example of
	<ol class="answer_list">
	<li> Snooping.
	<li> Network sniffing attack.
	<li> Worm Armageddon.
	<li> Trojan stampede.
	<li> None of the above
	</ol>
</li><br/>
<li> Smurfing is an example of a
	<ol class="answer_list">
	<li> Network sniffing attack.
	<li> Worm Armageddon.
	<li> Distributed Denial of Service (DDoS) attack.
	<li> Trojan stampede.
	<li> None of the above
	</ol>
</li><br/>
<li> A Distributed Denial of Service (DDoS) attack can be conducted by 
	<ol class="answer_list">
	<li> network sniffing from a single host.
	<li> having bots in a large botnet target a single system.
	<li> smurfing.
	<li> a trojan stampede.
	<li> None of the above
	</ol>
</li><br/>
<li> A Distributed Denial of Service attach that uses faked sender IP
	addresses (of the targeted system) on requests sent to a large set of
	unsuspecting intermediary systems is called
	<ol class="answer_list">
	<li> smurfing.
	<li> a trojan stampede.
	<li> network sniffing.
	<li> a botnet flood.
	<li> None of the above
	</ol>
</li><br/>
<li> Most systems today are <strong>not</strong> vulnerable to smurfing
	attacks because of these common measures:
	<ol class="answer_list">
	<li> do <strong>not</strong> respond to broadcast messages.
	<li> use in-bound filtering to prevent spoofing the source IP address.
	<li> do <strong>not</strong> forward broadcast messages.
	<li> only respond to messages from white-listed source IP addresses.
	<li> None of the above
	</ol>
</li><br/>
<li> Passively listening in on traffic, particularly on unencrypted wireless
	networks, using a packet analyzer is called
	<ol class="answer_list">
	<li> smurfing.
	<li> keylogging.
	<li> sporking.
	<li> network sniffing.
	<li> None of the above
	</ol>
</li><br/>
<li> The risk of a successful communication attack can be greatly reduced
	by using
	<ol class="answer_list">
	<li> digital signatures.
	<li> one-way hash functions.
	<li> symmetric-key encryption between systems (e.g., SSL).
	<li> free public (and anonymous) WiFi services.
	<li> None of the above
	</ol>
</li><br/>
<li> If Mallory conducts a man-in-the-middle attack on the communications
	between Alice and Bob, Mallory's existence will likely go unnoticed
	for the longest period of time if
	<ol class="answer_list">
	<li> Alice and Bob use a variety of communication methods.
	<li> either Alice or Bob (but <strong>not</strong> both) digitally
		signs their communications.
	<li> Mallory is able to intercept <strong>all</strong> of the traffic
		coming from either Alice or Bob (but <strong>not</strong> both).
	<li> Mallory intercepts <strong>all</strong> traffic/messages between
		Alice and Bob. 
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following <strong>ensures</strong> confidentiality when
	using public WiFi services?
	<ol class="answer_list">
	<li> Only use those that have a password, often issued when a
		purchase is made (e.g., Starbucks).
	<li> Make sure that the majority of web communications are
		conducted using https (e.g., http with Secure Sockets Layer
		encryption).
	<li> Disconnect and reconnect to the network every 15 minutes or less.
	<li> Run your own network sniffer to see if anyone else is snooping
		on the WiFi communications.
	<li> None of the above
	</ol>
</li><br/>

<li> Using multiple layers of defenses, so that if one layer is breached there
	are others behind it to prevent the attack from progressing, is
	known as
	<ol class="answer_list">
	<li> layered defense.
	<li> defense in depth.
	<li> castle walls.
	<li> ringed fortifications.
	<li> None of the above
	</ol>
</li><br/>
<li> Requiring approval for software installation or changes to the Windows
	registry, and informing the user of outbound communications are
	examples of what type of security defense?
	<ol class="answer_list">
	<li> Informed consent
	<li> Administrator authorization
	<li> User/Administrator confirmation
	<li> Permissioned protection
	<li> None of the above
	</ol>
</li><br/>
<li> The weakness of user confirmation as a means of defense is that
	<ol class="answer_list">
	<li> users may not know enough to determine the appropriate action to
		take.
	<li> users seldom have the necessary system permissions to accomplish
		the desired action.
	<li> pop-up confirmation screens are so common in software that most
		users turn them off.
	<li> they annoy users, who after answer randomly after reading the
		information note.
	<li> None of the above
	</ol>
</li><br/>
<li> As a security defense layer, firewalls are used to
	<ol class="answer_list">
	<li> transform message traffic so that its contents are guaranteed
		to be safe.
	<li> route inbound network message traffic to the correct computer
		system.
	<li> convert messages from one network protocol into another so as
		to avoid errors caused by using the wrong protocol on a
		network segment.
	<li> serve as gatekeepers, determining which messages are passed
		along (both into and out of the system/network).
	<li> None of the above
	</ol>
</li><br/>
<li> Data Loss Prevention (DLP) examines the contents of outbound message
	packets to ensure that
	<ol class="answer_list">
	<li> no unencrypted personal identifying or financial information is
		sent out.
	<li> <strong>all</strong> data in the packet
		(except header information) is encrypted.
	<li> only data sent by authorized users is sent out.
	<li> no corporate trade secrets or intellectual property is
		inadvertently sent out.
	<li> None of the above
	</ol>
</li><br/>
<li> What type of firewall inspects the full contents of messages
	(<strong>not</strong> just their header information) in determining
	whether or not to ACCEPT or DENY message packets?
	<ol class="answer_list">
	<li> Standalone firewall
	<li> Stateless firewall
	<li> Stateful firewall
	<li> Intrusion Detection System (IDS)
	<li> None of the above
	</ol>
</li><br/>
<li> What type of firewall uses information from previous messages in addition
	to the current message in determining whether or not
	to ACCEPT or DENY the current message?
	<ol class="answer_list">
	<li> Standalone firewall
	<li> Stateless firewall
	<li> Stateful firewall
	<li> Intrusion Detection System (IDS)
	<li> None of the above
	</ol>
</li><br/>
<li> What type of firewall only uses the information within the message
	packet for making decisions about whether or not to ACCEPT or
	DENY the message?
	<ol class="answer_list">
	<li> Standalone firewall
	<li> Stateless firewall
	<li> Stateful firewall
	<li> Intrusion Detection System (IDS)
	<li> None of the above
	</ol>
</li><br/>
<li> Anti-virus scanners identify files potentially infected by a virus.
	Which of the following limit the utility of this approach to
	securing a system?
	<ol class="answer_list">
	<li> Polymorphic viruses change their code when infecting new files
		to avoid detection.
	<li> False positive matches can cause users to delete uninfected files.
	<li> False negative matches give users a false sense of security,
		enabling the virus to continue replicating.
	<li> Signatures for a zero-day virus are <strong>not</strong>
		in the scanner's database, and thus cannot be detected.
	<li> None of the above
	</ol>
</li><br/>
<li> Calculating the hash value of every file in a system known
	<strong>not</strong> to be infected with any viruses and keeping them
	for later comparison, is the basis for what anti-virus technique?
	<ol class="answer_list">
	<li> Jailers
	<li> Code Signers
	<li> Behavioral Checkers
	<li> Integrity Checkers
	<li> None of the above
	</ol>
</li><br/>
<li> For integrity checking to be effective in detecting the existence of virus
	infected files, the hash values it creates <strong>must</strong> be
	<ol class="answer_list">
	<li> periodically recalculated and compared to the original stored
		values.
	<li> calculated on infected files.
	<li> kept in a secure location (or digitally signed) so that they
		cannot be altered (<strong>without</strong> detection).
	<li> created using the running process if the file is a program.
	<li> None of the above
	</ol>
</li><br/>
<li> Examining the actions of processes to catch questionable behavior,
	is used by which anti-virus technique?
	<ol class="answer_list">
	<li> Code Signers
	<li> Behavioral Checkers
	<li> Integrity Checkers
	<li> Sandboxing
	<li> None of the above
	</ol>
</li><br/>
<li> The difficulty in distinguishing legitimate actions from actions due
	to malware reduces the effectiveness of which anti-virus technique?
	<ol class="answer_list">
	<li> Code Signers
	<li> Behavioral Checkers
	<li> Integrity Checkers
	<li> Sandboxing
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are good ways to <em>avoid</em> infection by a
	virus?
	<ol class="answer_list">
	<li> Only install software from reputable sources.
	<li> Run regular anti-virus scans of the file system.
	<li> Configure a personal firewall to only ACCEPT messages in
		response to out-going requests.
	<li> Keep frequent (and multiple version/generation) file backups.
	<li> Avoid opening email attachments that haven't passed a virus
		scan.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are good ways to <em>limit</em> the damage done
	by a virus infection should one occur?
	<ol class="answer_list">
	<li> Only install software from reputable sources.
	<li> Use a microkernel OS with the remaining non-kernel software
		running with non-superuser privileges.
	<li> Avoid opening email attachments that haven't passed a virus
		scan.
	<li> Keep frequent (and multiple version/generation) file backups.
	<li> None of the above
	</ol>
</li><br/>
<li> To ensure that downloaded software has been unchanged and is what it
	claims to be and really comes from the indicated source, use
	<ol class="answer_list">
	<li> Digital Signatures (aka Code Signing).
	<li> a white-list of reputable web URLs from which to download software.
	<li> a black-list of web URLs to avoid.
	<li> Jailing.
	<li> None of the above
	</ol>
</li><br/>
<li> When (new) software is run within a container and <strong>all</strong>
	system calls are passed through an examiner that determines whether
	or not to pass the call along to the kernel or abort the running
	program (as a hazard and probable malware), this is an example of
	<ol class="answer_list">
	<li> Jailing.
	<li> Sandboxing.
	<li> Smurfing.
	<li> Code Signing.
	<li> None of the above
	</ol>
</li><br/>
<li> Running new software within its own virtual machine (until the software
	is fully trusted) is an example of
	<ol class="answer_list">
	<li> Jailing.
	<li> Sandboxing.
	<li> Quarantining.
	<li> Code Signing.
	<li> None of the above
	</ol>
</li><br/>
<li> When a program is limited to a range of addresses within its address
	space as holding executable code, with data residing in another
	limited range of addresses in the address space, and no code or
	data outside the address space can be used, this is called
	<ol class="answer_list">
	<li> Jailing.
	<li> Sandboxing.
	<li> Quarantining.
	<li> Segregation.
	<li> None of the above
	</ol>
</li><br/>
<li> Java applets use which techniques to ensure safe execution?
	<ol class="answer_list">
	<li> Sandboxing.
	<li> Segregation.
	<li> Compilation.
	<li> Interpretation.
	<li> None of the above
	</ol>
</li><br/>
<li> Some of the actions performed by an interpreter to improve the safety
	of execution (e.g., passing resource requests to a security manager)
	are similar to those used in
	<ol class="answer_list">
	<li> Jailing.
	<li> Sandboxing.
	<li> Quarantining.
	<li> Segregation.
	<li> None of the above
	</ol>
</li><br/>

<li> CPU speeds are limited by these factors:
	<ol class="answer_list">
	<li> the speed of light.
	<li> heat dissipation.
	<li> electrical resistance.
	<li> feature (e.g., wire) size.
	<li> None of the above
	</ol>
</li><br/>
<li> Vertical scaling refers to the increasing of system performance by
	<ol class="answer_list">
	<li> Adding more resources (e.g., memory, CPU), or replacing
		existing resources with faster versions.
	<li> Adding additional compute nodes (and network switches as needed).
	<li> Increasing the number of VMs running on a type 1 hypervisor.
	<li> Increasing the number of VMs running on a type 2 hypervisor.
	<li> None of the above
	</ol>
</li><br/>
<li> Horizontal scaling refers to the increasing of system performance by
	<ol class="answer_list">
	<li> Adding more resources (e.g., memory, CPU), or replacing
		existing resources with faster versions.
	<li> Adding additional compute nodes (and network switches as needed).
	<li> Increasing the number of VMs running on a type 1 hypervisor.
	<li> Increasing the number of VMs running on a type 2 hypervisor.
	<li> None of the above
	</ol>
</li><br/>
<li> Horizontal scaling for handling many simple, independent, and stateless
	requests benefits <strong>most</strong> from
	<ol class="answer_list">
	<li> the communication between system components having low latency.
	<li> the communication between system components having high bandwidth.
	<li> a large shared memory architecture.
	<li> a large number of independent compute nodes.
	<li> None of the above
	</ol>
</li><br/>
<li> Horizontal scaling for handling many interdependent requests requires
	<ol class="answer_list">
	<li> a large shared memory architecture.
	<li> a large number of independent compute nodes.
	<li> the communication between system components having high latency.
	<li> the communication between system components having low bandwidth.
	<li> None of the above
	</ol>
</li><br/>
<li> The categories of multiprocessor systems are determined by
	<ol class="answer_list">
	<li> the communication system (e.g., bus, packet switched).
	<li> the number of simultaneous instructions performed (e.g., one or
		many).
	<li> the amount of data operated on at a time (e.g., one or many).
	<li> the types of compute nodes used (e.g., multi-core, multi-cpu).
	<li> None of the above
	</ol>
</li><br/>
<li> Symmetric multiproccessors (SMPs) that use multiple CPUs sharing a
	single memory are an example of which type of multiprocessor system?
	<ol class="answer_list">
	<li> Single Instruction, Single Data (SISD)
	<li> Single Instruction, Multiple Data (SIMD)
	<li> Multiple Instruction, Single Data (MISD)
	<li> Multiple Instruction, Multiple Data (MIMD)
	<li> None of the above
	</ol>
</li><br/>
<li> Graphical processing units (GPUs) are a good example of which type of
	multiprocessor system?
	<ol class="answer_list">
	<li> Single Instruction, Single Data (SISD)
	<li> Single Instruction, Multiple Data (SIMD)
	<li> Multiple Instruction, Single Data (MISD)
	<li> Multiple Instruction, Multiple Data (MIMD)
	<li> None of the above
	</ol>
</li><br/>
<li> The Space Shuttle control computers, which perform the same set of
	instructions on the same data but in different computers (providing
	fault tolerance) is an example of which type of multiprocessor system?
	<ol class="answer_list">
	<li> Single Instruction, Single Data (SISD)
	<li> Single Instruction, Multiple Data (SIMD)
	<li> Multiple Instruction, Single Data (MISD)
	<li> Multiple Instruction, Multiple Data (MIMD)
	<li> None of the above
	</ol>
</li><br/>
<li> A typical single CPU computer, such as a laptop, is an example of
	which type of multiprocessor system?
	<ol class="answer_list">
	<li> Single Instruction, Single Data (SISD)
	<li> Single Instruction, Multiple Data (SIMD)
	<li> Multiple Instruction, Single Data (MISD)
	<li> Multiple Instruction, Multiple Data (MIMD)
	<li> None of the above
	</ol>
</li><br/>
<li> Loosely coupled communication systems are exemplified by
	<ol class="answer_list">
	<li> longer latency times
	<li> shorter latency times
	<li> controlled within a single system (that also contains the CPUs)
	<li> control is shared/distributed across many systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Tightly coupled communication systems are exemplified by
	<ol class="answer_list">
	<li> longer latency times
	<li> shorter latency times
	<li> controlled within a single system (that also contains the CPUs)
	<li> control is shared/distributed across many systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type(s) of communication system divides messages into smaller
	pieces, each of which may take a different route from source to
	destination?
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> The most common type of communication system for systems with only
	a small number of CPUs (e.g., 1-4) is the 
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> Which type(s) of communication system provides a dedicated/fixed
	path for the duration of the communication?
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> The Internet is example of which type of communication system?
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> The plain old telephone system (POTS) network is an example of which type
	of communication system?
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> The original incarnation of Ethernet networking is an example of which
	type of communication system?
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>
<li> While simple, contention and collisions are the primary drawbacks of
	this type of communication system.
	<ol class="answer_list">
	<li> Bus
	<li> Circuit Switched
	<li> Packet Switched
	<li> Mesh
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following are a primary category of shared memory
	multiprocessors?
	<ol class="answer_list">
	<li> Global Memory Access
	<li> Non-uniform Memory Access
	<li> Hierarchical Memory Access
	<li> Uniform Memory Access
	<li> Cached Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are a primary category of shared memory
	multiprocessors?
	<ol class="answer_list">
	<li> Global Memory Access
	<li> Non-uniform Memory Access
	<li> Hierarchical Memory Access
	<li> Cached Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are a primary category of shared memory
	multiprocessors?
	<ol class="answer_list">
	<li> Global Memory Access
	<li> Hierarchical Memory Access
	<li> Uniform Memory Access
	<li> Cached Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> When every memory word can be read as fast as any other memory word,
	this is called
	<ol class="answer_list">
	<li> Global Memory Access
	<li> Hierarchical Memory Access
	<li> Cached Memory Access
	<li> Uniform Memory Access
	<li> Non-uniform Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> When some memory words can be read faster than other memory words,
	this is called
	<ol class="answer_list">
	<li> Global Memory Access
	<li> Hierarchical Memory Access
	<li> Cached Memory Access
	<li> Uniform Memory Access
	<li> Non-uniform Memory Access
	<li> None of the above
	</ol>
</li><br/>
<li> A symmetric multiproccesor (SMP) system is an example of a
	<ol class="answer_list">
	<li> Bus-based Uniform Memory Access architecture (UMA).
	<li> Bus-based Non-uniform Memory Access architecture (NUMA).
	<li> Switch-based Uniform Memory Access architecture (UMA).
	<li> Switch-based Non-uniform Memory Access architecture (NUMA).
	<li> None of the above
	</ol>
</li><br/>
<li> The system architecture(s) that work well for a small number of CPUs
	(< 4) but suffers too much contention as the number of CPUs becomes
	larger (> 8) is/are
	<ol class="answer_list">
	<li> Bus-based Uniform Memory Access architecture (UMA).
	<li> Bus-based Non-uniform Memory Access architecture (NUMA).
	<li> Switch-based Uniform Memory Access architecture (UMA).
	<li> Switch-based Non-uniform Memory Access architecture (NUMA).
	<li> None of the above
	</ol>
</li><br/>
<li> A system architecture that divides memory up into smaller equal
	sized chunks and uses an omega network to CPUs to memories is an
	example of
	<ol class="answer_list">
	<li> Bus-based Uniform Memory Access architecture (UMA).
	<li> Bus-based Non-uniform Memory Access architecture (NUMA).
	<li> Switch-based Uniform Memory Access architecture (UMA).
	<li> Switch-based Non-uniform Memory Access architecture (NUMA).
	<li> None of the above
	</ol>
</li><br/>
<li> Contention on a bus-based uniform memory access architecture can be
	reduced by
	<ol class="answer_list">
	<li> adding a small number of additional CPUs.
	<li> increasing the size of the shared memory.
	<li> adding a cache to each CPU.
	<li> reducing the number of address lines on the bus. 
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>write-thru cache</em> updates memory whenever a cache line is
	modified.  To maintain cache coherency on a uniform memory access
	architecture, the
	<ol class="answer_list">
	<li> cache doing the write-thru directly updates <strong>all</strong>
		of the other caches as well.
	<li> other caches should recognize the update on the system bus
		and update themselves to reflect the change if they
		hold the same data.
	<li> cache doing the write-thru sends a signal out on the system bus
		causing <strong>all</strong> other caches to be reread from
		memory.
	<li> system bus automatically updates any cache(s) containing the
		same data.
	<li> None of the above
	</ol>
</li><br/>
<li> Having caches that <em>snoop</em> on the bus is one way in which a
	symmetric multiprocessor (SMP) system can
	<ol class="answer_list">
	<li> reduce bus contention.
	<li> increase memory access speeds.
	<li> synchronize different CPU caches.
	<li> increase the bus' bandwidth. 
	<li> None of the above
	</ol>
</li><br/>
<li> A common <em>cache-coherence protocol</em> on a bus-based uniform
	memory access architecture uses
	<ol class="answer_list">
	<li> write-thru cache
	<li> shared CPU caching
	<li> uniform access bus caching
	<li> (bus) snooping
	<li> None of the above
	</ol>
</li><br/>
<li> The <em>coherency wall</em>, on a bus-based uniform memory access
	architecture, refers to the point at which the
	<ol class="answer_list">
	<li> combined size of <strong>all</strong> the CPU caches is the
		same as that of main memory.
	<li> number of CPUs (and associated caches) causes too much contention
		on the system bus for effective cache snooping to occur.
	<li> cost of cache-coherence hardware negates the advantage of having
		additional cores on a single CPU chip.
	<li> bus contention for cache writes to memory is greater than the
		number of collision free writes.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantage(s) of using crossbar switching (over an omega network)
	when connecting CPUs to memory chunks in a switch-based uniform memory
	access architecture are
	<ol class="answer_list">
	<li> each CPU can access a different memory chunk at the same time
		(reduced contention).
	<li> the number of switches grows as O(n^2) where n is the number of
		CPUs (and the number of memory chunks).
	<li> in some situations, it is possible for mulitple CPUs to access
		the same memory chunk.
	<li> write-thru caches with snooping can be effectively implemented.
	<li> None of the above
	</ol>
</li><br/>
<li> The disadvantage(s) of using crossbar switching (over an omega network)
	when connecting CPUs to memory chunks in a switch-based uniform memory
	access architecture are
	<ol class="answer_list">
	<li> not all of the CPUs can access a different memory chunk at the
		same time.
	<li> the number of switches grows as O(n^2) where n is the number of
		CPUs (and the number of memory chunks).
	<li> in some situations, it is possible for mulitple CPUs to access
		the same memory chunk.
	<li> write-thru caches with snooping cannot be effectively implemented.
	<li> None of the above
	</ol>
</li><br/>
<li> The disadvantage(s) of using an omega network (over crossbar switching)
	when connecting CPUs to memory chunks in a switch-based uniform memory
	access architecture are
	<ol class="answer_list">
	<li> not all of the CPUs can access a different memory chunk at the
		same time.
	<li> the number of switches grows as O(n^2) where n is the number of
		CPUs (and the number of memory chunks).
	<li> in some situations, it is possible for mulitple CPUs to access
		the same memory chunk.
	<li> write-thru caches with snooping cannot be effectively implemented.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantage(s) of using an omega network (over crossbar switching)
	when connecting CPUs to memory chunks in a switch-based uniform memory
	access architecture are
	<ol class="answer_list">
	<li> each CPU can access a different memory chunk at the same time
		(reduced contention).
	<li> the number of switches grows as O(n^2) where n is the number of
		CPUs (and the number of memory chunks).
	<li> in some situations, it is possible for mulitple CPUs to access
		the same memory chunk.
	<li> write-thru caches with snooping can be effectively implemented.
	<li> None of the above
	</ol>
</li><br/>
<li> Since memory references are often consecutive, one way to reduce memory
	contention in a switch-based uniform memory access architecture that
	is using an omega network, is to
	<ol class="answer_list">
	<li> read data from memory in large chunks (e.g., 4K).
	<li> write data to memory in large chunks (e.g., 4K).
	<li> interleave the data between the memory chunks (i.e., consecutive
		memory words appear in consecutive memory modules). 
	<li> use snooping caches, but without write-thrus.
	<li> None of the above
	</ol>
</li><br/>
<li> Unlike a crossbar switch, a CPU connecting to a memory chunk in an
	omega network
	<ol class="answer_list">
	<li> will almost always be connected through multiple (rather than
		a single) switch.
	<li> will <strong>always</strong> be connected through a single
		(rather than multiple) switch.
	<li> has lower latency if the same speed switches are used.
	<li> prevents any other CPU from accessing at least some other memory
		chunk(s).
	<li> None of the above
	</ol>
</li><br/>
<li> Unlike an omega network, a CPU connecting to a memory chunk in using a
	crossbar switch
	<ol class="answer_list">
	<li> will almost always be connected through multiple (rather than
		a single) switch.
	<li> will <strong>always</strong> be connected through a single
		(rather than multiple) switch.
	<li> has lower latency if the same speed switches are used.
	<li> prevents any other CPU from accessing at least some other memory
		chunk(s).
	<li> None of the above
	</ol>
</li><br/>
<li> Non-uniform memory access architectures with locally cached memory
	values, are commonly implemented using a
	<ol class="answer_list">
	<li> single global file system, with different directories in the
		file system corresponding to each of the different nodes
		(and their memory values).
	<li> database at each node that keeps track of which caches have
		copies of the data from this node's memory.
	<li> message broadcast to <strong>all</strong> nodes, with the first
		positive response being used for reads (while writes require
		no response).
	<li> separate dedicated bus for each grouping of 2-6 nodes, with a
		crossbar switch connecting the groups to one another.
	<li> None of the above
	</ol>
</li><br/>
<li> A non-uniform memory access architecture divides the memory up into
	chunks just like for uniform memory access systems, except that
	<ol class="answer_list">
	<li> individual memory chunks are directly associated with a
		single CPU (forming a node).
	<li> accessing the local memory on a node is faster than accessing
		remote memory (memory associated with another node).
	<li> local (node) memory is really just a form of cache, with a single
		global memory used to store most values.
	<li> each of the local node memories <strong>must</strong> be of
		different sizes.
	<li> None of the above
	</ol>
</li><br/>
<li> Gustafson's law indicates the
	<ol class="answer_list">
	<li> degree of attainable speedup as the problem size grows (with no
		limit to the number of processors that can be used).
	<li> minimum number of memory references (reads and writes) necessary
		to solve a problem on a set of N nodes.
	<li> minimum number of CPU "instructions" necessary to solve a problem
		on a set of N nodes.
	<li> maximum degree of speedup possible (with no limit to the number
		of processors that can be used) for a fixed sized problem.
	<li> None of the above
	</ol>
</li><br/>
<li> Amdahl's law indicates the
	<ol class="answer_list">
	<li> degree of attainable speedup as the problem size grows (with no
		limit to the number of processors that can be used).
	<li> minimum number of memory references (reads and writes) necessary
		to solve a problem on a set of N nodes.
	<li> minimum number of CPU "instructions" necessary to solve a problem
		on a set of N nodes.
	<li> maximum degree of speedup possible (with no limit to the number
		of processors that can be used) for a fixed sized problem.
	<li> None of the above
	</ol>
</li><br/>

<li> The following are a type of OS for multiprocessor systems:
	<ol class="answer_list">
	<li> OS for each CPU
	<li> Peer-to-peer
	<li> Master-Slave
	<li> Cooperative
	<li> None of the above
	</ol>
</li><br/>
<li> The following are a type of OS for multiprocessor systems:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> Symmetric Multiprocessors
	<li> Peer-to-peer
	<li> None of the above
	</ol>
</li><br/>
<li> The following are a type of OS for multiprocessor systems:
	<ol class="answer_list">
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> These OS types for multiprocessor systems use CPUs that share both the
	same memory and the same I/O devices:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> The system calls on these OS types for multiprocessor systems are handled
	(trap to kernel) on the same CPU that made the system calls: 
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> Which of these OS types for multiprocessor systems is bottlenecked by a
	single CPU? 
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> Which of these OS types for multiprocessor systems is bottlenecked not
	by a single CPU, but by shared data structures within the OS? 
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> These OS types for multiprocessor systems are rarely used today:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> These OS types for multiprocessor systems are the most popularly
	implemented:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> Processes on these OS types for multiprocessor systems are scheduled
	to run <strong>only</strong> on the CPU on which they were created:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> The CPU loads on these OS types for multiprocessor systems are generally
	well balanced:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> These OS types for multiprocessor systems use a separate private memory
	for each CPU:
	<ol class="answer_list">
	<li> Communal
	<li> Master-Slave
	<li> OS for each CPU
	<li> Cooperative
	<li> Peer-to-peer
	<li> Symmetric Multiprocessors
	<li> None of the above
	</ol>
</li><br/>
<li> Disabling interrupts on a CPU for a symmetric multiprocessors OS in order
	to implement process/thread synchronization
	<ol class="answer_list">
	<li> is preferred over using TSL instructions.
	<li> works, but is wasteful of the CPU resource.
	<li> works, but only if caching is <strong>not</strong> used.
	<li> does <strong>not</strong> work since interrupts on the other CPUs
		aren't disabled.
	<li> None of the above
	</ol>
</li><br/>
<li> To safely implement process synchronization on a symmetric multiprocessors
	OS, the minimal implementation must use a
	<ol class="answer_list">
	<li> simple TSL instruction.
	<li> TSL instruction that also locks the system bus.
	<li> TSL instruction that also locks the cache on the issuing CPU.
	<li> TSL instruction that also locks the system bus and the cache
		on the issuing CPU.
	<li> None of the above
	</ol>
</li><br/>
<li> For a symmetric multiprocessor OS, the cache block holding a TSL lock
	variable can bounce between different caches because
	<ol class="answer_list">
	<li> a failed request still writes the TSL lock variable, causing the
		cache block to move, but since other variables in the critical
		section are probably in the same cache block, the lock holder
		will invalidate the block when the variables are written,
		causing it to move back.
	<li> the sleep and wakeup calls of the competing critical section
		threads must alternate.
	<li> the separate OS instances running on each CPU must continually
		reinitialize their cache blocks to keep them synchronized with
		the shared memory.
	<li> the system bus is locked by the TSL instruction, requiring that
		all cache blocks be reinitialized.
	<li> None of the above
	</ol>
</li><br/>
<li> For a symmetric multiprocessor OS, the cache block holding a TSL lock
	variable can bounce between different caches. This can be greatly
	reduced, while maintaining the correctness and safety of the
	critical sections, if
	<ol class="answer_list">
	<li> <strong>all</strong> caches are write-thru and snoop on the
		system bus to determine if a cache block has been invalidated
		by another cache's write-thru.
	<li> the page frame in which the lock variable is located is pinned.
	<li> the TSL instruction doesn't also (temporarily) disable the system
		bus.
	<li> processes/threads attempting to enter a TSL guarded critical
		section check the value of the TSL lock variable first, and
		only issue the TSL instruction if the lock variable value
		indicates it is <strong>not</strong> locked.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary strategies for kernel thread scheduling on a symmetric
	multiprocessor OS are
	<ol class="answer_list">
	<li> Cache sharing.
	<li> Time sharing.
	<li> (System) Bus sharing.
	<li> Gang scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary strategies for kernel thread scheduling on a symmetric
	multiprocessor OS are
	<ol class="answer_list">
	<li> (System) Bus sharing.
	<li> Gang scheduling.
	<li> Non-preemptive scheduling.
	<li> Space sharing.
	<li> None of the above
	</ol>
</li><br/>
<li> The primary strategies for kernel thread scheduling on a symmetric
	multiprocessor OS are
	<ol class="answer_list">
	<li> Cache sharing.
	<li> (System) Bus sharing.
	<li> Non-preemptive scheduling.
	<li> Space sharing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which strategies for kernel thread scheduling on a symmetric
	multiprocessor OS are best for multiple threads that need to
	communicate with one another?
	<ol class="answer_list">
	<li> Cache sharing.
	<li> Time sharing.
	<li> (System) Bus sharing.
	<li> Gang scheduling.
	<li> Non-preemptive scheduling.
	<li> Space sharing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which strategies for kernel thread scheduling on a symmetric
	multiprocessor OS are best for reducing the amount of TLB and
	cache data reloads?
	<ol class="answer_list">
	<li> Cache sharing.
	<li> Time sharing.
	<li> (System) Bus sharing.
	<li> Gang scheduling.
	<li> Non-preemptive scheduling.
	<li> Space sharing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of kernel thread scheduling tries to schedule a thread on
	the same CPU on which it very recently ran?
	<ol class="answer_list">
	<li> Smart scheduling.
	<li> Quasi scheduling.
	<li> Affinity scheduling.
	<li> Non-preemptive scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of kernel thread scheduling will give a thread additional
	time in the CPU if it currently holds a lock?
	<ol class="answer_list">
	<li> Smart scheduling.
	<li> Preemptive scheduling.
	<li> Affinity scheduling.
	<li> Multi-queue scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> The advantages of two-level scheduling are
	<ol class="answer_list">
	<li> roughly balances the load between CPUs on a symmetric
		multiprocessor.
	<li> leverages cache affinity.
	<li> greatly reduces the chance that a thread holding a lock will
		be preempted. 
	<li> reduces contention on the ready list used for scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Which type of scheduling assigns threads to CPUs, but a separate ready
	list and scheduler dispatches threads to the CPU?
	<ol class="answer_list">
	<li> Smart scheduling.
	<li> Two-level scheduling.
	<li> Affinity scheduling.
	<li> Gang scheduling.
	<li> None of the above
	</ol>
</li><br/>
<li> Gang scheduling attempts to combine the benefits of
	<ol class="answer_list">
	<li> Smart scheduling.
	<li> Affinity scheduling.
	<li> Time sharing.
	<li> Space sharing.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are key elements of gang scheduling?
	<ol class="answer_list">
	<li> Thread groups are scheduled together.
	<li> Each thread is scheduled to run on the same CPU that it ran on
		last.
	<li> All members of the thread group run at the same time on different
		CPUs.
	<li> All members of the thread group start and end their time slices
		together.
	<li> None of the above
	</ol>
</li><br/>

<li> Multicomputers differ from multiprocessors in that they
	<ol class="answer_list">
	<li> use loosely coupled CPUs.
	<li> do <strong>not</strong> use a shared memory - each node has
		its own private memory.
	<li> have secondary storage (i.e., local disk) associated with each
		node.
	<li> each node has its own network interface.
	<li> None of the above
	</ol>
</li><br/>
<li> The critical component in a multicomputer is
	<ol class="answer_list">
	<li> the shared memory used by the collection of nodes.
	<li> a hierarchy of cache memories with each higher level of cache
		being (indirectly) shared by a larger set of nodes.
	<li> each node's very fast CPU (much faster than the CPUs of a
		multiprocessor).
	<li> its interconnection network.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are common network topologies? 
	<ol class="answer_list">
	<li> Star
	<li> Ring
	<li> Mesh
	<li> Tree
	<li> Torus
	<li> Hybercube
	<li> None of the above
	</ol>
</li><br/>
<li> For a multicomputer of <em>N</em> nodes, the number of switches in these
	network topologies grows as <em>O(N)</em>:
	<ol class="answer_list">
	<li> Bus
	<li> Star
	<li> Ring
	<li> Mesh
	<li> Tree
	<li> Torus
	<li> None of the above
	</ol>
</li><br/>
<li> For a multicomputer of <em>N</em> nodes, which network topologies have a
	switch count that grows at a less than linear rate?
	<ol class="answer_list">
	<li> Ring
	<li> Mesh
	<li> Tree
	<li> Torus
	<li> None of the above
	</ol>
</li><br/>
<li> For a binary <em>tree</em> network topology, what is the maximum number of
	switches/nodes/hops that a message will need to pass through on its
	way from sender to receiver in a network of <em>N</em> nodes?
	<ol class="answer_list">
	<li> 1
	<li> 2 * log<sub>2</sub>(N)
	<li> sqrt(N)
	<li> N/2
	<li> N
	<li> None of the above
	</ol>
</li><br/>
<li> For a simple <em>mesh</em> network topology (not a torus), what is the
	maximum number of switches/nodes/hops that a message will need to
	pass through on its way from sender to receiver in a network of
	<em>N</em> nodes?
	<ol class="answer_list">
	<li> 1
	<li> 2 * log<sub>2</sub>(N)
	<li> sqrt(N)
	<li> N/2
	<li> N
	<li> None of the above
	</ol>
</li><br/>
<li> For a <em>ring</em> network topology, what is the maximum number of
	switches/nodes/hops that a message will need to pass through on its
	way from sender to receiver in a network of <em>N</em> nodes?
	<ol class="answer_list">
	<li> 1
	<li> 2 * log<sub>2</sub>(N)
	<li> sqrt(N)
	<li> N/2
	<li> N
	<li> None of the above
	</ol>
</li><br/>
<li> For a <em>star</em> network topology, what is the maximum number of
	switches/nodes/hops that a message will need to pass through on its
	way from sender to receiver in a network of <em>N</em> nodes?
	<ol class="answer_list">
	<li> 1
	<li> 2 * log<sub>2</sub>(N)
	<li> sqrt(N)
	<li> N/2
	<li> N
	<li> None of the above
	</ol>
</li><br/>
<li> Most network interfaces have their own RAM (rather than relying on the
	node's RAM) to better support store-and-forward packet switching
	<ol class="answer_list">
	<li> by providing independent operation of the network interface
		in case the node suffers a catastrophic failure.
	<li> so when packets arrive out of order the local RAM is used
		to resort the messages back into the order they were sent.
	<li> to avoid delays caused by contention on the node's local system
		bus.
	<li> by enabling acknowledgement messages to be sent.
	<li> None of the above
	</ol>
</li><br/>
<li> As network interfaces become more capable (with the inclusion of CPUs),
	they can often handle tasks like
	<ol class="answer_list">
	<li> encryption/decryption.
	<li> data compression.
	<li> broadcast/multicast.
	<li> message receipt acknowledgement.
	<li> None of the above
	</ol>
</li><br/>
<li> Internode communication in a multicomputer can be relatively slow
	(compared to the local node's system bus) due to
	<ol class="answer_list">
	<li> having to copy data from applications to the kernel buffer, then
		to the network interface to send a message (reversing the
		process when messages are received).
	<li> the propigation delay of electricity in metal wires (or light
		in fiber optic cables) - even when the nodes are only
		a few feet apart.
	<li> needing to receive an acknowledgement back from the receiver
		before sending the next packet (piece) of a long message.
	<li> the potential for packets to arrive out of order at their
		destination.
	<li> None of the above
	</ol>
</li><br/>

<li> Even if no store-and-forward switching is done (e.g., direct communication
	links), the number of copies that must be created to send data from
	a source (hosting the original version of the data) to a target is
	<ol class="answer_list">
	<li> 2, if the network interface is mapped to the user process' address
		space.
	<li> 3, if the network interface is mapped to the user process' address
		space.
	<li> 4, regardless of whether or not the network interface is mapped
		to the user process' address space.
	<li> 5, if <strong>no</strong> mapping of the network interface
		to the user process' address space is done.
	<li> None of the above
	</ol>
</li><br/>
<li> Multicomputers commonly have 2 network interfaces, one each for the
	OS and one for all of the user processes to share, so that
	<ol class="answer_list">
	<li> user processes sharing one of the interfaces, which is mapped
		to all of their address spaces, cannot prevent the OS from
		using the network interface (e.g., to access remote files).
	<li> the OS can use the network interface to contact other nodes in
		the multicomputer that user processes are <strong>not</strong>
		allowed to access.
	<li> user processes can use the network interface to contact other
		nodes in the multicomputer that the OS is <strong>not</strong>
		allowed to access.
	<li> the bandwidth to other nodes in the multicomputer is kept high
		enough.
	<li> None of the above
	</ol>
</li><br/>
<li> Using DMA to copy data from memory to the network interface (of the
	message sender) requires that the
	<ol class="answer_list">
	<li> CPU poll the network interface to determine when the copy
		is completed.
	<li> message be sent before the DMA notifies the CPU that the
		copy is completed. 
	<li> message be sent <strong>and</strong> acknowledged before
		the DMA notifies the CPU that the copy is completed. 
	<li> page in the page frame be pinned until the copy is completed.
	<li> None of the above
	</ol>
</li><br/>
<li> Using DMA to copy data from the network interface (of the message
	receiver) to memory requires that
	<ol class="answer_list">
	<li> CPU poll the network interface to determine when the copy
		is completed.
	<li> reciept of the message must be acknowledged (to the sender)
		before the DMA notifies the CPU that the copy is completed. 
	<li> page in the page frame be pinned until the copy is completed.
	<li> page frame into which the data is copied <strong>must</strong>
		be mapped to the OS kernel (even when the data is for a
		user process).
	<li> None of the above
	</ol>
</li><br/>
<li> Remote DMA, which allows one computer to write to memory on another
	computer, requires that
	<ol class="answer_list">
	<li> the pages be pinned to their page frames on each of the computers
		until the data copy is completed.
	<li> the page frame on the sending computer be mapped to the OS
		kernel (even when the data is for a user process).
	<li> the page frame on the receiving computer be mapped to the OS
		kernel (even when the data is for a user process).
	<li> a special value be written to a particular location, indicating
		the the data copy is complete.
	<li> None of the above
	</ol>
</li><br/>
<li> The destination address for a message is usually composed of
	<ol class="answer_list">
	<li> the CPU location (i.e., network address).
	<li> the process ID of the receiver.
	<li> the thread ID of the receiver.
	<li> the memory address of the variable in which to store the message.
	<li> None of the above
	</ol>
</li><br/>
<li> The usual case for messaging systems is for the send and receive
	operations to have
	<ol class="answer_list">
	<li> non-blocking semantics for send.
	<li> non-blocking semantics for receive.
	<li> blocking semantics for send.
	<li> blocking semantics for receive.
	<li> None of the above
	</ol>
</li><br/>
<li> Blocking semantics for both send and receive enables
	<ol class="answer_list">
	<li> only synchronous communication.
	<li> only asynchronous communication.
	<li> both synchronous and asynchronous communication.
	<li> neither synchronous and asynchronous communication.
	<li> None of the above
	</ol>
</li><br/>
<li> A receive operation that returns a value indicating whether or not a
	message was available/read is an example of a
	<ol class="answer_list">
	<li> blocking operation.
	<li> non-blocking operation.
	<li> None of the above
	</ol>
</li><br/>
<li> Advantage(s) of a non-blocking send operation include
	<ol class="answer_list">
	<li> reducing the number of times the message must be copied (e.g.,
		into various buffers).
	<li> enabling the message to be sent (and thus received) faster.
	<li> not forcing the sending process to wait if many other processes
		are also sending messages at nearly the same time.
	<li> simplifying the implementation of the send operation.
	<li> None of the above
	</ol>
</li><br/>
<li> Disadvantage(s) of a non-blocking send operation include
	<ol class="answer_list">
	<li> requiring the corresponding receive operation to also be
		non-blocking.
	<li> increasing the time it takes for the message to be sent (and
		thus received).
	<li> forcing the sending process to wait if many other processes are
		also sending messages at nearly the same time.
	<li> the implementation of the send operation becomes more complex
		since the send must be completed before its message buffer			can (safely) be reused.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to the receive operation for messaging, from the perspective
	of the user process, a
	<ol class="answer_list">
	<li> blocking receive can be used to implement a non-blocking receive.
	<li> non-blocking receive can be used to implement a blocking receive.
	<li> None of the above
	</ol>
</li><br/>
<li> With respect to the send operation for messaging, from the perspective
	of the user process, a
	<ol class="answer_list">
	<li> blocking send can be used to implement a non-blocking send.
	<li> non-blocking send can be used to implement a blocking send.
	<li> None of the above
	</ol>
</li><br/>
<li> A remote procedure call (RPC) implements
	<ol class="answer_list">
	<li> synchronous communication.
	<li> asynchronous communication.
	<li> provides semantics equivalent to blocking send/receive messaging.
	<li> provides semantics equivalent to non-blocking send/receive messaging.
	<li> None of the above
	</ol>
</li><br/>
<li> Match the actions of each numbered arrow with its corresponding
	description for a remote procedure call (RPC).
	<br/>
	<img src="/eckart/classes/cpsc3125/topics/content/RPCcall.png" width="400" height="300" alt="Remote Procedure Call" />
	<table><tr><td>
		<ol class="match_list">
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		<li> __________________
		</ol>
	</td><td>
		<ol class="answer_list">
		<li> Client procedure calls the procedure stub, which
			marshals the arguments.
		<li> The stub uses OS system calls to send the message.
		<li> The kernel sends the message to the remote system.
		<li> A server stub on the remote machine unmarshals the
			procedure call arguments.
		<li> The server stub executes a local procedure call.
		<li> The (server) procedure completes, returning execution
			to the server stub.
		<li> The server stub marshals the return values into a
			return message.
		<li> The remote OS sends the return message to the
			originating system.
		<li> The client stub reads the data from the return message.
		<li> The message data is unmarshalled and the return
			values are placed on the stack for the local process.
		</ol>
	</td></tr></table>
</li><br/>
<li> Passing pointers as parameters to a remote procedure call (RPC)
	<ol class="answer_list">
	<li> is problematic since the remote implementation won't have direct
		access to the referenced memory.
	<li> can be used without difficulty so long as it is not a dangling
		reference.
	<li> can be used without difficulty, except that the remote procedure
		cannot free the storage it points to.
	<li> works exactly like a local procedure call with no additional
		limitations or concerns.
	<li> None of the above
	</ol>
</li><br/>
<li> In C, passing arrays as parameters to a remote procedure call (RPC)
	<ol class="answer_list">
	<li> works exactly like a local procedure call with no additional
		limitations or concerns.
	<li> works as expected so long as a primitive type is used for the
		elements of the array.
	<li> is problematic because arrays are really pointers in C, and
		the remote procedure implementation won't be able to access
		the referenced memory.
	<li> is problematic since arrays in C don't have an associated size,
		thus the array size would have to be passed as a separate
		argument and the client stub would have to know to use that
		value for (un)marshalling the array.
	<li> None of the above
	</ol>
</li><br/>
<li> In C, the types of parameters to a remote procedure call (RPC)
	<ol class="answer_list">
	<li> can <strong>always</strong> be determined by the compiler, thus
		the means for (un)marshalling them is always known.
	<li> are limited to only primitive types.
	<li> are sometimes determined by other parameters (e.g., the format
		string for "printf"), making it difficult to properly
		(un)marshal them without implementing some of the semantics
		of the remote procedure within the client stub.
	<li> must <strong>always</strong> be cast (even if simply to the
		same type they already are) so that the client stub will know
		how to (un)marshal them.
	<li> None of the above
	</ol>
</li><br/>
<li> Using global variables in the implementation of a procedure
	<ol class="answer_list">
	<li> greatly increases its run-time efficiency, especially when
		exposed as a remote procedure call (RPC).
	<li> makes it difficult to expose the procedure via RPC because
		this makes them stateful.
	<li> makes it easier to expose the procedure via RPC since it
		reduces the number of arguments that must be (un)marshalled.
	<li> often causes unexcepted behavior if more than one
		process/thread calls the procedure at a time.
	<li> None of the above
	</ol>
</li><br/>
<li> The basic concept behind distributed shared memory is to
	<ol class="answer_list">
	<li> use a multiport disc (either HDD or SSD) so that more than
		one computer can access the virtual address space of a
		given process.
	<li> allow virtual address space pages to be loaded into page frames
		of remote computers.
	<li> use remote procedure calls (RPC) to read the memory contents
		of a process running on a remote CPU.
	<li> broadcast the contents of a virtual address page to all computers
		on the network whenever a request for access is made, enabling
		all the associated caches to be updated. 
	<li> None of the above
	</ol>
</li><br/>
<li> False sharing can happen when two processes running on different
	computers, using distributed shared memory, are writing to
	different variables
	<ol class="answer_list">
	<li> that happen to have the same address in their virtual address
		space. While not actually shared, this creates a collision
		in the TLB, causing the problem.
	<li> that share the exact same name in their respective source code,
		even though the variables are actually in different processes.
	<li> which are in the same virtual address space page, causing the
		page to bounce back and forth between the two computers as
		the writes occur. 
	<li> which are in the same cache block on a third computer, preventing
		their being accessed by either of the two processes.
	<li> None of the above
	</ol>
</li><br/>
<li> When using shared distributed memory, the replication of pages
	<ol class="answer_list">
	<li> is prohibited since this <strong>always</strong> creates a race
		condition for variable access.
	<li> can improve performance if the pages are read-only.
	<li> requires that <strong>all</strong> replicates be invalidated if
		a write to any of the page copies ever occurs.
	<li> can occur without restriction so long as all modifications to
		pages use write-thru semantics.
	<li> None of the above
	</ol>
</li><br/>
<li> Process scheduling on a multicomputer consists of a
	<ol class="answer_list">
	<li> single scheduler that exists on the master, with all other
		computers requesting the next process to run from the master.
	<li> low-level scheduler that performs dispatch of its assigned
		processes.
	<li> high-level scheduler that assigns processes to each computer.
	<li> single scheduler that migrates across each of the individual
		computers. This reduces contention on the shared data within
		the OS kernel and prevents one computer from being a single
		point of scheduling failure. 
	<li> None of the above
	</ol>
</li><br/>
<li> When the overloaded computer, of a multicomputer, probes other computers
	to find one that is less loaded to which a newly created process can
	migrate, this is called the
	<ol class="answer_list">
	<li> peer-initiated distributed heuristic.
	<li> sender-initiated distributed heuristic.
	<li> receiver-initiated distributed heuristic.
	<li> slave-initiated distributed heuristic.
	<li> None of the above
	</ol>
</li><br/>
<li> When a lightly loaded computer, of a multicomputer, probes other computers
	to find one that is more loaded from which a newly created process can
	migrate, this is called the
	<ol class="answer_list">
	<li> peer-initiated distributed heuristic.
	<li> sender-initiated distributed heuristic.
	<li> receiver-initiated distributed heuristic.
	<li> slave-initiated distributed heuristic.
	<li> None of the above
	</ol>
</li><br/>

<li> Loosely coupled multicomputers with their own peripherals and running
	their own (possibly different) OS at each node are called
	<ol class="answer_list">
	<li> clusters of workstations (COWs).
	<li> symmetric multiprocessors (SMPs).
	<li> distributed systems.
	<li> transcendtal computer systems (TCS).
	<li> None of the above
	</ol>
</li><br/>
<li> As a by-product of the interconnection network, which of the following
	sytstems is typically confined to a single room?
	<ol class="answer_list">
	<li> multiprocessor
	<li> multicomputer
	<li> distributed system
	<li> clusters of workstations (COWs).
	<li> None of the above
	</ol>
</li><br/>
<li> As a by-product of the interconnection network, which of the following
	sytstems can reasonable span the entire Earth?
	<ol class="answer_list">
	<li> multiprocessor
	<li> multicomputer
	<li> distributed system
	<li> clusters of workstations (COWs).
	<li> None of the above
	</ol>
</li><br/>
<li> The primary advantage(s) of a dynamically switched packet based network
	is (are) that
	<ol class="answer_list">
	<li> packets can be rerouted around network congestion.
	<li> there is lower communication latency than a circuit based network
		(e.g., Plain Old Telephone Service - POTS).
	<li> the maximum bandwidith between two nodes is higher than that of
		a circuit based network.
	<li> packets can be rerouted around failed communication nodes.
	<li> None of the above
	</ol>
</li><br/>
<li> The Ethernet protocols rely on the ability
	<ol class="answer_list">
	<li> to detect when two or more computers are trying to use the same
		network segment at the same time (i.e., collision detection).
	<li> for a computer to lock the network segment it wants to communicate
		on so that no other computer can use it, thus avoiding
		communication collisions.
	<li> that if a communication collision is sensed, to wait increasingly
		longer periods of time before retrying.
	<li> to create a circuit between two computers to ensure fast and
		uninterrupted communications.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Binary exponential backoff</em> is a strategy used when two
	(or more) computers are trying to share the same resource
	(e.g., a network) at the same time in which
	<ol class="answer_list">
	<li> the computer with the lower Internet Protocol (IP) address is
		given priority and the one with the larger IP address waits
		until no computer with a lower IP address is trying to use
		the resource.
	<li> the computer with the higher total number of processes is
		given priority and the one with the lower number of processes
		waits until no computer with a higher number of processes is
		trying to use the resource.
	<li> the computers stop trying to use the resource, wait a random
		period of time (between 0 and T microseconds), then retry
		using the resource. If there's still a collision, the
		computers double the wait time (2 to 2T microseconds) before
		trying again, repeating until a maximum wait time is
		reached.
	<li> None of the above
	</ol>
</li><br/>
<li> Ethernet networks were originally designed as a type of bus, and thus
	are best suited to be used as networks
	<ol class="answer_list">
	<li> spanning a large number of machines (> 1000) over a
		large geographical area (e.g., country).
	<li> spanning a large number of machines (> 1000) but over a
		small geographical area (e.g., a single building).
	<li> spanning a small number of machines (10 - 100) over a
		large geographical area (e.g., country).
	<li> spanning a small number of machines (10 - 100) but over a
		small geographical area (e.g., a single building).
	<li> None of the above
	</ol>
</li><br/>
<li> The primary difference between Wide Area Networks (WANs) and
	Local Area Networks (LANs) is the
	<ol class="answer_list">
	<li> amount of network bandwidth they are designed to support.
	<li> degree of communication latency they must endure.
	<li> longest communication distance between nodes that
		they are designed to support.
	<li> maximum number of nodes they can connect.
	<li> None of the above
	</ol>
</li><br/>
<li> The Internet is an example of a
	<ol class="answer_list">
	<li> Local Area Network (LAN).
	<li> Wide Area Network (WAN).
	<li> POTS network.
	<li> circuit switch network.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following provides a logical pipe that enables two computers
	to communicate with one another in which the messages are received in
	the order they were sent?
	<ol class="answer_list">
	<li> connection-oriented services
	<li> connectionless services
	<li> user datagram protocol (UDP)
	<li> transmission control protocol (TCP)
	<li> Internet protocol (IP)
	<li> reliable user datagram protocol (UDP)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following sends each message packet out as separate items,
	requiring each packet contain a destination address?
	<ol class="answer_list">
	<li> connection-oriented services
	<li> connectionless services
	<li> user datagram protocol (UDP)
	<li> transmission control protocol (TCP)
	<li> Internet protocol (IP)
	<li> reliable user datagram protocol (UDP)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following provides reliable delivery of message packets?
	<ol class="answer_list">
	<li> connection-oriented services
	<li> connectionless services
	<li> user datagram protocol (UDP)
	<li> transmission control protocol (TCP)
	<li> Internet protocol (IP)
	<li> reliable user datagram protocol (UDP)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following send acknowledgement packets as a way to provide
	their advertised quality of service?
	<ol class="answer_list">
	<li> connection-oriented services
	<li> connectionless services
	<li> user datagram protocol (UDP)
	<li> transmission control protocol (TCP)
	<li> Internet protocol (IP)
	<li> reliable user datagram protocol (UDP)
	<li> None of the above
	</ol>
</li><br/>
<li> What service is responsible for translating a web address like
	"columbusstate.edu" into its corresponding Internet Protocol
	numeric address?
	<ol class="answer_list">
	<li> connection-oriented services.
	<li> connectionless services.
	<li> domain name service.
	<li> network name translation service.
	<li> None of the above
	</ol>
</li><br/>
<li> Which protocol(s) is best suited for a quality of service that is
	tolerant of some missing packets (e.g., streaming music service).
	<ol class="answer_list">
	<li> user datagram protocol (UDP)
	<li> transmission control protocol (TCP)
	<li> Internet protocol (IP)
	<li> reliable user datagram protocol (UDP)
	<li> None of the above
	</ol>
</li><br/>

<li> Middleware is that software which
	<ol class="answer_list">
	<li> implements the abstract software layer that provides uniformity
		of use and operation despite hardware and OS differences.
	<li> sits between the hypervisor and the virtual machine's OS.
	<li> provides the abstract machine on which a type 1 hypervisor runs.
	<li> provides the abstract machine on which a type 2 hypervisor runs.
	<li> None of the above
	</ol>
</li><br/>
<li> The original implementation of the World Wide Web is an example of this
	middleware approach.
	<ol class="answer_list">
	<li> Document-based middleware
	<li> File-System-based middleware
	<li> Object-based middleware
	<li> Coordination-based middleware
	<li> None of the above
	</ol>
</li><br/>
<li> Middleware that transfers data from a remote file server to the local
	system, then operates on the data locally before sending it back for
	storage on the host is an example of
	<ol class="answer_list">
	<li> Document-based middleware
	<li> File-System-based middleware
	<li> Object-based middleware
	<li> Coordination-based middleware
	<li> None of the above
	</ol>
</li><br/>
<li> When data is collected together into units, is stored on remote servers,
	and accessed via method calls, this is an example of
	<ol class="answer_list">
	<li> Document-based middleware
	<li> File-System-based middleware
	<li> Object-based middleware
	<li> Coordination-based middleware
	<li> None of the above
	</ol>
</li><br/>
<li> Tuple spaces and publish/subscribe systems are examples of
	<ol class="answer_list">
	<li> Document-based middleware
	<li> File-System-based middleware
	<li> Object-based middleware
	<li> Coordination-based middleware
	<li> None of the above
	</ol>
</li><br/>
<li> Linda tuple spaces use a share list of time ordered published data
	that is accessed via the operations
	<ol class="answer_list">
	<li> "in" which returns the first matching tuple, blocking until a
		matching value is found.
	<li> "read" which returns the first matching tuple, returning no
		match if no tuple is found (i.e., non-blocking).
	<li> "out" that publishes a new tuple to the shared list.
	<li> "delete" which removes the matching tuple from the shared list.
	<li> None of the above
	</ol>
</li><br/>
<li> The publish/subscribe model requires that consumers (subscribers)
	<ol class="answer_list">
	<li> cannot also be publishers. 
	<li> register their interest in topics (about which data may be
		published) with a centralized authority as one way to ensure
		data receipt.
	<li> directly acknowledge receipt (to the publisher) of all the data
		that they receive, regardless of whether this was a topic
		in which the subscriber was interested.
	<li> poll publishers periodically to get data that is available on
		topics that the subscriber has indicated an interest.
	<li> None of the above
	</ol>
</li><br/>
<li> The publish/subscribe model requires that publishers
	<ol class="answer_list">
	<li> must also be subscribers themselves.
	<li> make all published data for a topic available to subscribers
		that have registered an interest in that topic.
	<li> broadcast the existence of all new published data (though not
		the actual data itself) to all
		subscribers (even if the data doesn't belong to a topic in
		which the subscriber has registered an interest)
	<li> indicate what topic published data belongs to.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are examples of object-based middleware?
	<ol class="answer_list">
	<li> Simple Object Access Protocol (SOAP).
	<li> File Transfer Protocol (ftp).
	<li> Common Object Request Broker Architecture (CORBA).
	<li> Representational State Transfer (REST).
	<li> None of the above
	</ol>
</li><br/>
<li> The Common Object Request Broker Architecture (CORBA) uses which of
	the following elements to enable data access?
	<ol class="answer_list">
	<li> Interface Definition Language (IDL) is used to describe the
		object access methods and parameter types.
	<li> Object Request Brokers (ORBs) which enable clients to contact
		servers (on which objects reside).
	<li> Object adapters which wrap non-CORBA objects so that they can
		be used within the CORBA framework.
	<li> Native Language Translation (NLT) which convert the programming
		language independent IDL descriptions to those of specific
		languages (e.g., Java).
	<li> None of the above
	</ol>
</li><br/>
<li> The transfer models most commonly used for File-System-based middleware
	<ol class="answer_list">
	<li> send edit commands to the remote server so that the file
		contents never leave the server.
	<li> edit parts of the file without downloading the entire file (e.g.,
		using NFS).
	<li> download the entire file, alter it, then upload the file (e.g.,
		using ftp)
	<li> send the message digest of the file to the client, rather than
		the file itself. Edits on the message digest are then sent
		back and reverse applied to the original file on the server.
	<li> None of the above
	</ol>
</li><br/>
<li> The principle of <em>location transparency</em> for File-System-based
	middleware
	<ol class="answer_list">
	<li> automatically converts encoding systems as files are moved
		between systems (e.g., EBCIDIC vs ASCII).
	<li> uses a system independent encoding for files independent of the
		file system on which they are stored.
	<li> tags each file with its current physical location.
	<li> gives no indication of where the file is physically located.
	<li> None of the above
	</ol>
</li><br/>
<li> The principle of <em>location independence</em> for File-System-based
	middleware
	<ol class="answer_list">
	<li> uses a system independent encoding for files independent of the
		file system on which they are stored.
	<li> means that the path name of the file does not change when
		the file's physical location changes.
	<li> gives no indication of where the file is physically located.
	<li> enables files to be accessible regardless of their physical
		location.
	<li> None of the above
	</ol>
</li><br/>
<li> Using the original World Wide Web as an example of Document-based
	middleware
	<ol class="answer_list">
	<li> the Uniform Resource Locator (URL) identifies the document to
		be retrieved.
	<li> file locations are indicated separately from the URL.
	<li> only entire documents are delivered.
	<li> web browsers are the only way of accessing remote documents.
	<li> None of the above
	</ol>
</li><br/>
<li> Sequential consistency for File-System-based middleware
	<ol class="answer_list">
	<li> requires that files be locked by a single process thread when
		they are being edited, so that simultaneous changes are
		<strong>not</strong> possible.
	<li> ensures that actions on shared resources appear as if they are
		performed in order (even if they aren't).
	<li> enforces sequential access for a file so that changes cannot
		be made in random locations.
	<li> means that changes to an open file are initially visible only
		to the process that made the changes. Other processes see
		the changes only after the file is closed.
	<li> None of the above
	</ol>
</li><br/>
<li> Session semantics for File-System-based middleware
	<ol class="answer_list">
	<li> ensures that actions on shared resources appear as if they are
		performed in order (even if they aren't).
	<li> requires that files be locked by a single process thread when
		they are being edited, so that simultaneous changes are
		<strong>not</strong> possible.
	<li> waits until a process terminates before making the edits to a
		file by that process permanent. 
	<li> means that changes to an open file are initially visible only
		to the process that made the changes. Other processes see
		the changes only after the file is closed.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following are reasons why an organization might use many
	computers rather than a single large computer system (running a single
	OS)?
	<ol class="answer_list">
	<li> Different operating systems need to be used.
	<li> Prevent problems with applications from impacting other
		(otherwise unrelated) applications.
	<li> Using different computer systems for software development and
		testing.
	<li> Using a separate computer system for testing newly acquired
		(untrusted) software.
	<li> With multiple systems, having one compromised by an attacker
		does <strong>not</strong> impact applications running on
		the other (UNcompromised) systems.
	<li> None of the above
	</ol>
</li><br/>
<li> Some of the problems associated with an organization having and
	maintaining a large number of separate smaller computers, as opposed
	to a single large computer, are that they are
	<ol class="answer_list">
	<li> upgraded incrementally, so that not all of the computers may
		be the same.
	<li> time consuming to administer.
	<li> more likely to deadlock.
	<li> wasteful of compute resources when machines are sitting idle
		(while others could be overloaded).
	<li> None of the above
	</ol>
</li><br/>
<li> A hypervisor enables a single physical computer to
	<ol class="answer_list">
	<li> do more computations per second than is possible without it.
	<li> appear as if it is many computers.
	<li> run serveral different operating systems at the same time.
	<li> use less electricity.
	<li> None of the above
	</ol>
</li><br/>
<li> Advantages of running multiple virtual machines on a hypervisor over
	separate physical computers include 
	<ol class="answer_list">
	<li> using less electricity.
	<li> generating less heat.
	<li> taking up less space.
	<li> requiring less effort to administer.
	<li> None of the above
	</ol>
</li><br/>
<li> Advantages of running multiple virtual machines on a hypervisor over
	separate physical computers include 
	<ol class="answer_list">
	<li> reducing the overall system cost since some elements of the
		separate computer systems will not be fully replicated.
	<li> easy migration of an application from one hypervisor to another.
	<li> reducing the wasted CPU cycles of unbalanced utilization
	<li> the ability to run legacy software that requires older versions
		of the OS not available on the newer hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> The disadvantages of running virtual machines (within a hypervisor)
	on a single physical machine (over using separate physical
	machines) include
	<ol class="answer_list">
	<li> having to run the same OS on each virtual machine (VM).
	<li> a reduced ability to survive failures (e.g., a single power
		supply failure now impacts many operating systems and their
		applications).
	<li> requiring all the VMs to use the same file system.
	<li> a larger degree of wasted CPU resources when all of the VMs
		are idle.
	<li> None of the above
	</ol>
</li><br/>
<li> The first hypervisors were developed around
	<ol class="answer_list">
	<li> 1965 to 1968.
	<li> 1974.
	<li> 1999.
	<li> 2005.
	<li> None of the above
	</ol>
</li><br/>
<li> When a hypervisor is able to provide virtual machines that are
	indistinguishable from the underlying hardware, this is called
	<ol class="answer_list">
	<li> pseudo virtualization.
	<li> quasi virtualization.
	<li> paravirtualization.
	<li> full virtualization.
	<li> None of the above
	</ol>
</li><br/>
<li> When a hypervisor requires that the OS running on a virtual machine
	use hypercalls to access certain capabilities, this is called 
	<ol class="answer_list">
	<li> pseudo virtualization.
	<li> quasi virtualization.
	<li> paravirtualization.
	<li> full virtualization.
	<li> None of the above
	</ol>
</li><br/>
<li> The computer architecture hardware requirements for virtualization
	include
	<ol class="answer_list">
	<li> safety.
	<li> reliability.
	<li> orthoganality.
	<li> fidelity.
	<li> efficiency.
	<li> None of the above
	</ol>
</li><br/>
<li> To ensure that all virtualized resources are fully controlled by the
	hypervisor, it is
	<ol class="answer_list">
	<li> necessary that <strong>all</strong> instructions issued
		to the VM be interpreted.
	<li> possible that some (but likely not all) machine instructions
		can be executed directly (without interpretation).
	<li> <strong>never</strong> necessary to use interpretation -
		<strong>all</strong> instructions can be directly executed
		on the hardware.
	<li> None of the above
	</ol>
</li><br/>
<li> Instructions that behave differently depending upon whether or not
	they are executed in kernel mode or user mode, are called
	<ol class="answer_list">
	<li> sensitive instructions.
	<li> virtual instructions.
	<li> permissive instructions.
	<li> priviledged instructions.
	<li> None of the above
	</ol>
</li><br/>
<li> Instructions that cause a trap if executed in user mode, are called
	<ol class="answer_list">
	<li> sensitive instructions.
	<li> virtual instructions.
	<li> permissive instructions.
	<li> priviledged instructions.
	<li> None of the above
	</ol>
</li><br/>
<li> A computer architecture is virtualizable only if the set of
	<ol class="answer_list">
	<li> priviledged instructions is identical to the set of
		sensitive instructions.
	<li> priviledged instructions is a subset of the sensitive
		instructions.
	<li> sensitive instructions is a subset of the priviledged
		instructions.
	<li> priviledged instructions and the set of sensitive instructions
		do not overlap (i.e., no common members).
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following is true of a type 1 hypervisor?
	<ol class="answer_list">
	<li> Runs directly on the physical hardware.
	<li> Runs within the host operating system.
	<li> More CPU cycles are used for running the VMs, their corresponding
		OS, and associated user processes than is the case for a
		type 2 hypervisor.
	<li> Relies on the underlying OS to help provide some services.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is true of a type 1 hypervisor?
	<ol class="answer_list">
	<li> Also called a hosted hypervisor since it runs on a
		host operating system.
	<li> To support complete copies of the hardware, it must also provide
		many OS services (at least for itself).
	<li> Is less efficient than a type 2 hypervisor since there is an
		extra implementation layer (the host OS).
	<li> Relies on the underlying OS to help provide some services.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is true of a type 2 hypervisor?
	<ol class="answer_list">
	<li> Runs directly on the physical hardware.
	<li> Runs within the host operating system.
	<li> More CPU cycles are used for running the VMs, their corresponding
		OS, and associated user processes than is the case for a
		type 2 hypervisor.
	<li> Relies on the underlying OS to help provide some services.
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is true of a type 2 hypervisor?
	<ol class="answer_list">
	<li> Also called a hosted hypervisor since it runs on a
		host operating system.
	<li> To support complete copies of the hardware, it must also provide
		many OS services (at least for itself).
	<li> Is less efficient than a type 1 hypervisor since there is an
		extra implementation layer (the host OS).
	<li> Relies on the underlying OS to help provide some services.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Guest Operating System</em> is that (another) name used to
	refer to the
	<ol class="answer_list">
	<li> underlying OS in which a type 2 hypervisor runs.
	<li> type 1 hypervisor which is running directly on the hardware.
	<li> virtual machine that is running within a hypervisor.
	<li> OS which is running within a virtual machine.
	<li> None of the above
	</ol>
</li><br/>
<li> Both type 1 and type 2 hypervisors must execute
	<strong>all</strong> the instructions on their virtual machines
	<ol class="answer_list">
	<li> via interpretation.
	<li> using a trap-and-emulate approach.
	<li> in a <em>safe</em> manner.
	<li> using binary translation.
	<li> None of the above
	</ol>
</li><br/>

<li> For a type 1 hypervisor, which of the following would be running in
	kernel mode?
	<ol class="answer_list">
	<li> User process
	<li> Guest OS
	<li> Virtual Machine (VM)
	<li> Type 1 Hypervisor
	<li> None of the above
	</ol>
</li><br/>
<li> For a type 1 hypervisor, which of the following would be running in
	user mode within the hypervisor?
	<ol class="answer_list">
	<li> User process
	<li> Guest OS
	<li> Virtual Machine (VM)
	<li> Type 1 Hypervisor
	<li> None of the above
	</ol>
</li><br/>
<li> For a type 1 hypervisor, which of the following would be running in
	virtual kernel mode within the virtual machine?
	<ol class="answer_list">
	<li> User process
	<li> Guest OS
	<li> Virtual Machine (VM)
	<li> Type 1 Hypervisor
	<li> None of the above
	</ol>
</li><br/>
<li> For a type 1 hypervisor, which of the following would be running in
	virtual user mode within the virtual machine?
	<ol class="answer_list">
	<li> User process
	<li> Guest OS
	<li> Virtual Machine (VM)
	<li> Type 1 Hypervisor
	<li> None of the above
	</ol>
</li><br/>
<li> If a VM on a type 1 hypervisor receives a sensitive instruction
	(from a user process) to execute, and the CPU supports virtualization
	technology, then
	<ol class="answer_list">
	<li> the instruction is carried out by the hypervisor on behalf
		of the guest OS.
	<li> the hypervisor emulates what the hardware would do if the
		sensitive (and priveleged) instruction was executed on the
		hardware in user mode.
	<li> the instruction is ignored (i.e., treated as a NOP - No Operation).
	<li> the instruction fails and the calling process crashes.
	<li> None of the above
	</ol>
</li><br/>
<li> If a VM on a type 1 hypervisor receives a sensitive instruction
	(from the guest OS) to execute, and the CPU supports virtualization
	technology, then
	<ol class="answer_list">
	<li> the instruction is carried out by the hypervisor on behalf
		of the guest OS.
	<li> the hypervisor emulates what the hardware would do if the
		sensitive (and priveleged) instruction was executed on the
		hardware in user mode.
	<li> the instruction is ignored (i.e., treated as a NOP - No Operation).
	<li> the instruction fails and the calling process crashes.
	<li> None of the above
	</ol>
</li><br/>
<li> If a VM on a type 1 hypervisor receives a sensitive instruction
	to execute, and the CPU does <strong>not</strong> support
	virtualization technology, then
	<ol class="answer_list">
	<li> the instruction is carried out by the hypervisor on behalf
		of the guest OS.
	<li> the hypervisor emulates what the hardware would do if the
		sensitive (and priveleged) instruction was executed on the
		hardware in user mode.
	<li> the instruction is ignored (i.e., treated as a NOP - No Operation).
	<li> the instruction fails and the calling process crashes.
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>basic block</em> is
	<ol class="answer_list">
	<li> the body of a function, procedure, or method.
	<li> a block of code that always starts at the top/beginning, but
		can have multiple branches (i.e., JMP) so long as they
		all jump to a label that is the last instruction in the block.
		instruction.
	<li> a straight-line code that ends with a brank (i.e., JMP)
	<li> any structured programming element that has a single point of
		entry and a single point of exit.
	<li> None of the above
	</ol>
</li><br/>
<li> Before virtualization technology was available on the x86 CPU
	architecture, its 4 rings of protected execution modes were used
	by type 1 hypervisors
	<ol class="answer_list">
	<li> to separate the hypervisor, guest OS, and user processes
		from one another based on priviledge by running them in
		rings 0, 1, and 3 respectively.
	<li> for preventing virtual machines from interfering with one
		another, with each guest OS running in a different ring
		(so the hypervisor was limited to a maximum of 4 VMs).
	<li> to ensure secure communications between the hypervisor and
		VMs (running in ring 0), preventing the user processes
		(running in ring 1) from "ease dropping".
	<li> None of the above
	</ol>
</li><br/>
<li> To perform binary translation, a hypervisor examines each basic block
	and
	<ol class="answer_list">
	<li> interprets <strong>all</strong> of the instructions in the
		block before examining the next basic block.
	<li> if it contains any sensitive instructions, replaces them with
		calls to the corresponding hypervisor routines.
	<li> replaces the branch instruction(s) with a call into the
		hypervisor (so that it can examine and rewrite the next
		basic block).
	<li> caches any translated code to avoid having to redo the
		translation.
	<li> None of the above
	</ol>
</li><br/>
<li> Using modern virtualization technology (VT) hardware
	<ol class="answer_list">
	<li> <strong>always</strong> yields faster execution times than
		binary translation.
	<li> can sometimes be slower (in the long-term) than binary
		translation.
	<li> creates additional overhead since the generated traps will
		tend to crowd out needed entries in the TLB and caches.
	<li> enables hypervisor functionality that are not possible with
		the binary translation approach.
	<li> None of the above
	</ol>
</li><br/>

<li> The guest OS on each VM creates its own page tables, but without further
	management by the hypervisor this would likely result in the disastrous
	case of
	<ol class="answer_list">
	<li> each guest OS eventually mapping different physical page frames
		to the same virtual address space page.
	<li> each guest OS eventually mapping different virtual address space
		pages to the same physical page frame.
	<li> the guest operating systems being deadlocked waiting for the same
		physical page frame.
	<li> all of the VMs being deadlocked waiting for the same
		virtual address space page to load.
	<li> None of the above
	</ol>
</li><br/>
<li> The hypervisor maintains a shadow page table which is the mapping of
	the virtual page frames (holding guest physical addresses) to
	the physical page frames (holding host physical addresses) for
	each guest OS, so that
	<ol class="answer_list">
	<li> the physical page frames can be used as shared memory segments
		between different VMs.
	<li> the virtual address spaces on all VMs can be directly accessed
		and shared across VMs.
	<li> for each guest OS, the virtual set of memory page frames is
		indistinguishable from physical memory. 
	<li> the hypervisor can provide improved process scheduling for the
		guest OS running on each VM.
	<li> None of the above
	</ol>
</li><br/>
<li> Instead of allowing each guest OS to map virtual pages to physical page
	frames, a type 1 hypervisor
	<ol class="answer_list">
	<li> allocates a fixed set of physical page frames to each VM that
		its guest OS is allowed to use.
	<li> directly handles address translation and all page faults on
		behalf of the guest OS.
	<li> provides and manages the virtual address space for the processes
		running on each VM, avoiding the need for virtual pages.
	<li> provides a virtual set of memory page frames that are in
		turn mapped to the physical memory.
	<li> None of the above
	</ol>
</li><br/>
<li> Page faults caused by the guest OS (or processes running within the
	guest OS) are called
	<ol class="answer_list">
	<li> hypervisor-induced page faults.
	<li> VM-induced page faults.
	<li> guest-induced page faults.
	<li> user-induced page faults.
	<li> None of the above
	</ol>
</li><br/>
<li> Page faults that happen to keep the physical memory page frames and
	the shadow page table in sync are called
	<ol class="answer_list">
	<li> hypervisor-induced page faults.
	<li> VM-induced page faults.
	<li> guest-induced page faults.
	<li> user-induced page faults.
	<li> None of the above
	</ol>
</li><br/>
<li> Situations where the hypervisor takes control, really just a full
	process context switch, are called a
	<ol class="answer_list">
	<li> guest OS swap.
	<li> VM exit.
	<li> hypervisor-induced page fault.
	<li> hypervisor swap.
	<li> None of the above
	</ol>
</li><br/>
<li> Minimizing page faults is an important issue for hypervisors since
	every guest OS page fault results in a
	<ol class="answer_list">
	<li> VM exit.
	<li> hypervisor-induced page fault.
	<li> guest OS swap.
	<li> purge of the entire memory cache. 
	<li> None of the above
	</ol>
</li><br/>
<li> Since writing to memory is <strong>not</strong> a
	sensitive instruction, changes to a page table by the guest OS will
	<strong>not</strong> create a trap and thus
	<ol class="answer_list">
	<li> the hypervisor will <strong>not</strong> be aware of
		the change via the mechanism used for CPU virtualization
		(enabling it to update the shadow page table).
	<li> the VM <strong>must</strong> perform all virtual
		address translations in software so that it can determine
		whether or not a write is to a page table.
	<li> the hypervisor <strong>must</strong> use binary
		translation to replace <strong>all</strong>
		memory writing instructions with a call to the hypervisor
		(so that it can check for writes to page tables).
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are the basis for techniques that enable a
	hypervisor to be aware of page table changes made by a guest OS?
	<ol class="answer_list">
	<li> The hypervisor uses binary translation to replace
		<strong>all</strong> memory writing instructions with a
		call to the hypervisor (so that it can check for writes
		to page tables).
	<li> The hypervisor marks each page table of the guest OS as
		being read-only so that writing to the table causes a fault
		(alerting the hypervisor).
	<li> When page faults occur for guest OS mapped pages, the hypervisor
		takes control and examines the page table on the guest OS.
	<li> Every time a <em>VM exit</em> occurs, the hypervisor
		examines the page tables on each guest OS for changes.
	<li> None of the above
	</ol>
</li><br/>
<li> Loading the location of the top-level page table by the guest OS
	into its special hardware register is a sensitive instruction,
	trapping to the hypervisor. This enables
	<ol class="answer_list">
	<li> the hypervisor to directly perform all virtual address	
		translations in software.
	<li> a clumsy, though workable, mechanism for updating the
		hypervisor's shadow page table.
	<li> each guest OS to signal the hypervisor when it is about to
		modify one of its page tables.
	<li> the hypervisor to mark each guest OS page table as read only,
		causing faults when the page table is changed, allowing 
		the hypervisor to update its shadow page table.
	<li> None of the above
	</ol>
</li><br/>
<li> Relying on page faults and TLB entry invalidations (a sensitive
	instruction) as opportunities for the hypervisor to learn about
	guest OS page table changes, enables
	<ol class="answer_list">
	<li> individual changes to guest OS page tables to occur without
		having to immediately update the hypervisor's shadow page
		table.
	<li> consistently faster virtual address translation to physical
		addresses.
	<li> fewer page faults to be generated by the VM.
	<li> a clumsy, though workable, mechanism for updating the
		hypervisor's shadow page table.
	<li> None of the above
	</ol>
</li><br/>
<li> Second Level Address Translation (SLAT) on the x86 architecture that
	handles much of the (shadow) page table manipulation in hardware,
	without the need for traps,
	<ol class="answer_list">
	<li> uses the shadow page table to translate the
		<em>guest physical address</em> into a
		<em>host physical address</em> without
		any hypervisor software intervention.
	<li> allows the CPU to translate a <em>guest virtual address</em>
		into a <em>guest physical address</em>
		using the guest OS page tables, in the same way as is
		done for the non-virtualized case.
	<li> avoids the need for techniques that rely on read only access of
		page tables or TLB entry invalidations to update the shadow
		page table.
	<li> greatly reduces the number of full process context switches
		that the hypervisor must perform.
	<li> None of the above
	</ol>
</li><br/>
<li> The sum of the virtual physical memories allocated to each VM
	<ol class="answer_list">
	<li> may exceed the amount of actual physical memory, but only
		if the sum of the used page frames is less than the amount
		of physical memory.
	<li> can exceed the amount of actual physical memory.
	<li> <strong>must never</strong> exceed the amount of
		actual physical memory.
	<li> must be <strong>strictly</strong> less than the amount of
		actual physical memory, so as to leave space for the
		hypervisor's shadow page table.
	<li> None of the above
	</ol>
</li><br/>
<li> While a hypervisor is allowed to overcommit its physical memory, doing so
	<ol class="answer_list">
	<li> is almost never a good idea and often leads to excessive
		page faults and system thrashing.
	<li> should be limited to situations in which all of the VMs are
		running the same guest OS (to take advantage of page sharing).
	<li> requires that each guest OS limit the size of its user process
		virtual address spaces to be less than the size of
		physical memory.
	<li> is quite useful since most VMs won't need the maximum amount
		all of the time.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Ballooning</em> is a technique used by hypervisors to
	<ol class="answer_list">
	<li> control the amount of virtual physical memory that a VM has.
	<li> expand the size of a process' virtual address space beyond
		that normally supported by the CPU architecture.
	<li> indirectly force the guest OS to decide which data to
		page in/out of virtual physical memory.
	<li> increase the maximum number of user processes that a guest OS is
		able to simultaneously support.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Ballooning</em> is a technique used by hypervisors that
	<ol class="answer_list">
	<li> expands the size of a process' virtual address space beyond
		that normally supported by the CPU architecture.
	<li> loads a small pseudo device driver into each VM.
	<li> increases the maximum number of user processes that a guest OS is
		able to simultaneously support.
	<li> changes the (apparent) amount of the virtual physical memory
		currently being used by a VM.
	<li> None of the above
	</ol>
</li><br/>
<li> The impact of hypervisor memory overcommitment can be reduced by
	<ol class="answer_list">
	<li> second level address translation (SLAT).
	<li> ballooning.
	<li> deduplication.
	<li> shadow page tables.
	<li> None of the above
	</ol>
</li><br/>

<li> Using the hypervisor to mediate between guest physical devices and
	host physical devices enables
	<ol class="answer_list">
	<li> <strong>all</strong> input devices (e.g., keyboards, mice)
		to be effectively shared between VMs.
	<li> secondary storage devices (e.g., HDDs, SSDs)
		to be effectively shared between VMs.
	<li> the devices to be different (e.g., use an SSD as if it were
		an HDD).
	<li> hardware upgrades without the need to upgrade software
		(e.g., device drivers).
	<li> None of the above
	</ol>
</li><br/>
<li> I/O MMUs use page tables to map guest physical addresses to host
	physical addresses for memory-mapped I/O, which
	<ol class="answer_list">
	<li> prevents the DMA use by one VM from using memory that is
		assigned to another VM.
	<li> prevents the direct sharing of I/O devices by multiple VMs.
	<li> allows devices to directly access their assigned VMs without
		compromising a guest OS on another VM.
	<li> enables multiple guest OSs to share block devices.
	<li> None of the above
	</ol>
</li><br/>
<li> I/O MMUs use interrupt remapping tables to convert interrupts
	(and their interrupt vectors) from the physical system bus to
	<ol class="answer_list">
	<li> an interrupt  that the hypervisor directly handles on behalf
		of any VM.
	<li> a memory-mapped I/O address that the hyervisor directly handles
		on behalf of any VM.
	<li> a memory-mapped I/O address for the current VM.
	<li> an interrupt that the current VM expects.
	<li> None of the above
	</ol>
</li><br/>
<li> A <em>device domain</em> for a type 1 hypervisor is
	<ol class="answer_list">
	<li> a VM (and guest OS) that is assigned to control a specific
		device (essentially serving like a type 2 hypervisor for
		these devices).
	<li> required for each type of device type, with different device
		domains for HDDs, SSD, printers, etc.
	<li> created by the hypervisor in order to buffer I/O transfers
		between devices and the target guest OS.
	<li> useful when a device driver isn't available for the hypervisor,
		but is available for one of the guest OSs.
	<li> None of the above
	</ol>
</li><br/>
<li> Single Root I/O Virtualization (SR-IOV) is provided by the
	<ol class="answer_list">
	<li> hypervisor.
	<li> VM.
	<li> guest OS.
	<li> device and its controller.
	<li> None of the above
	</ol>
</li><br/>
<li> Use of Single Root I/O Virtualization (SR-IOV)
	<ol class="answer_list">
	<li> provides <em>virtual functions</em> to the guest OS,
		but they do <strong>not</strong> enable any
		device configuration.
	<li> is required in order for the hypervisor to be able to create
		a single <em>device domain</em> to control
		<strong>all</strong> devices.
	<li> simplifies the hypervisor and improves its operational efficiency.
	<li> enables remote devices to be mapped, as a remote special file, 
		so that they can be shared between multiple hypervisors.
	<li> None of the above
	</ol>
</li><br/>
<li> What functions enable Single Root I/O Virtualization (SR-IOV) devices
	to constrain what VMs are able to do while providing access to most
	of its functionality?
	<ol class="answer_list">
	<li> physical functions.
	<li> i/o functions.
	<li> mapped functions.
	<li> virtual functions.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following enables cloud system resource usage to be
	automatically monitored, controlled, and reported, providing
	transparency to both the provider and cloud service consumer?
	<ol class="answer_list">
	<li> On-demand self-service
	<li> Broad network access
	<li> Resource pooling
	<li> Rapid elasticity
	<li> Measured service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is a composition of two or more distinct
	cloud infrastructures that are bound together by standardized or
	proprietary technologies that enable data and application portability?
	<ol class="answer_list">
	<li> Private cloud
	<li> Community cloud
	<li> Public cloud
	<li> Hybrid cloud
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following enables consumers to automatically provision
	computing resources as needed without human interaction from each
	service provider?
	<ol class="answer_list">
	<li> On-demand self-service
	<li> Broad network access
	<li> Resource pooling
	<li> Rapid elasticity
	<li> Measured service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following is provisioned for exclusive use by a specific
	group of consumers from organizations that have shared concerns.
	It may be owned, managed, and operated by one or more of the
	organizations in the group, a third party, or some combination?
	<ol class="answer_list">
	<li> Private cloud
	<li> Community cloud
	<li> Public cloud
	<li> Hybrid cloud
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following enables computing resources to be collected
	together by the provider so that they can serve multiple tenants,
	with different physical and virtual resources dynamically
	(re)assigned according to consumer demand?
	<ol class="answer_list">
	<li> On-demand self-service
	<li> Broad network access
	<li> Resource pooling
	<li> Rapid elasticity
	<li> Measured service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following provides a cloud infrastructure for the exclusive
	use of a single organization and it may exist on or off premises?
	<ol class="answer_list">
	<li> Private cloud
	<li> Community cloud
	<li> Public cloud
	<li> Hybrid cloud
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following provides capabilities that can be automatically
	provisioned and released to quickly adapt to demand?
	<ol class="answer_list">
	<li> On-demand self-service
	<li> Broad network access
	<li> Resource pooling
	<li> Rapid elasticity
	<li> Measured service
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following provides access by the general population and
	may be owned, managed, and operated by one or more organizations?
	<ol class="answer_list">
	<li> Private cloud
	<li> Community cloud
	<li> Public cloud
	<li> Hybrid cloud
	<li> None of the above
	</ol>
</li><br/>
<li> Which of following describes services that are available over the
	network and are accessed via standard mechanisms and protocols?
	<ol class="answer_list">
	<li> On-demand self-service
	<li> Broad network access
	<li> Resource pooling
	<li> Rapid elasticity
	<li> Measured service
	<li> None of the above
	</ol>
</li><br/>
<li> Virtualization has been key in enabling the commercialization of clouds by
	<ol class="answer_list">
	<li> reducing the extra costs incurred by unused computing headroom.
	<li> providing a means for different customers to share the same
		physical hardware.
	<li> supporting the remote access of computing and storage services.
	<li> enabling vendors to oversell their physical hardware capacity,
		knowing that the full capacity will likely never be needed.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Live migration</em> of a VM occurs by
	<ol class="answer_list">
	<li> shutting down the VM, copying all of its associated files
		to a new target, then restarting the VM on the new hypervisor.
	<li> pausing the VM, copying all of its associated files and process
		information, then resume the VM on the new hypervisor.
	<li> copying the files and memory pages of a VM while it is still
		running, then briefly pause the VM while the last few modified
		pages are copied before resuming the VM on the new hypervisor.
	<li> copying the files from the old VM, while it is still running,
		into a new VM that is already running on the new hypervisor.
		All new processes are created on the new VM and the old VM
		decommissioned after its last process completes.
	<li> None of the above
	</ol>
</li><br/>
<li> <em>Seamless live migration</em> of a VM occurs when
	<ol class="answer_list">
	<li> the VM experiences <strong>no</strong> down-time.
	<li> the amount of VM down-time is unnoticable to the customer.
	<li> any files to be migrated are remote mounted, thus avoiding
		any file copying.
	<li> the migration of the VM is done automatically without any
		initiation or oversight by the customer.
	<li> None of the above
	</ol>
</li><br/>

<li> Which of the following provides the consumer with the ability to
	provision and control processing, storage, networks, and other
	basic resources in order to deploy and run arbitrary software?
	<ol class="answer_list">
	<li> Resource as a Service (RaaS)
	<li> Infrastructure as a Service (IaaS)
	<li> Platform as a Service (PaaS)
	<li> Software as a Service (SaaS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following enables the consumer to use the provider's
	applications running on a cloud infrastructure, which are accessible
	through either a thin client (e.g., web browser), or
	program interface?
	<ol class="answer_list">
	<li> Resource as a Service (RaaS)
	<li> Infrastructure as a Service (IaaS)
	<li> Platform as a Service (PaaS)
	<li> Software as a Service (SaaS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following allows the consumer to deploy, configure, and
	control acquired or consumer-created applications built using
	programming languages, libraries, services, and tools supported by
	the cloud provider?
	<ol class="answer_list">
	<li> Resource as a Service (RaaS)
	<li> Infrastructure as a Service (IaaS)
	<li> Platform as a Service (PaaS)
	<li> Software as a Service (SaaS)
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are an example of Infrastructure as a Service
	(IaaS)?
	<ol class="answer_list">
	<li> Google Compute Engine
	<li> Force.com
	<li> Yahoo Groups
	<li> Microsoft Office 365
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are an example of Platform as a Service
	(PaaS)?
	<ol class="answer_list">
	<li> Google Compute Engine
	<li> Force.com
	<li> Yahoo Groups
	<li> Microsoft Office 365
	<li> None of the above
	</ol>
</li><br/>
<li> Which of the following are an example of Software as a Service
	(SaaS)?
	<ol class="answer_list">
	<li> Google Compute Engine
	<li> Force.com
	<li> Yahoo Groups
	<li> Microsoft Office 365
	<li> None of the above
	</ol>
</li><br/>

<li> The only practical way to detect message loss between message
	senders and their receivers is via
	<ol class="answer_list">
	<li> enumerated messages.
	<li> timeouts.
	<li> challege response.
	<li> sticky sessions.
	<li> None of the above
	</ol>
</li><br/>
<li> When a request for a large piece of work is issued, it is important
	that any failures on sub-parts of that work cause
	<ol class="answer_list">
	<li> the original request to be reissued to a different deployment
		of the called service.
	<li> a retry of the entire piece of work.
	<li> a retry of only the failed sub-parts of the work.
	<li> the request to report a failure for the entire piece of work.
	<li> None of the above
	</ol>
</li><br/>
<li> In the context of cloud computing the observed request failure rate is
	<ol class="answer_list">
	<li> determined solely by the communication channel error rate.
	<li> determined solely by the failure rate of the involved servers.
	<li> the combined error rate of the communication channel and the
		failure rate of the servers involved in the computation.
	<li> the difference in the error rate of the communication channel and
		the failure rate of the servers involved in the computation.
	<li> None of the above
	</ol>
</li><br/>
<li> One cause for transient faults is the
	<ol class="answer_list">
	<li> use of sticky sessions, even in the absence of any computer
		failures. 
	<li> loss of (service) requests as they are rerouted from a failed
		computer to one that is still functioning.
	<li> failure of an idempotent operation to return the same result
		when called multiple times with the same data.
	<li> use of multi-tier architectures which often lose data
		when sending messages between the different tiers.
	<li> None of the above
	</ol>
</li><br/>
<li> A best practice is for systems to expose interfaces that
	<ol class="answer_list">
	<li> pass all the data for a complex transaction back and forth as
		a single large structure parameter, updating the structure
		as work is completed.
	<li> are stateful, remembering what was communicated previously
		so that all of the data need not be passed back and forth.
	<li> are idempotent, so simple retry logic can be used to mask most
		transient failures.
	<li> are sticky, meaning that calls from the same original request
		will be handled by the same component running on the same
		computer.
	<li> None of the above
	</ol>
</li><br/>
<li> A common architectural pattern used to deal with transient failures is to
	<ol class="answer_list">
	<li> divide the system into pure computational and storage layers,
		frequently saving system computations, making restarts
		simple and easy.
	<li> combine all of the components of an software system into a
		single application, enabling tighter integration of the
		components for better maintainability.
	<li> <strong>always</strong> use journaling file systems so that
		no data is ever lost.
	<li> run the software on enough computers to handle twice the
		maximum load, so that if half fail, the expected max work
		load can still be handled.
	<li> None of the above
	</ol>
</li><br/>
<li> Dividing a software system into multiple tiers (e.g., presentation,
	business logic, and data storage) provides
	<ol class="answer_list">
	<li> simplicity.
	<li> reliability.
	<li> maintainability.
	<li> scalability.
	<li> None of the above
	</ol>
</li><br/>
<li> A common technique for dealing with frequent server-level failures in a
	cloud data center is to
	<ol class="answer_list">
	<li> implement the key software services in two very different ways
		and run them on different systems. All the data is run through
		both implementations, but if one fails it's very unlikely that
		the other would too.
	<li> decompose the system into one or more tiers of (software) servers
		that process requests on a best-effort basis, storing critical
		application state in a dedicated storage tier. 
	<li> ignore the possibility of failures since they occur infrequently.
	<li> run the software on enough computers to handle twice the
		maximum load, so that if half fail, the expected max work
		load can still be handled.
	<li> None of the above
	</ol>
</li><br/>
<li> The large number of components that typically comprise a cloud computer
	require that
	<ol class="answer_list">
	<li> each computer in the cloud be doubly redundant to
		keep the probability of failure low enough to prevent
		constant system failures.
	<li> the realiability (e.g., up-time) be 99.999% in order to prevent
		cascading failures from rendering the system unusable.
	<li> the software be run on enough computers to handle twice the
		maximum load, so that if half fail, the expected max work
		load can still be handled.
	<li> the software give careful consideration for handling failures
		of those components.
	<li> None of the above
	</ol>
</li><br/>
<li> Linux application containers rely on name spaces to help
	<ol class="answer_list">
	<li> isolate process groups from one another.
	<li> prevent collisions between applications with the same name.
	<li> restrict access to parts of the file system by a container's
		processes.
	<li> applications within different containers dynamically discover
		one another so that they can interact (e.g., exchange
		messages).
	<li> None of the above
	</ol>
</li><br/>
<li> Linux application containers rely on control groups (cgroups) in order to
	<ol class="answer_list">
	<li> the maximum amount of memory the container can use.
	<li> limit the amount of CPU utilization by the container.
	<li> restrict the network throughput used by the container.
	<li> cap the amount of disk space the container can use.
	<li> None of the above
	</ol>
</li><br/>
<li> While application containers have a number of benefits over virtual
	machines, their biggest disadvantage is that
	<strong>all</strong> of the
	<ol class="answer_list">
	<li> containers (and their applications)
		<strong>must</strong> run on the same version
		of the same OS kernel.
	<li> containers (and their applications)
		<strong>must</strong> must share the same
		set of permissions (i.e., running under
		a single user account/login).
	<li> applications <strong>must</strong> share
		the same exact set of libraries.
	<li> applications <strong>must</strong> be implemented
		using the same programming language and run-time system.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers are a relatively recent development, having been
	first created and used
	<ol class="answer_list">
	<li> in late 2009 with the release of Windows 7 (and Server 2008 R2)
		from Microsoft. 
	<li> in the early 1990s shortly after Linux first emerged.
	<li> around 2000 with the development of FreeBSD's Jails.
	<li> in March 2013 with the release of Docker.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers (e.g., Rocket, Docker) running within a stripped
	down OS (e.g., CoreOS) is analagous to having the same guest OS
	running on multiple VMs within a type 1 hypervisor, where the
	<ol class="answer_list">
	<li> application container corresponds to the VM (and its guest OS).
	<li> application container corresponds to the hypervisor.
	<li> stripped down OS corresponds to the guest OS (and its VM).
	<li> stripped down OS corresponds to the hypervisor.
	<li> None of the above
	</ol>
</li><br/>
<li> As a general rule of best practice <strong>all</strong> of the
	<ol class="answer_list">
	<li> application containers used to house different
		components/tiers of an application should be configured
		to run on the same OS version.
	<li> components of a multi-tier application should reside
		in the same application container.
	<li> application containers housing componentes/tiers of the same
		application should be run on the same VM.
	<li> application containers housing componentes/tiers of the same
		application should be run on the same hypervisor.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers provide a lightweight virtual environment that
	<ol class="answer_list">
	<li> unlike a the guest OS on a VM, shares the same OS kernel as
		the host system.
	<li> can be run on <strong>any</strong> OS.
	<li> enables the application it contains to be easily and seamless
		migrated to another platform without any interruption in
		service.
	<li> guarantees that any processes inside the container cannot see
		any processes or resources outside the container.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers provide a lightweight virtual environment that
	<ol class="answer_list">
	<li> is the equivalent of a guest OS running on a VM, but easier
		to manage.
	<li> groups and isolates processes and their resources,
		such as memory, CPU, and disk.
	<li> can be run on <strong>any</strong> OS.
	<li> enables the application it contains to be easily and seamless
		migrated to another platform without any interruption in
		service.
	<li> None of the above
	</ol>
</li><br/>
<li> As a general rule of best practice
	<ol class="answer_list">
	<li> application containers should contain only a single application.
	<li> application containers may contain many applications, so long as
		they use the same set of libraries.
	<li> VMs should contain only a single application.
	<li> VMs may contain several applications.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers provide a "locked box" of capability which
	<ol class="answer_list">
	<li> ensures the same run-time environment for the application(s)
		within the container.
	<li> prevents any application compromise by an attacker from ever
		extending beyond the confines of the container.
	<li> uses encryption to ensure that the application code they
		contain cannot be reverse engineered.
	<li> can limit the application(s) it contains, preventing them from
		using a newer version of a resource (e.g., library) unless
		the container is update and redeployed.
	<li> None of the above
	</ol>
</li><br/>
<li> Compared to VMs (and a commerical guest OS running on it), application
	containers are
	<ol class="answer_list">
	<li> easier to secure.
	<li> harder to secure.
	<li> are more likely to contain a Trojan Horse since containers are
		new and not all providers are reputable.
	<li> more resource intensive.
	<li> None of the above
	</ol>
</li><br/>
<li> Application containers can provide a portable and consistent
	operating environment for
	<ol class="answer_list">
	<li> development.
	<li> testing.
	<li> deployment.
	<li> None of the above
	</ol>
</li><br/>
<li> When comparing application containers to VMs on the same computer,
	<ol class="answer_list">
	<li> VMs and their guest OS consume more resources than do
		application containers.
	<li> VMs provide <strong>less</strong> security than do application
		containers.
	<li> containers allow applications to migrate seamless, without any
		interruption in the services their applications provide.
	<li> containers can support 2-6 times the number of applications
		as VMs.
	<li> None of the above
	</ol>
</li><br/>

</ol>

<hr/>
<em>
<a href="mailto:eckart_dana@columbusstate.edu?subject=web_pages" style="float: left">eckart_dana@columbusstate.edu</a>
<a href="/eckart/classes/cpsc6125" style="float: right">CPSC 6125</a>
</em>

</body>
</html>

